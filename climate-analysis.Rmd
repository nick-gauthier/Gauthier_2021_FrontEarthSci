---
title: "Supplemental Information"
subtitle: 'Analysis scripts for "Drought Variability and the Robustness of Agrarian Social Networks"'
author: "Nicolas Gauthier"
date: "Last knit on: `r format(Sys.time(), '%d %B, %Y')`"
#bibliography: bibliography.bibtex
output:
  tufte::tufte_handout:
    citation_package: natbib
    toc: true
    highlight: pygments
  tufte::tufte_html:
    tufte_features: ["fonts", "italics"]
    toc: true
link-citations: yes
monofont: courier
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

```{r, message = FALSE}
library(raster)
library(tidyverse)
library(broom)
library(wql)
library(furrr)
library(mgcv)
```



# Climate Analysis: Drought Patterns

First we estimate present and past climate patterns.

## Study Area

First we define a boundary box covering much of the western United States, ranging between W$124.5\deg$ and W$-107\deg$ and N$31\deg$ and N$37.5\deg$, which we'll use to constrain all subsequent climate analyses. This study area is significantly larger than the one we'll define in the next section for the network analysis. This allows us to sample a much wider range of climatic variability, while still remaining within the broader western US climate zone. This ensures that both A) our statistical analyses will be more robust to sampling error and B) the results will be less sensitive to the exact location and dimensions of our study area.

```{r}
bbox_wus <- extent(c(-124.5, -102, 30, 42.1))
```

```{r bbox_map, fig.margin=TRUE, echo = FALSE, fig.cap = 'Locations of 2 bounding boxes.'}
bb1 <- as(bbox_wus, 'SpatialPolygons') %>%
  fortify
bb2 <- as(bbox <- extent(c(-113, -107, 31, 37.5)), 'SpatialPolygons') %>%
  fortify

map_data("usa") %>%
  ggplot(aes(x = long, y = lat, group = group)) + 
  geom_polygon(color = 'darkgrey', fill = NA) + 
  geom_polygon(data = bb1, color = 'black', fill = NA) +
  geom_polygon(data = bb2, color = 'black', fill = NA) +
  coord_fixed(1.3) +
  theme_void()
```

## Climate Data
Import high resolution SPEI maps generated from PRISM data. Calculate the average summertime (JJA) SPEI value for each year.

```{r import_observations, warning=FALSE}
# calculate average JJA SPEI
spei_obs <- ((brick('data/spei12_6_PRISM.nc') + 
              brick('data/spei12_7_PRISM.nc') + 
              brick('data/spei12_8_PRISM.nc')) / 3) %>%
  crop(bbox_wus) %>% # crop to the study area bounding box
  `names<-`(1895:2017) %>% # add year names
  .[[-1]] # SPEI calculated on 12 month lag, so drop 1st year
```

```{r}
# calculate average JJA SPEI
spei_obs <- brick('data/spei24_8_PRISM.nc') %>%
  crop(bbox_wus) %>% # crop to the study area bounding box
  `names<-`(1895:2018) %>% # add year names
  .[[-c(1:2)]] # SPEI calculated on 12 month lag, so drop 1st year
```


Import the reconstructed SPEI fields from PHYDA. PHYDA uses a novel off-line data assimilation approach, using simulated SPEI from the CESM LME experiments as physically-consistent model priors and a network of tree rings, ice cores, and corals as assimilated observations.

```{r import_reconstructions}
spei_recon <- brick('data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc',
                    varname = 'spei_mn') %>%
   .[[1100:1999]] %>% # extract years of interest
   rotate %>% # rotate longitudes to -180 to 180
   crop(bbox_wus, snap = 'out')
```

Let's compare the reconstructions for the summer of 1985
```{r, echo = FALSE, fig.margin = TRUE, fig.cap = 'Observed and reconstructed SPEI for summer 1985.'}
brick(c(spei_obs[[90]], resample(spei_recon[[886]], spei_obs, method = 'ngb'))) %>%
  `names<-`(c('Observed', 'Reconstructed')) %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(type, spei, 3:4) %>%
  ggplot(aes(x, y)) +
  geom_raster(aes(fill = spei)) +
  facet_wrap(~type) +
  scale_fill_distiller(palette = 'Spectral', direction = 1,
                       limits = c(-3, 3),
                       guide = 'legend') +
  coord_quickmap() +
  theme_void()
```

```{r plot_variance_trend, echo = FALSE, fig.margin=TRUE, fig.cap = 'Plotting out the time series of the reconstruction reveals no clear trend in the mean. It does, however, show a pattern of increasing variance over time, which is entirely a function of the changing number of proxies used in the data assimilation approach.'}
spei_recon %>% 
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, value, 3:902) %>%
  mutate(year = str_sub(year, 2),
         year = as.numeric(year)) %>%
  ggplot(aes(year, value)) +
  geom_line(alpha = .1, aes(group = interaction(x, y))) +
  scale_y_continuous(limits = c(-5,5)) +
  geom_smooth() +
  ggtitle('Standardized Precipitation-Evapotranspiration Index in the American Southwest',
          'June-August, 12 month lag') + 
  theme_bw()
```

## Empirical Orthogonal Functions

We calculate the leading Empirical Orthogonal Functions of a ~100 year series of the summertime average Standardized Precipitation-Evapotranspiration Index. These patterns then undergo a varimax rotation to highlight more physically meaningful spatial patterns. We compare these patterns to those derived from a long term (~2ka) reconstruction based on data assimilation and CESM LME. We confirm that the spatial patterns detected for the past 100 years have been robust over time and explain up to 85% of the variance in a full 1,000 year sequence of reconstructed drought dynamics.

This is a function to reweight our observations based on the latitude, to acount for the areal distortion of each cell as latitude changes.

```{r}
area_weight <- function(x){
  names_x <- names(x)
  x %>%
    init('y') %>% # get a map of latitudes
    `*`(pi/180) %>% # convert to radians
    cos %>%
    sqrt %>%
    `*`(x) %>%
    `names<-`(names_x)
}
```

This function calculates the effective observations in an autocorrelated time series of rasters.

```{R}
n_effective <- function(x){
  n <- nlayers(x)
  x %>%
    area_weight %>%
    as.data.frame(na.rm = TRUE) %>%
    t %>%
    as_tibble %>%
    gather(cell, value) %>%
    nest(value) %>%
    mutate(rho = map_dbl(data, ~cor(.$value, lag(.$value), use = 'comp')),
           effective_n = n * (1 - rho^2) / (1 + rho^2)) %>% #bretherton et al 1999

    summarise(mean(effective_n)) %>%
    pull
}
```

Principal components analysis of observation and recon data

```{r}
# these are adpated from wql and sinkr
obs_pca <- spei_obs %>%
  area_weight %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  prcomp(scale. = FALSE) # use the covariance matrix

recon_pca <- spei_recon %>%
  area_weight %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  prcomp(scale. = FALSE)
```

Now we calculate the rotated empirical orthogonal functions for both the observed and reconstructed drought maps. For the observations, we see that the leading 5 eofs explain 85% of the variance in the series, so we'll retain those for rotation.

we want to look for separation in the error bars. eofs that are not well separate can be considered effective multiplets, and hsould not be split in truncation.

```{r}
obs_eigs <- obs_pca %>%
  tidy(matrix = 'pcs') %>%
  mutate(eigenvalues = std.dev ^ 2,
         error = sqrt(2 / n_effective(spei_obs)),
         low =  eigenvalues * (1 - error) * 100 / sum(eigenvalues),
         hi = eigenvalues * (1 + error) * 100 / sum(eigenvalues),
         cumvar_line = hi + 0.02 * max(hi))

recon_eigs <- recon_pca %>% 
  tidy(matrix = 'pcs') %>%
  mutate(eigenvalues = std.dev ^ 2,
         error = sqrt(2 / n_effective(spei_recon)),
         low =  eigenvalues * (1 - error) * 100 / sum(eigenvalues),
         hi = eigenvalues * (1 + error) * 100 / sum(eigenvalues),
         cumvar_line = hi + 0.02 * max(hi))
```

```{r plot_variance_obs, echo = FALSE, fig.cap = 'Variance explained'}
obs_eigs %>% 
 # filter(eigenvalues > 1) %>%
  mutate(separated = if_else(is.na(lag(low)), TRUE, hi < lag(low)),
                   test = cumsum(separated),
         weights = if_else(PC < 50, 0, 1))%>%
    filter(PC <= 12) %>%
ggplot(aes(x = PC, y = percent * 100)) +
  geom_errorbar(aes(x = PC, ymin = low, ymax = hi), width = 0.4) +
  geom_point(size = 2, aes(color = as.factor(test))) + 
  geom_text(aes(x = PC, y = cumvar_line, label = round(cumulative * 100, 1)), size = 3, vjust = 0) +
  labs(list(x = "Principal Component", y = "Normalized Eigenvalue")) + 
  theme_bw() + guides(color = F) + scale_x_continuous(
    breaks = c(4, 8, 12)
  )#scale_y_continuous(trans = 'log')# + stat_smooth(aes(weight = weights), method = 'lm') 
  #geom_smooth(method = 'gam', formula = y ~ s(x, k = 10, bs= 'ps', m = 0))

ggsave('figures/variance_explained.pdf', width = 5, height = 5)
```


the overlaps mean these are effective multiplets. the real eof can be some linear combination of these. This won't in practice impact our successive results, as long as we don't truncate within these multplets, but only between them.
using the log linear test, keep 7 of recon and 6 for obs
Look at the variances for the eofs of each field, to inform truncation. Let's retain the leading four modes from PHYDA

```{r plot_variance_phyda, fig.margin=TRUE, echo = FALSE,fig.cap = 'Variance explained'}
recon_eigs %>%
  mutate(separated = if_else(is.na(lag(low)), TRUE, hi < lag(low)),
          test = cumsum(separated),
         weights = if_else(PC < 3, 0, 1))%>%
  filter(PC <=12) %>%
ggplot(aes(x = PC, y = percent * 100)) +
  geom_errorbar(aes(x = PC, ymin = low, ymax = hi), width = 0.4) +
  geom_point(size = 3, aes(color = as.factor(test))) + 
  geom_text(aes(x = PC, y = cumvar_line, label = round(cumulative * 100, 1)), size = 3, vjust = 0) +
  labs(list(x = "PC", y = "Normalized Eigenvalue")) + #scale_y_continuous(trans = 'log') +
  theme_bw() + #stat_smooth(aes(weight = weights), method = 'lm') +
  guides(color = FALSE)
```

Repeat for the prism observations. Again, we'll retain the leading four modes.


It looks like there is some autocorrelation in phyda but not in the observations, will have to adjust in the future.


Now, calculate the eofs for both fields, retaining the 4 leading components in each case for rotation

```{r calc_eofs}
# Decide how many modes to retain
n_modes <- 6 # choose 2, 4, 6, 9

obs_reof <- spei_obs %>% 
  area_weight %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  eof(n_modes, scale. = FALSE) # we don't rescale (ie we use the covariance matrix, because spei is already normalized and rescaled)

recon_reof <- spei_recon %>%
  area_weight %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  eof(n_modes, scale. = FALSE)
```

```{r}
plan(multicore)
get_EOFs <- function(pc_object, eigs, rast, n_modes){
  pc_object %>%
    tidy(matrix = 'variables') %>%
    filter(PC <= n_modes) %>%
    group_by(PC) %>%
    nest %>%
    left_join(eigs[1:2]) %>%
    mutate(data = future_map2(data, std.dev, ~mutate(.x, value = value * .y))) %>%
    unnest %>%
    bind_cols(as.data.frame(rast[[10]], xy = T, na.rm = T)[1:2] %>% slice(rep(1:n(), times = n_modes)))
}

eofs_obs <- get_EOFs(obs_pca, obs_eigs, spei_obs, n_modes)
eofs_recon <- get_EOFs(recon_pca, recon_eigs, spei_recon, n_modes)
```

We see that the leading 3 eigenvectors from the observations and reconstructions are a good match, including their ordering. 4-6 look good too

```{r}
states <- maps::map('state', regions = c('arizona', 'new mexico', 'colorado', 'california', 'utah', 'nevada'), 
                    fill = TRUE, plot = FALSE)
```

Let's map out the spatial and temporal patterns in the REOFs. First we'll look at the spatial patterns. It looks like the observed and reconstructed datasets reveal very similar spatial patterns for the 4 leading modes.

```{r plot_robs_eof, echo = FALSE, fig.width = 12, fig.height = 8, fig.cap = 'Observed drought REOFs'}
as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>% 
  cbind(obs_reof$REOF) %>%
  select(-3) %>%
  #mutate(EOF1 = EOF1 * -1, EOF2 = EOF2 * -1, EOF7 = EOF7 * -1) %>%
  gather(eof, value, 3:(n_modes + 2)) %>%
  mutate(value = value * -1) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-1,1)) +
  facet_wrap(~eof, ncol = 2) +
  theme_void() +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  coord_quickmap() +
  #ggtitle(paste('Leading', n_modes, 'rotated empirical orthogonal functions')) +
  theme(legend.position = "bottom")

ggsave('figures/reof_observed.pdf', height = 8, width = 7)
```

```{r, echo = FALSE, fig.margin = TRUE, fig.cap='Observed and reconstructed Empirical Orthogonal Functions before rotation.'}
ggplot(eofs_obs, aes(x, y, fill = value)) +
  facet_wrap(~PC, ncol = 2) +
  geom_raster() +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-.85, .85)) +
  theme_void() + 
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  coord_quickmap()+  theme(legend.position = "bottom")
ggsave('figures/eof_observed.pdf', height = 8, width = 7)
ggplot(eofs_recon, aes(x, y, fill = value)) +
  facet_wrap(~PC) +
  geom_raster() +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-.73, .73)) +
  theme_void() + 
  coord_quickmap()
```

The colors here are eigenvectors weighted by the sqare root of the associated eigenvalue, so these loadings represent the covariance between each grid cell and each amplitude (principal component). A key assumption here is that the REOFs calculated from both the observations and reconstructions correspond to the same physical phenomena. That way we can just use the observed patterns as the high resolution patterns, and infer their temporal evolution from the reconstructions. This means we don't have to downscale the reconstructed reofs to use them, as otherwise we'd have to deal with issues of spatial represntativeness and non-overlap. Now we can be ensured that these are the same signals.

```{r plot_recon_reofs, echo = FALSE, fig.cap = 'Reconstructed drought REOFs'}
as.data.frame(spei_recon[[1]], xy = TRUE, na.rm = TRUE) %>% cbind(recon_reof$REOF) %>%
  select(-3) %>%
  gather(eof, value, 3:(n_modes+2)) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
    scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() + 
  coord_quickmap()
```

To confirm this, let's plot the time series (PCs) of each observed and reconstructed REOF against each other, to confirm that they correspond to the same patterns. Look at the correlations between the modes.

## Drought Amplitudes

These are the amplitudes of the EOFs, not the REOFs

```{r amplitudes}
calc_amplitude <- function(x, n_modes){
  x %>%
    tidy(matrix = 'samples') %>%
    filter(PC <= n_modes) %>%
    rename(year = row, amplitude = value) %>%
    mutate(year = as.numeric(str_sub(year, 2)),
           PC = as.factor(PC)) %>%
    group_by(PC) %>%
    mutate(amplitude = scale(amplitude)) %>%
    ungroup
}

# this is the way to do it with the eofs, no reofs until i implement it
obs_eof_amp <- calc_amplitude(obs_pca, n_modes)

ggplot(obs_eof_amp, aes( year, amplitude, color = PC)) +
  geom_line() +
  facet_wrap(~PC) + geom_vline(xintercept = 1956) + geom_smooth()
recon_eof_amp <- calc_amplitude(recon_pca, n_modes)

amplitude_modern <- recon_eof_amp %>% 
  inner_join(obs_eof_amp, by = c('year', 'PC'), 
            suffix = c('_recon', '_obs'))
```

```{r}
#this is the temporary workaround while I still use wql
obs_reof_amp <- obs_reof$amplitude %>%
  as_tibble(rownames = 'year') %>%
  mutate(year = parse_number(year)) %>%
  gather(PC, amplitude, -year) %>%
  mutate(PC = case_when(PC == 'EOF1' ~ 'REOF1',
                        PC == 'EOF2' ~ 'REOF2',
                        PC == 'EOF3' ~ 'REOF3',
                        PC == 'EOF4' ~ 'REOF4',
                        PC == 'EOF5' ~ 'REOF5',
                        PC == 'EOF6' ~ 'REOF6'))

recon_reof_amp <- recon_reof$amplitude %>%
  as_tibble(rownames = 'year') %>%
  mutate(year = parse_number(year)) %>%
  gather(PC, amplitude, -year) %>%
  mutate(PC = case_when(PC == 'EOF1' ~ 'REOF2',
                        PC == 'EOF2' ~ 'REOF4',
                        PC == 'EOF3' ~ 'REOF5',
                        PC == 'EOF4' ~ 'REOF1',
                        PC == 'EOF5' ~ 'REOF3',
                        PC == 'EOF6' ~ 'REOF6')) %>%
  arrange(PC)
```

```{r}
ggplot(obs_reof_amp, aes( year, amplitude, color = PC)) +
  geom_line() +
  facet_wrap(~PC) + geom_vline(xintercept = 1956) + geom_smooth()
```


```{r eval = FALSE, include = FALSE}
obs_reof_amp %>%
  group_by(PC) %>%
  filter(amplitude == max(amplitude) | amplitude == min(amplitude)) %>%
  pull(year) %>%
  `-`(1895) %>%
  spei_obs[[.]] %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, SPEI, X1915:X2008) %>%
  mutate(year = factor(year, levels = paste0('X', c(1915, 1996 ,1987, 2011, 1934, 1984, 2014, 2017, 1947, 1963, 1977, 2008)))) %>%
  ggplot(aes(x, y, fill = SPEI)) +
  geom_raster() +
  facet_wrap(~year) +
  coord_quickmap() +
  scale_fill_distiller(palette = 'Spectral', limits = c(NA, 4), direction = -1)
```



To cite use of dataset: Boyin Huang, Peter W. Thorne, Viva F. Banzon, Tim Boyer, Gennady Chepurin, Jay H. Lawrimore, Matthew J. Menne, Thomas M. Smith, Russell S. Vose, and Huai-Min Zhang (2017): NOAA Extended Reconstructed Sea Surface Temperature (ERSST), Version 5. [indicate subset used]. NOAA National Centers for Environmental Information. doi:10.7289/V5T72FNM [access date].
Please note: If you acquire NOAA_ERSST_V5 data products from PSD, we ask that you acknowledge us in your use of the data. This may be done by including text such as NOAA_ERSST_V5 data provided by the NOAA/OAR/ESRL PSD, Boulder, Colorado, USA, from their Web site at https://www.esrl.noaa.gov/psd/ in any documents or publications using these data. We would also appreciate receiving a copy of the relevant publications. This will help PSD to justify keeping the NOAA_ERSST_V5 data set freely available online in the future. Thank you!

```{r, eval = FALSE}
sst_mean <- brick('data/sst.mon.ltm.1981-2010.nc')

#this is for the 24 month map
# sst_jan <- brick('data/sst.mnmean.nc') %>% 
#   .[[517:1978]] %>%
#   .[[seq(1, 1462, 12)]] %>% 
#   `-`(sst_mean[[1]]) %>% crop(extent(c(-1, 359, -75, 75))) %>%
#   disaggregate(fact = 2, method = 'bilinear')

sst_jan <- brick('data/sst.mnmean.nc') %>% 
  .[[505:1966]] %>%
  .[[seq(1, 1462, 12)]] %>% 
  `-`(sst_mean[[1]]) %>% crop(extent(c(-1, 359, -75, 75))) %>%
  disaggregate(fact = 2, method = 'bilinear')

world <- maps::map('world',wrap=c(0,360),fill = TRUE, plot = FALSE)

reof_cor <- brick(calc(sst_jan, fun=function(x) cor(c(x),c(obs_reof$amplitude[,1]))),
calc(sst_jan, fun=function(x) cor(c(x),c(obs_reof$amplitude[,2]))),
calc(sst_jan, fun=function(x) cor(c(x),c(obs_reof$amplitude[,3]))),
calc(sst_jan, fun=function(x) cor(c(x),c(obs_reof$amplitude[,4]))),
calc(sst_jan, fun=function(x) cor(c(x),c(obs_reof$amplitude[,5]))),
calc(sst_jan, fun=function(x) cor(c(x),c(obs_reof$amplitude[,6])))) %>%
  `names<-`(paste0('PC', 1:6))


reof_cor %>% 
  as.data.frame(xy = T, na.rm = T) %>%
  gather(eof, value, PC1:PC6) %>%
    filter(value > .1 | value < -.1) %>%
  ggplot(aes(x, y)) +
  geom_raster(aes(fill = value)) +
  facet_wrap(~eof, nrow = 3) +
  scale_fill_distiller(name = 'Correlation', palette = 'RdBu', direction = -1, limits = c(-.7,.7)) +
  coord_quickmap(ylim = c(-75,75), expand = FALSE) +
  geom_polygon(data = world, aes(x = long, y = lat, group = group)) +
  labs(title = 'Drought teleconnections', 
       subtitle = 'PC correlation to winter sea surface temperatures',
       x = 'Longitude', y = 'Latitude') +
  theme_bw()
ggsave('figures/sst_correlation.pdf')
```

```{r, eval = FALSE}
eof_cor <- brick(calc(sst_jan, fun=function(x) cor(c(x),obs_eof_amp %>% filter(PC == 1) %>% pull(amplitude))),
calc(sst_jan, fun=function(x) cor(c(x),obs_eof_amp %>% filter(PC == 2) %>% pull(amplitude))),
calc(sst_jan, fun=function(x) cor(c(x),obs_eof_amp %>% filter(PC == 3) %>% pull(amplitude))),
calc(sst_jan, fun=function(x) cor(c(x),obs_eof_amp %>% filter(PC == 4) %>% pull(amplitude))),
calc(sst_jan, fun=function(x) cor(c(x),obs_eof_amp %>% filter(PC == 5) %>% pull(amplitude))),
calc(sst_jan, fun=function(x) cor(c(x),obs_eof_amp %>% filter(PC == 6) %>% pull(amplitude))))

eof_cor %>% 
  as.data.frame(xy = T, na.rm = T) %>%
  gather(eof, value, layer.1:layer.6) %>%
  filter(value > .1 | value < -.1) %>%
  ggplot(aes(x, y)) +
  geom_raster(aes(fill = value)) +
  facet_wrap(~eof, nrow = 3) +
  scale_fill_distiller(palette = 'RdBu', direction = -1, limits = c(-.7,.7)) +
  coord_quickmap(ylim = c(-75,75), expand = FALSE) +
  geom_polygon(data = world, aes(x = long, y = lat, group = group)) +
  ggtitle('EOFs') +
  theme_bw()
```



Need to just switch
```{r plot_amplitudes, fig.fullwidth=TRUE, fig.height=4, fig.width=10, echo = FALSE, fig.cap = 'Strong linear correlation between the observed and reconstructed EOFs'}
ggplot(amplitude_modern, aes(amplitude_recon, amplitude_obs)) +
  geom_point(aes(color = year)) +
  geom_smooth(method = 'lm') +
  scale_color_viridis_c() +
  facet_wrap(~PC, nrow = 1) +
  ggtitle('Linear fits between observed and reconstructed amplitudes', '1896-1999') +
  theme_minimal() +
  coord_fixed() +
  theme(legend.position = "bottom")
```

Now let's fit these models in practice. First create a data frame of past amplitudes.
```{r past_amplitudes}
amplitudes_modern <- obs_reof_amp %>%
  inner_join(amplitudes_past, by = c('year', 'PC'),
            suffix = c('_obs', '_recon'))
amplitudes_past <- recon_reof_amp %>%
  rename(amplitude_recon = amplitude) %>%
  group_by(PC) %>%
  nest(.key = 'recons')
```


We can fit gams between the observed and reconstructed PC time series, and then use these to predict the eof amplitudes back in time. Adding in this regression step essentalially bias-corrects the reconstructed amplitudes
```{r recon_amplitudes, eval = FALSE}
recon_amplitudes <- amplitudes_modern %>%
  group_by(PC) %>%
  nest %>%
  mutate(mod = purrr::map(data, ~lm(amplitude_obs ~ amplitude_recon, 
                                     data = .))) %>%
  left_join(amplitudes_past) %>%
  mutate(predictions = purrr::map2(mod, recons, 
                                   ~predict(.x, .y, type = 'response')),
         predictions = purrr::map(predictions, 
                                  ~tibble(recons = .,
                                          amp_smooth = zoo::rollmean(recons, k = 15,fill = NA)))) %>%
  select(PC, recons, predictions) %>%
  unnest %>%
  select(-amplitude_recon) %>%
  mutate(period = floor((year/50)) * 50) 
```

How have the amplitudes of these patterns changed over time? Let's plot the reconstructed time series for each, along with an estimated trend line.
```{r plot_recon_amplitudes, eval = FALSE, fig.width = 10, fig.height = 4, echo = FALSE, fig.fullwidth=TRUE}
ggplot(filter(recon_amplitudes, between(year, 1200, 1450)), aes(year, recons, group = PC)) +
  geom_line(aes(color = PC), alpha = 1) +
  geom_line(aes(y = amp_smooth)) +
  scale_color_brewer(palette = 'Spectral') +
  facet_wrap(~PC) +
  theme_minimal()+
  theme(legend.position = "bottom")+  geom_hline(yintercept = 0, color = 'red', linetype = 2)+  geom_vline(xintercept = 1275, color = 'red', linetype = 2)
```

These results point to REOFs 2 and 3 as having major shocks at around 1300 AD.
```{r, eval = FALSE, fig.margin=TRUE, fig.cap='Are there any time periods with marked signals?'}
recon_amplitudes %>%
  filter(between(period, 1200, 1400)) %>%
  ggplot(aes(factor(period), abs(recons))) +
  geom_boxplot() +
  geom_hline(yintercept = 0, color = 'red', linetype = 2) +
  scale_color_brewer(palette = 'Spectral') +
  facet_wrap(~PC) +
  theme_minimal()

recon_amplitudes %>%
  mutate(PC = paste0('reof',parse_number(PC))) %>%
  rename(time = period) %>%
  filter(between(time, 1200, 1400)) %>%
  group_by(PC, time) %>% 
  summarise(amplitude = mean(abs(recons))) %>%
  mutate(time = as.factor(time)) %>%
  write_csv('output/amplitudes.csv')

recon_amplitudes %>%
  filter(between(period, 1200, 1400)) %>%
  ggplot(aes((recons), as.factor(period),fill = ..x.., height = ..density..)) +
    ggridges::geom_density_ridges_gradient(stat = 'density', scale = 1.5)+
  facet_wrap(~PC) +
  scale_fill_viridis_c(guide = F) +
  geom_vline(xintercept= 0, linetype = 2) +
  ggridges::theme_ridges(grid = FALSE, center_axis_labels = TRUE)
```



```{r reof_raster, warning = FALSE}
reof_raster <- as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>%
  select(x:y) %>%
  cbind(obs_reof$REOF) %>%
  rasterFromXYZ %>%
  `crs<-`('+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0')
eof_raster <- eofs_obs %>%
  select(-std.dev, -column) %>%
  spread(PC, value)  %>%
  rasterFromXYZ %>%
  `crs<-`('+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0')
```


```{r}
writeRaster(reof_raster, 'output/reofs.tif', overwrite = TRUE)
writeRaster(eof_raster, 'output/eofs.tif', overwrite = TRUE)
```


```{r, echo = FALSE, fig.margin = TRUE, fig.caption = 'Climate regionalization.'}
which.max(abs(reof_raster)) %>%
  as.data.frame(xy = T, na.rm = T)  %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = as.ordered(layer))) +
  theme_void() +
    geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  coord_quickmap() +
  theme(legend.position = "bottom")
```
One issue with these reconstructions is that they are sensitive to the number of proxies in the assimilation. That means the variance in srought appears to increase with time, but that this is entirely a function of the proxies. To eliminate this source of bias, we calculate a relationship between the number of proxies and the variance of the time series, and use this to reqeight the time series to remove any spurious trends in the variance.

Read in data on the number of proxies for each time period. Interpolate from the observations to get an approximate estimate of the number of proxies for each year of the calcualtion. In the future, this step can be replace by the actual proxy counts from PHYDA

```{r}
proxies <- read_csv('data/proxy_counts.csv', col_names = FALSE) %>% 
  approx(xout = 5:2001) %>%
  bind_cols %>%
  rename(year = x, count = y)
```

We can see a pretty large increase in the number of proxies during our study period.

```{r, echo = FALSE}
ggplot(proxies, aes(year, count)) +
  geom_vline(aes(xintercept = 1200), alpha = .8, color = 'orange', linetype = 2) +
  geom_vline(aes(xintercept = 1500), alpha = .8, color = 'orange', linetype = 2) +
  geom_line()
```
```{r}
var_mod <- spei_recon %>% 
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, value, 3:902) %>%
  mutate(year = parse_number(year)) %>%
  left_join(proxies) %>%
  mutate(dummy = 0, cell = interaction(x, y)) %>%
  gam(list(value ~ dummy, ~s(count, k = 6)), data = ., family = gaulss)
```

```{r, fig.margin = TRUE, echo = FALSE}
tibble(count = 0:3000, dummy = 0) %>%
  mutate(., sd = 1/predict(var_mod, ., type = 'response')[,2]) %>%
  ggplot(aes(count, sd)) +
  geom_line() +
  ggtitle('Change in SPEI variance due to number of proxies in the reconstruction')
```
need to multiply each year or whatever by the moving average of the ratio of the standard deviations 

so, from the year, predict the number of proxies, then predict the the variance. find the variance at the end, divide by the variances at each time point, then muliply by the original time series.
```{r}
test_weight <- proxies %>%
  mutate(dummy = 0) %>%
  mutate(sd = 1/predict(var_mod, newdata = ., type = 'response')[,2]) %>%
  filter(between(year, 1100, 1999)) %>% 
  mutate(weight = max(sd, na.rm = T) / sd) %>%
  pull(weight)

spei_recon %>% 
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, value, 3:902) %>%
  mutate(year = parse_number(year)) %>%
  ggplot(aes(year, value * test_weight)) +
    scale_y_continuous(limits = c(-5,5)) +
  geom_line(alpha = .1, aes(group = interaction(x, y))) +
  geom_smooth() +
  ggtitle('Standardized Precipitation-Evapotranspiration Index in the American Southwest',
          'June-August, 12 month lag') + 
  theme_bw()
```
