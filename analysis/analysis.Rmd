---
title: "Supplemental Information"
subtitle: 'Analysis scripts for "Hydroclimate variability influence social interaction in the prehistoric American Southwest"'
author: "Nicolas Gauthier"
date: "Last knit on: `r format(Sys.time(), '%d %B, %Y')`"
#bibliography: bibliography.bibtex
output:
  tufte::tufte_handout:
    citation_package: natbib
    toc: true
    #highlight: pygments
    
  tufte::tufte_html:
    tufte_features: ["fonts", "italics"]
    toc: true
link-citations: yes
monofont: courier
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE)
```

# Introduction
\begin{fullwidth}
This RMarkdown document contains all analysis code used to produce the results and figures in the main text. This three-part analysis details the mapping of spatial and temporal hydroclimate patterns, the estimation of prehistoric social networks and travel routes from archaeological data, and the statistical network analysis comparing the archaeological and climatic data.
\end{fullwidth}

## Data sources
This R Markdown script requires raster data of the 100-year [observed Standardized Precipitation-Evapotranspiration Index](https://wrcc.dri.edu/wwdt/time/)^[https://wrcc.dri.edu/wwdt/time/], as well as [reconstructed SPEI maps from the past millennium](https://zenodo.org/record/1198817)^[https://zenodo.org/record/1198817], and the CGIAR version of the SRTM [digital elevation model](http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/)^[http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/], resampled to 250m. Finally, with permission, acquire the [Southwest Social Networks Database](http://www.southwestsocialnetworks.net/data.html)^[http://www.southwestsocialnetworks.net/data.html]. The Southwest Social Networks database contains sensitive site information, so its access is controlled. This analysis provides a preprocessed version of the dataset, with individual sites aggregated to 10km grid cells, to ensure reproducibility.

## Packages
Import the packages needed to reproduce the analysis.
```{r, message = FALSE}
library(raster) # raster data manipulation
library(ncdf4) # netcdf import
library(tidyverse) # data analysis and visualization
library(sf) # spatial data processing
library(philentropy) # entropy measures
library(tidygraph) # network analysis
library(broom) # PCA object handling
library(gdistance) # least-cost paths
library(mgcv) # GAM fitting
#devtools::install_github('nspope/corMLPE')
library(corMLPE) # correlation structure for the GAMM
```

Next import packages needed to reproduce the visualizations.
```{r, message = FALSE}
library(ggraph) # network visualization
library(mgcViz) # GAM plotting
#devtools::install_github('thomasp85/patchwork') 
library(patchwork)
```

# Climate Analysis: Drought and Flood Spatial Patterns

First we estimate robust spatial modes of hydroclimate variability in the Southwest US using present-day observations and reconstructions from the past millennium. We calculate the leading Empirical Orthogonal Functions of a ~100 year series of the summertime average Standardized Precipitation-Evapotranspiration Index. These patterns then undergo a varimax rotation to highlight more physically meaningful spatial patterns. We compare these patterns to those derived from a long term reconstruction based that fuses several climate proxies with outputs from the CESM Last Millennium Ensemble. We confirm that the spatial patterns detected for the past 100 years have been robust over time and explain up to 83% of the variance in a full 1,000 year sequence of reconstructed drought dynamics.

## Study Area

First we define a bounding box covering much of the western United States to constrain the climate analyses, ranging between W124.5$^\circ$ and W-107$^\circ$ and N31$^\circ$ and N37.5$^\circ$. ^[Using a larger study area for the climate analysis allows us to sample a much wider range of climatic variability while still remaining within the broader western US climate zone. This ensures that both A) our statistical analyses will be more robust to sampling error and B) the results will be less sensitive to the exact location and dimensions of our study area.] Also define a smaller area in AZ and NM that will be the focus of the archaeological analysis.
      
```{r}
bbox_wus <- extent(c(-124.5, -102, 30, 42.1))
bbox_swsn <- extent(c(-113, -107, 31, 37.5)) 
```

```{r, echo = FALSE}
state_names <- c('arizona', 'new mexico', 'colorado', 
                 'california', 'utah', 'nevada')

states_wus <- maps::map('state', regions = state_names, 
                    fill = TRUE, plot = FALSE) %>% st_as_sf

states_swsn <- maps::map('state', regions = c('arizona', 'new mexico'), 
                    fill = TRUE, plot = FALSE) %>% st_as_sf

country <- maps::map('usa',fill = TRUE, plot = FALSE) %>% st_as_sf
```


```{r bbox_map, fig.margin=TRUE, echo = FALSE, fig.cap = 'Locations of 2 bounding boxes.'}
ggplot() + 
  geom_sf(data = states_wus) +
  geom_sf(data = country, fill = NA) +
  geom_sf(data = st_as_sfc(st_bbox(bbox_wus, crs = st_crs(4326))), fill = NA, color = 'red') +
  geom_sf(data = st_as_sfc(st_bbox(bbox_swsn, crs = st_crs(4326))), fill = NA, color = 'red') +
  theme_bw()
```

## Climate Data
Import high resolution SPEI maps generated from PRISM data. Calculate the average summertime (JJA) SPEI value for each year.

```{r import_observations, warning=FALSE}
spei_obs <- list.files('data/PRISM', full.names = TRUE) %>%
  map(brick) %>%
  map(crop, bbox_wus) %>%  # crop to the study area bounding box
  reduce(`+`) %>% # average over the 3 months
  `/`(3) %>%
  `names<-`(1895:2017) %>% # add year names
  .[[-1]] # SPEI calculated on 12 month lag, so drop 1st year
```

Import the reconstructed SPEI fields from PHYDA.^[PHYDA uses an off-line data assimilation approach, with simulated SPEI from the CESM LME experiments as physically-consistent model priors and a network of tree rings, ice cores, and corals as assimilated observations. The ensemble Kalman filter uses this information, as well as the spatial covariances from the climate-model prior (e.g. teleconnections), to ``spread out'' information from the point-based proxies.]

```{r import_reconstructions, cache = TRUE}
recon_path <- 'data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc'
spei_recon <- brick(recon_path, varname = 'spei_mn') %>%
   .[[1100:1999]] %>% # extract years of interest
   rotate %>% # rotate longitudes to -180 to 180
   crop(bbox_wus, snap = 'near')
```

```{r, echo = FALSE, fig.margin = TRUE, fig.cap = 'Observed and reconstructed SPEI for summer 1985.'}
brick(c(spei_obs[[90]], resample(spei_recon[[886]], spei_obs, method = 'ngb'))) %>%
  `names<-`(c('Observed', 'Reconstructed')) %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(type, spei, 3:4) %>%
  ggplot(aes(x, y)) +
  geom_raster(aes(fill = spei)) +
  facet_wrap(~type, nrow = 2) +
  scale_fill_distiller(palette = 'Spectral', direction = 1,
                       limits = c(-3, 3),
                       guide = 'legend',
                       name = 'SPEI') +
  coord_quickmap() +
  theme_void()
```

```{r plot_variance_trend, echo = FALSE, message = FALSE, fig.margin=TRUE, fig.cap = 'Plotting the reconstruction time series reveals no clear trend in the mean. It does, however, show a pattern of increasing variance over time, which reflects the increasing number of proxies assimilated over time.'}
spei_recon %>% 
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, value, 3:902) %>%
  mutate(year = str_sub(year, 2),
         year = as.numeric(year)) %>%
  ggplot(aes(year, value)) +
  geom_line(alpha = .1, aes(group = interaction(x, y))) +
  scale_y_continuous(limits = c(-4,4), name = 'SPEI') +
  geom_smooth() +
  theme_bw()
```

## Principal Components Analysis and Empirical Orthogonal Functions

Calculate the (rotated) empirical orthogonal functions of the observed and reoconstructed SPEI data using PCA and varimax rotation. These represent coherent, recurring spatial patterns of drought and flood variability over our study area. First, define a function to reweight the SPEI data based on the latitude to acount for the areal distortion of each cell as latitude changes.

```{r}
area_weight <- function(x){
  names_x <- names(x)
  x %>%
    init('y') %>% # get a map of latitudes
    `*`(pi/180) %>% # convert to radians
    cos %>% # cosine
    sqrt %>%
    `*`(x) %>%
    `names<-`(names_x)
}
```

Now run a principal components analysis on the observed SPEI data. Much of the code that follows is adapted from the *wql* and *sinkr* packages.

```{r}
obs_pca <- spei_obs %>%
  area_weight %>% # reweight for latitudinal distoritions
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  prcomp(scale. = FALSE) # FALSE because SPEI is already normalized
```

Autocorrelation in the observed time series may bias our assessment of the principal components. Define a function to calculate the effective number of observations given the sample autocorrelation. 
```{R}
n_effective <- function(x){
  n <- nlayers(x)
  x %>%
    area_weight %>%
    as.data.frame(na.rm = TRUE) %>%
    t %>%
    as_tibble %>%
    gather(cell, value) %>%
    nest(data = c(value)) %>%
    mutate(rho = map_dbl(data, ~cor(.$value, lag(.$value), use = 'comp')),
           effective_n = n * (1 - rho^2) / (1 + rho^2)) %>% # from Bretherton et al 1999
    summarise(mean(effective_n)) %>%
    pull
}
```

Plot the results of the PCA. For the observations, we see that the leading 6 PCs explain 83% of the variance in the series, so we'll retain those for rotation. Truncating at 4 PCs would also be justified, but because we later rotate these PCs the results are less sensitive to the exact number of PCs retained (see below). 

```{r, echo = FALSE}
obs_eigs <- obs_pca %>%
  tidy(matrix = 'pcs') %>%
  mutate(eigenvalues = std.dev ^ 2,
         error = sqrt(2 / n_effective(spei_obs)),
         low =  eigenvalues * (1 - error) * 100 / sum(eigenvalues),
         hi = eigenvalues * (1 + error) * 100 / sum(eigenvalues),
         cumvar_line = hi + 0.02 * max(hi))
```

```{r, echo = FALSE}
recon_pca <- spei_recon %>%
  area_weight %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  prcomp(scale. = FALSE)

recon_eigs <- recon_pca %>% 
  tidy(matrix = 'pcs') %>%
  mutate(eigenvalues = std.dev ^ 2,
         error = sqrt(2 / n_effective(spei_recon)),
         low =  eigenvalues * (1 - error) * 100 / sum(eigenvalues),
         hi = eigenvalues * (1 + error) * 100 / sum(eigenvalues),
         cumvar_line = hi + 0.02 * max(hi))
```

```{r plot_variance_obs, fig.width = 5, fig.height = 4, echo = FALSE, fig.cap = 'Percent variance explained by the leading principal components of the observed SPEI record. PCs that are not well separated can be considered effective multiplets due to temporal autocorrelation, and should not be split in truncation.'}
obs_eigs %>% 
  mutate(separated = if_else(is.na(lag(low)), TRUE, hi < lag(low)),
                   separated_colors = cumsum(separated),
         weights = if_else(PC < 50, 0, 1))%>%
    filter(PC <= 12) %>%
ggplot(aes(x = PC, y = percent * 100)) +
  geom_errorbar(aes(x = PC, ymin = low, ymax = hi), width = 0.4) +
  geom_point(size = 2, aes(color = as.factor(separated_colors))) + 
  geom_text(aes(x = PC, y = cumvar_line, label = paste0(round(cumulative * 100, 0), '%')), size = 2.5, vjust = 0) +
  labs(x = "Principal Component", y = "Normalized Eigenvalue") + 
  geom_vline(xintercept = 6.5, linetype = 2, color = 'red', alpha = .7) +
  theme_bw() + 
  guides(color = F) + 
  scale_x_continuous(breaks = seq(0, 12, 2))
```


```{r plot_variance_phyda, fig.width = 5, fig.height = 4, fig.margin=TRUE, echo = FALSE, fig.cap = 'Percent variance explained by the principal components of the PHYDA reconstruction.'}
recon_eigs %>%
  mutate(separated = if_else(is.na(lag(low)), TRUE, hi < lag(low)),
          test = cumsum(separated),
         weights = if_else(PC < 3, 0, 1))%>%
  filter(PC <=12) %>%
ggplot(aes(x = PC, y = percent * 100)) +
  geom_errorbar(aes(x = PC, ymin = low, ymax = hi), width = 0.4) +
  geom_point(size = 3, aes(color = as.factor(test))) + 
  geom_text(aes(x = PC, y = cumvar_line, label = round(cumulative * 100, 0)), size = 2.5, vjust = 0) +
  labs(x = "PC", y = "Normalized Eigenvalue") +
  theme_bw() + 
  guides(color = FALSE) + 
  scale_x_continuous(breaks = seq(0, 12, 2))
```

Define a function to calculate the empirical orthogonal functions and rotated empirical orthogonal functions, as well as their associated PC amplitude time series. Save all relevant information in a list for later use.

```{r}
get_EOFs <- function(pc_object, eigs, rast_template, n_modes){
  eofs <- pc_object %>% # calculate unrotated EOFs
    tidy(matrix = 'variables') %>%
    filter(PC <= n_modes) %>%
    left_join(eigs[1:2], by = 'PC') %>%
    mutate(eof = value * std.dev,
           PC = as.character(PC)) %>%
    select(-std.dev, -value) 
  
  varim <- eofs %>% # varimax rotation
    pivot_wider(names_from = PC, values_from = eof) %>%
    column_to_rownames(var = 'column') %>%
    as.matrix %>%
    varimax
  rot_mat <- varim$rotmat # rotater
  
  reofs <- unclass(varim$loadings) %>%
    as_tibble(rownames = 'column') %>%
    pivot_longer(-column, names_to = 'PC', values_to = 'reof') %>%
    right_join(eofs, by = c('column', 'PC'))
  
  eof_amps <- pc_object$x %>%
    .[,1:n_modes] %>%
    scale %>%
    as_tibble(rownames = 'year', .name_repair = ~1:n_modes) %>%
    gather(PC, eof_amp, -year) %>%
    mutate(year = as.numeric(str_sub(year, 2)),
           PC = as.factor(paste0('PC', PC)))
  
  reof_amps <- pc_object$x %>%
    .[,1:n_modes] %>%
    scale %>%
    `%*%`(rot_mat) %>%
    as_tibble(rownames = 'year', .name_repair = ~1:n_modes) %>%
    gather(PC, reof_amp, -year) %>%
    mutate(year = as.numeric(str_sub(year, 2)),
           PC = as.factor(paste0('PC', PC)))
  
  reofs <- rast_template[[1]] %>%
    as.data.frame(xy = TRUE, na.rm = TRUE) %>% 
    .[1:2] %>%
    slice(rep(1:n(), times = n_modes)) %>%
    bind_cols(reofs, .) %>%
    select(-column) %>%
    mutate(PC = paste0('EOF', PC))
  
  list(EOFs = reofs, 
       amps = left_join(eof_amps, reof_amps, by = c('year', 'PC')), 
       rotater = rot_mat)
}
```

Use this function to calculate the EOFs and REOFs for the observations and reconstructions, retaining the 6 leading components in each case for rotation.

```{r calc_eofs}
n_modes <- 6
reof_obs <- get_EOFs(obs_pca, obs_eigs, spei_obs, n_modes)
reof_recon <- get_EOFs(recon_pca, recon_eigs, spei_recon, n_modes)
```

```{r reof_raster, warning = FALSE, echo = FALSE}
#Save the reof data as a raster, so we can crop it to the state boundaries
reof_raster <- reof_obs$EOFs %>%
  select(-eof) %>%
  spread(PC, reof)  %>%
  rasterFromXYZ %>%
  `crs<-`(value = '+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0')%>%
  crop(states_wus) %>%
  mask(states_wus)

# invert reof 6 for better visualization
reof_raster[[6]] <- reof_raster[[6]] * -1
```

```{r, include = FALSE}
writeRaster(reof_raster, '../output/reofs.tif', overwrite = TRUE)
```

```{r, include = FALSE, warning = FALSE}
eof_raster <- reof_obs$EOFs %>%
  select(-reof) %>%
  spread(PC, eof)  %>%
  rasterFromXYZ %>%
  `crs<-`(value = '+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0')%>%
  crop(states_wus) %>%
  mask(states_wus)
```

```{r, echo = FALSE, fig.margin = TRUE, fig.cap='Observed EOFs before rotation.'}
as.data.frame(eof_raster, xy = TRUE, na.rm = TRUE, long= TRUE) %>%
ggplot() +
  facet_wrap(~layer, ncol = 2) +
  geom_raster(aes(x, y, fill = value)) +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-.82, .82)) +
  theme_void() + 
    geom_sf(data = states_wus, color = 'black', fill = NA) +
  coord_sf(datum = NA) +
  theme(legend.position = "bottom")
```

```{r, echo = FALSE, fig.margin = TRUE, fig.cap='Reconstructed EOFs before rotation.'}
ggplot(reof_recon$EOFs, aes(x, y, fill = eof)) +
  facet_wrap(~PC) +
  geom_raster() +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-.72, .72)) +
  theme_void() + 
  coord_quickmap()
```

Map the resulting spatial patterns. The colors in these REOF maps are eigenvectors weighted by the sqare root of the associated eigenvalue, so these loadings represent the covariance between each grid cell and each amplitude (principal component). Visual comparison of the observed and reconstructed REOFs show they correspond to the same physical phenomena. Thus we can use the observed patterns as the high resolution patterns, and infer their temporal evolution from the reconstructions. The next section focuses on these amplitude time series, and we'll confirm the correlations between the observed and reconstructed patterns there.

```{r plot_obs_reof, echo = FALSE, fig.fullwidth = FALSE, fig.width = 6, fig.height = 7, fig.cap = 'Observed drought REOFs'}
reof_raster %>%
  as.data.frame(xy = TRUE, na.rm = TRUE, long = TRUE) %>%
ggplot() +
  geom_raster(aes(x, y, fill = value)) +
  scale_fill_viridis_c(option = 'magma', name = 'Correlation') +
  facet_wrap(~layer, ncol = 2) +
  theme_void() +
  geom_sf(data = states_wus, color = 'black', fill = NA) +
  coord_sf(datum = NA) +
  theme(legend.position = "bottom")

ggsave('../figures/reof_observed.pdf', height = 7, width = 6)
```

```{r, echo = FALSE}
## the reconstructed and observed REOFs/PCs don't come in the same order, this corrects that
reof_recon$amps <- reof_recon$amps %>%
  mutate(PC = case_when(PC == 'PC1' ~ 'PC2',
                        PC == 'PC2' ~ 'PC4',
                         PC == 'PC3' ~ 'PC5',
                         PC == 'PC4' ~ 'PC3',
                         PC == 'PC5' ~ 'PC1',
                         PC == 'PC6' ~ 'PC6')) %>%
  arrange(PC) %>%
  mutate(reof_amp = if_else(PC %in% c('PC2', 'PC5', 'PC6'), reof_amp * -1, reof_amp))

reof_recon$EOFs <- reof_recon$EOFs %>%
  mutate(PC = case_when(PC == 'EOF1' ~ 'EOF2',
                        PC == 'EOF2' ~ 'EOF4',
                         PC == 'EOF3' ~ 'EOF5',
                         PC == 'EOF4' ~ 'EOF3',
                         PC == 'EOF5' ~ 'EOF1',
                         PC == 'EOF6' ~ 'EOF6')) %>%
  arrange(PC)
```

```{r plot_recon_reofs, fig.fullwidth = FALSE, echo = FALSE, fig.cap = 'Reconstructed drought REOFs'}
reof_recon$EOFs %>%
  mutate(reof = if_else(PC %in% c('EOF2', 'EOF5', 'EOF6'), reof * -1, reof)) %>%
  ggplot() +
  geom_raster(aes(x,y,fill = reof)) +
  scale_fill_viridis_c(option = 'magma', name = 'Correlation') +
  facet_wrap(~PC) +
  coord_sf(datum = NA) +
  theme_void()
```



```{r, echo = FALSE, fig.margin = TRUE, fig.cap = 'Climate regionalization showing the dominant REOF for each point in space.'}
which.max(abs(reof_raster)) %>%
  as.data.frame(xy = T, na.rm = T)  %>%
  ggplot() +
  geom_raster(aes(x,y, fill = as.ordered(layer))) +
  scale_fill_viridis_d(name = '') +
  theme_void() +
  geom_sf(data = states_wus, color = 'black', fill = NA) +
  coord_sf(datum = NA)
```

## Drought Amplitudes


```{r, echo = FALSE, include=FALSE}
# Not shown.
# For each PC amplitude time series, plot out the wettest and driest years from the observations.
# From this we can tell that all but PC6 is of the right sign (high pc amplitude == high positive SPEI). 
# So next we multiply PC6 by -1 
reof_obs$amps %>%
  group_by(PC) %>%
  filter(reof_amp == max(reof_amp) | reof_amp == min(reof_amp)) %>%
  pull(year) %>%
  `-`(1895) %>%
  spei_obs[[.]] %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, SPEI, X1915:X2008) %>%
  mutate(year = factor(year, levels = paste0('X', c(1915, 1996 ,1987, 2011, 1934, 1984, 2014, 2017, 1947, 1963, 1977, 2008)))) %>%
  ggplot(aes(x, y, fill = SPEI)) +
  geom_raster() +
  facet_wrap(~year) +
  coord_quickmap() +
  scale_fill_distiller(palette = 'Spectral', limits = c(NA, 4), direction = 1) +
  theme_void()
```

```{r, echo = FALSE}
reof_obs$amps <- reof_obs$amps %>%
  mutate(reof_amp = if_else(PC == 'PC6', reof_amp * -1, reof_amp))
```


```{r echo = FALSE, fig.width=6, fig.height=4, warning = FALSE}
reof_obs$amps %>%
  group_by(PC) %>%
  mutate(amp_smooth = zoo::rollmeanr(reof_amp, k = 10, fill = NA)) %>%
ggplot(aes(year, reof_amp, color = PC)) +
    geom_line(alpha = .4) +
  geom_line(aes(y = amp_smooth), size = 1.2) +
  facet_wrap(~PC) + 
  geom_hline(yintercept = 0, linetype = 2, alpha= 1, color = 'black') + 
  scale_color_viridis_d(guide = FALSE) +
  theme_bw() +
  labs(x = 'Year', y = 'SPEI')

ggsave('../figures/pc_obs.pdf', width = 6, height = 4)
```

```{r, echo = FALSE, fig.margin = TRUE}
reof_recon$amps %>%
  filter(year >= 1896) %>%
  group_by(PC) %>%
  mutate(amp_smooth = zoo::rollmeanr(reof_amp, k = 10, fill = NA)) %>%
ggplot(aes(year, reof_amp, color = PC)) +
  geom_line(alpha = .4) +
  geom_line(aes(y = amp_smooth), size = 1.2) +
  facet_wrap(~PC) + 
  geom_hline(yintercept = 0, linetype = 2, alpha= 1, color = 'black') + 
  scale_color_viridis_d(guide = FALSE) +
  theme_bw() +
  labs(x = 'Year', y = 'SPEI')
```


Now let's fit these models in practice. First create a data frame of past amplitudes.
```{r past_amplitudes}
amplitudes_modern <- reof_obs$amps %>%
  inner_join(reof_recon$amps, by = c('year', 'PC'),
            suffix = c('_obs', '_recon'))
amplitudes_past <- reof_recon$amps %>%
  select(-reof_amp) %>%
  rename(amplitude_recon = eof_amp) %>%
  group_by(PC) %>%
  nest(recons = c(year, amplitude_recon))
```

Plot the time series (PCs) of each observed and reconstructed REOF against each other, to confirm that they correspond to the same patterns. Look at the correlations between the modes.
```{r, plot_amplitudes, fig.fullwidth=TRUE, fig.height=4, fig.width=10, echo = FALSE, fig.cap = 'Strong linear correlation between the observed and reconstructed EOFs'}
#do the eofs match?
ggplot(amplitudes_modern, aes(reof_amp_recon, reof_amp_obs)) +
  geom_point(aes(color = year)) +
  geom_smooth(method = 'lm') +
  scale_color_viridis_c() +
  facet_wrap(~PC, nrow = 1) +
  labs(title = 'Linear fits between observed and reconstructed amplitudes', subtitle = '1896-1999',
       x = 'Reconstructed', y = 'Observed') +
  theme_minimal() +
  coord_fixed() +
  theme(legend.position = 'bottom')
```

How have the amplitudes of these patterns changed over time? Let's plot the reconstructed time series for each, along with an estimated trend line.
```{r plot_recon_amplitudes, fig.width = 9, fig.height = 4, echo = FALSE, fig.fullwidth=TRUE}
reof_recon$amps %>%
    mutate(amp_smooth = zoo::rollmeanr(reof_amp, k = 10, fill = NA)) %>%
  filter(between(year, 1200, 1450)) %>% 
ggplot(aes(year, reof_amp, group = PC)) +
  geom_line(aes(color = PC), alpha = .3) +
  geom_line(aes(y = amp_smooth, color = PC), size = 1.2) +
  scale_color_viridis_d(guide = FALSE) +
  facet_wrap(~PC) +
  theme_bw()+
  theme(legend.position = "bottom") +  geom_hline(yintercept = 0, color = 'black', linetype = 2)+  geom_vline(xintercept = 1275, color = 'red', linetype = 2) +
  labs(x = 'Year', y = 'SPEI (Reconstructed)')
```

These results point to REOFs 2 and 3 as having major shocks at around 1300 AD.
```{r, echo = FALSE, fig.margin=TRUE, fig.cap='Are there any time periods with marked signals?'}
reof_recon$amps %>%
  mutate(period = floor((year / 50)) * 50) %>%
  filter(between(period, 1200, 1400)) %>%
  ggplot(aes((reof_amp), as.factor(period),fill = ..x.., height = ..density..)) +
  ggridges::geom_density_ridges_gradient(stat = 'density', scale = 1.5)+
  facet_wrap(~PC) +
  scale_fill_viridis_c(guide = F) +
  geom_vline(xintercept= 0, linetype = 2) +
  ggridges::theme_ridges(grid = FALSE, center_axis_labels = TRUE)
```


```{r, echo = FALSE, message=FALSE, warning = FALSE}
rm(spei_obs, spei_recon, obs_pca);invisible(gc())
```

# Network Analysis: Spatial Interaction Modeling

First generate an estimate of the prehistoric social network using archaeological proxies from the Southest Social Networks project.^[Go to http://www.southwestsocialnetworks.net for more information on this project and dataset] First we read in a csv file containing information for all of the archaeological sites in the database, including spatial location and number of rooms present from each time period. These will become the node-level data for the network.

```{r}
crs_utm <- '+proj=utm +zone=12 +datum=NAD27'
```

```{r, message = FALSE}
sites <- read_csv('data/attributes_orig.csv') %>%
  st_as_sf(coords = c('EASTING', 'NORTHING'), crs = crs_utm) %>%
  st_transform('+proj=longlat +datum=WGS84')
```

Next we get the apportioned ceramic ware counts per site.
```{r}
load('ware_matrices')
wares <- list(AD1200cer, AD1250cer, AD1300cer, 
              AD1350cer, AD1400cer) %>%
  map(rownames_to_column) %>%
  bind_rows(.id = 'time') %>%
  mutate(time = as.numeric(time) * 50 + 1150) %>%
  rename(site = rowname)
```

```{r, fig.fullwidth = TRUE, echo = FALSE, fig.height = 4, fig.width = 6}
wares %>%
  gather(ware, count, 3:38) %>%
  group_by(ware, time) %>%
  summarise(Proportion = sum(count)) %>%
ggplot(aes(time, Proportion, fill = ware, group = ware)) +
  geom_bar(position='fill', stat = 'identity', color = 'black') +
  theme_bw()
```
Next we aggregate the sites into 10x10km patches. This is because an area of about 10 km radius from a site is the limit of raw material resources for ceramics (TODO cite Arnold1985). So 5km is roughly the limit of day to day social interaction, 10km is raw material procurement for ceramics, and 18 km is for day's journey round trip (TODO cite varien). translated to square units in equivalent areas, that's 3km square cells for regular face-to-face interaction and intensive agriculture. 12km for food and non food resources, and 32km grids for day's trip there and back. So 12 is the most reasonable. First import and crop the raster we'll use as a reference.
```{r}
elev <- raster('~/gdrive/Data/SRTM_W_250m_TIF/SRTM_W_250m.tif') %>% # import the SRTM DEM
  crop(bbox_swsn) %>% # crop to study area
  aggregate(fact = 40) # aggregate to 10km resolution
```

```{r, echo = FALSE}
#from https://stackoverflow.com/a/46846474
calculate_mode <- function(x) {
  uniqx <- unique(x)
  uniqx[which.max(tabulate(match(x, uniqx)))]
}
```

```{r}
patches <- sites %>%
  mutate(., patch_id = as.character(cellFromXY(elev, st_coordinates(.)))) %>%
  gather(period, rooms, P1room:P5room) %>%
  mutate(time = parse_number(period) * 50 + 1150) %>%
  group_by(patch_id, time) %>%
  summarise(Macro = calculate_mode(Macro),
            site = list(SWSN_Site),
            rooms = sum(rooms)) %>%
  ungroup %>%
  spread(time, rooms) %>%
  cbind(., xyFromCell(elev, as.numeric(.$patch_id))) %>%
  as.data.frame %>%
  st_as_sf(coords = c('x','y'), crs = 4326, remove = FALSE)
```

```{r}
theo_max <- -.5 * log(.5) - .5 * log(.5) # maximum theoretical divergence given equal weights
```

```{r}
getJSD <- function(data, time){
  patch_ids <- pull(data, patch_id)
  data %>%
    select(-patch_id) %>%
    select_if(~!all(. == 0)) %>%
    as.matrix %>%
    philentropy::distance(method = 'jensen-shannon', est.prob = 'empirical') %>%
    `/`(., theo_max) %>%
    sqrt %>%
    `-`(1,.) %>%
    `rownames<-`(patch_ids) %>%
    `colnames<-`(patch_ids) %>%
    replace(. == 0, 999) %>% # replace 0 values with 999 temporarily
    as_tbl_graph %E>% # convert to directed graph
    rename(JSD = weight) %>%
    mutate(JSD = if_else(JSD == 999, 0, JSD)) %>% # convert 999 values back to 0
    mutate(time = time) %>% # set the time period
    filter(!edge_is_loop()) %>%
  activate(nodes) 
}
```

```{r, message = FALSE}
swsn <- patches %>%
  gather(time, rooms, X1200:X1400) %>%
  mutate(time = parse_number(time)) %>%
  as.data.frame %>%
  select(patch_id, site, time) %>%
  unnest(cols = c(site)) %>%
  right_join(wares, by = c('site', 'time')) %>%
  as.data.frame %>%
  select(-site) %>%
  group_by(time, patch_id) %>%
  summarise_all(sum, na.rm = T) %>%
  nest %>%
  mutate(net = map2(data, time, getJSD)) %>%
  pull(net) %>%
  reduce(graph_join, by = 'name') %>%
  left_join(patches, by = c('name' = 'patch_id'))
```

```{r, echo = FALSE, fig.fullwidth = TRUE}
swsn %N>%
  mutate(Rooms = X1200 + X1250 + X1300 + X1350 + X1400) %>%
  arrange(Rooms) %>%
  ggraph('manual', x = x, y = y) +
      geom_sf(data = states_swsn, fill = NA, color = 'black') +
  geom_node_point(aes(size = Rooms, color = Rooms)) +
  scale_size_area(limits = c(0, 3000), breaks = seq(0, 3000, by = 500)) +
  scale_color_viridis(option = 'magma', limits = c(0, 3000), breaks = seq(0, 3000, by = 500), guide = 'legend') +
  coord_sf(datum = NA) +
  theme_void()  +
  theme(legend.position = c(1, 0), legend.justification = c(2.6,-.3))
```


```{r fig.fullwidth = TRUE, fig.width = 9, fig.height = 6, echo = FALSE}
swsn %E>% 
    filter(JSD > .001) %>%
    filter(from < to) %>%
    arrange(JSD) %>% 
ggraph('manual', x = x, y = y) +
  geom_edge_fan(aes( color = JSD)) +
  geom_sf(data = states_swsn, fill = NA, color = 'black') +
  facet_edges(~time, ncol = 3) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(name = 'Similarity', option = 'magma') +
  coord_sf(datum = NA) +
  theme_void() +
  theme(legend.position = c(1, 0), legend.justification = c(2.5,0))
```

```{r, echo=FALSE, fig.margin = TRUE, fig.width=4, fig.height = 8}
swsn %E>% 
    filter(JSD > .001) %>%
    filter(from < to) %>%
    mutate(JSD = qlogis(if_else(JSD > .999, .999, JSD))) %>%
    arrange(JSD) %>% 
ggraph('manual', x = x, y = y) +
  geom_edge_fan(aes( color = JSD)) +
  geom_sf(data = states_swsn, fill = NA, color = 'black') +
  facet_edges(~time, ncol = 2) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(name = 'Similarity', option = 'magma') +
  coord_sf(datum = NA) +
  theme_void() +
  theme(legend.position = c(1, 0), legend.justification = c(2.5,0))
```


```{r, fig.width= 10, fig.height = 5, fig.fullwidth = TRUE, echo = FALSE}
a <- swsn %N>%
  mutate(Rooms = X1200 + X1250 + X1300 + X1350 + X1400) %>%
  arrange(Rooms) %>%
  ggraph('manual', x = x, y = y) +
      geom_sf(data = states_swsn, fill = NA, color = 'black') +
  geom_node_point(aes(size = Rooms, color = Rooms)) +
  scale_size_area(limits = c(0, 3000), breaks = seq(0, 3000, by = 500)) +
  scale_color_viridis(option = 'magma', limits = c(0, 3000), breaks = seq(0, 3000, by = 500), guide = 'legend') +
  coord_sf(datum = NA) +
  theme_void()  +
  theme(legend.position = c(1, 0), legend.justification = c(2.6,-.3))

b <- swsn %E>% 
    filter(JSD > .001) %>%
    filter(from < to) %>%
    arrange(JSD) %>% 
ggraph('manual', x = x, y = y) +
  geom_edge_fan(aes( color = JSD)) +
  geom_sf(data = states_swsn, fill = NA, color = 'black') +
  facet_edges(~time, ncol = 2) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(name = 'Similarity', option = 'magma') +
  coord_sf(datum = NA) +
  theme_void() +
  theme(legend.position = c(1, 0), legend.justification = c(1.25, -1), legend.direction = 'horizontal')

c <- a + b + plot_layout(heights = c(1, 1)) + plot_annotation(tag_levels = 'A')
ggsave('swsn2.png', plot = c, height = 5, width = 10)
```

Comparing the two approaches show that the using the coarse grained patches approach reduces the number of long distance strong connections. This is likely do to the fact htat we're removing the impact of small sites with only one or two ware types, which can introduce spurious high correlations.

# Terrain Analysis: Least Cost Distances

Next we calculate the impact of rugged terrain on the potential flow of people and information between the sites in the SWSN database. For this we need to use the SRTM digital elevation model. From the height data in the DEM, we calculate slope, accounting for cognitive biases people have when assessing the steepness of high slopes (people tend to exaggerate slopes above a certain threshold). From this map of perceived slope, we calculate "perceived walking speeds", using Tobler's hiking function. We uses these perceived walking speeds as a measure of the perceived, symmetric costs of traveling between two locations on the landscape, and from these estimate the least cost paths from every site to every site. From there, we extract the 15 nearest neighbors of each site in each time period, and use this nearest neighbor network as a spatial network, from which to estimate new travel costs (limited to paths along this nearest neighbor network).

We focus on the perceived time costs of travel. People will consistently overestimate slopes, so the perceived cognitive costs of moving across the landscape will be greater. In rugged, mountainous terrain, slope is the key factor in influencing ease of travel. We focus specifically on paths, or the physical routes that people traveled. A good path connecting two places has to optimize for ease of travel in both directions. The resulting least-cost network will be used as a proxy measure for the constraints on moving both people and bulk goods across the landscape, and thus the topographic affordances for social exchange.

## Digital Elevation Model

First import the elevation dataset, crop to the study area, and resample.

```{r elevation}
elev_lcp <- raster('~/gdrive/Data/SRTM_W_250m_TIF/SRTM_W_250m.tif') %>% # import the SRTM DEM
  crop(bbox_swsn) %>% # crop to study area
  aggregate(fact = 2) # aggregate to ~500m resolution
```

## Hiking Speeds

Now we use the package *gdistance* to calculate the time it would take for a hiker to traverse this landscape on foot. Tobler's hiking function allows us to convert terrain slope to expected walking speed. We modify this function to account for cognitive bias in the perception of slope, which causes people to overestimate the steepness of steeper slopes. 

The maximum walking speed of 5 assumes on path movement.
```{r}
tobler_adjusted <- function(x){
  4 * exp(-3.5 * 2.15 * x) / 3.6 # 3.6 turns km/h into m/s
}
```

```{r, fig.margin = TRUE, echo = FALSE, fig.cap = "Tobler's hiking function"}
slope <- seq(-1, 1, .01)
qplot(slope, tobler_adjusted(abs(slope)) * 3.6, geom = 'line' ) +
  geom_vline(xintercept = 0, linetype = 2) +
  labs(x = 'Slope (rise over run)', y = 'Walking speed (km/h)') +
  theme_classic()
```

This package uses a transition matrix approach, whereby we work with matrices that contain the costs or transmissiveness of travel from each cell to its immediate 16 neighbor cells. This allows for a sparse matrix representation, because there is no possible connection to non neighboring cells, which considerably reduces the computational burden.

We're representing paths, not necessarily routes, and so focus on symmetric costs. This is an additional modification to tobler.

Calculate the absolute difference in elevation between each cell and its 16 neighbors.
```{r, cache = TRUE}
altDiff <- function(x){abs(x[2] - x[1])}
hd <- transition(elev_lcp, altDiff, 16, symm = TRUE)
```
Divide the height differences by the horizontal distances between cells, resulting in slopes.
```{r, cache = TRUE}
slope_c <- geoCorrection(hd, type = 'c')
```
Figure out which cells are adjacent to one another, queen's case.
```{r, cache = TRUE}
adj <- adjacent(elev_lcp, cells = 1:ncell(elev_lcp), directions = 16)
```
Use Tobler's hiking function to calculate walking speed from *cognitive* slope.
```{r, cache = TRUE}
speed_c <- slope_c
speed_c[adj] <- tobler_adjusted(slope_c[adj])
```
Divide by intercell distance again, resulting in the conductance matrix.
```{r, cache = TRUE}
conductance_c <- geoCorrection(speed_c, type = 'c')
```

```{r, echo = FALSE, fig.height = 6, fig.width = 4, fig.fullwidth = FALSE, fig.cap = 'Conductance map.'}
speed_c %>%
  raster %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
ggplot(aes(x, y, fill = layer * 3.6)) + # convert back to km/h
  geom_raster() +
  scale_fill_viridis_c(guide = 'legend', name = 'Walking\nspeed (km/h)', limits = c(NA,4)) +
  coord_quickmap() +
  labs(x = 'Longitude', y = 'Latitude') +
  theme_minimal()
```

```{r, echo = FALSE, message=FALSE, warning = FALSE}
rm(altDiff, hd, slope_c, speed_c, adj);invisible(gc())
```

## Least-cost Distances

Using the conductance matrix, calculate the full pairwise least cost distance matrix.
The points of sites are originally in utm zone 12, so reproject to latlong

```{r least-cost-distances, cache = TRUE}
distances <- swsn %>%
  as_tibble %>%
  st_as_sf %>%
  st_coordinates%>%
  costDistance(conductance_c, .) %>% # least cost distances
  as.matrix %>% 
  `colnames<-`(as_tibble(swsn)$name) %>%
  as_tibble %>%
  mutate(from_patch = as_tibble(swsn)$name) %>%
  gather(to_patch, distance, -from_patch) %>%
  mutate(distance = distance / 3600)
```


# Network analysis


Import the data produced in the other analyses to add to the sites data.
```{r}
reof_resamp <- reof_raster %>%
  crop(bbox_swsn) %>%
  resample(elev)
```

```{r, fig.margin = TRUE, echo = FALSE}
plot(reof_resamp, )
```

```{r, fig.margin = TRUE, echo = FALSE}
pairs(reof_resamp)
```


## Data Preparation

Now that we have all the data imported and preprocessed, let's put it all together into a data frame.

Define a function to calculate the relative difference between two numbers. We use this becuase it is a measure of multiplicative distance, rather than additive such as euclidean distance, which makes the values more robust to deviations.

```{r data_preparation, warning = FALSE}
swsn2 <- swsn %N>%
  mutate(reof1 = raster::extract(reof_resamp, st_coordinates(geometry))[,1],
         reof2 = raster::extract(reof_resamp, st_coordinates(geometry))[,2],
         reof3 = raster::extract(reof_resamp, st_coordinates(geometry))[,3],
         reof4 = raster::extract(reof_resamp, st_coordinates(geometry))[,4],
         reof5 = raster::extract(reof_resamp, st_coordinates(geometry))[,5],
         reof6 = raster::extract(reof_resamp, st_coordinates(geometry))[,6]) %E>%
  mutate(from_patch = .N()$name[from],
         to_patch = .N()$name[to]) %>%
  left_join(distances, by = c('from_patch', 'to_patch')) %>%
  mutate(reof1 = abs(.N()$reof1[from] - .N()$reof1[to]),
         reof2 = abs(.N()$reof2[from] - .N()$reof2[to]),
         reof3 = abs(.N()$reof3[from] - .N()$reof3[to]),
         reof4 = abs(.N()$reof4[from] - .N()$reof4[to]),
         reof5 = abs(.N()$reof5[from] - .N()$reof5[to]),
         reof6 = abs(.N()$reof6[from] - .N()$reof6[to])) %>%
  mutate(from_tmp = pmin(from, to), to_tmp = pmax(from, to)) %>%
  group_by(from_tmp, to_tmp, time) %>%
  sample_n(1) %>%
  ungroup %>%
  select(-from_tmp, -to_tmp) %>%
  convert(to_undirected)
```

```{r}
dat <- swsn2 %E>%
  as_tibble %>%
  mutate(JSD = if_else(JSD > .999, .999, JSD)) %>%
  mutate(from = as.factor(from), 
         to = as.factor(to), 
         time_fact = as.factor(time)) %>%
  filter(JSD >= .001)
```


jensen inequality, E(logit(y)) >=logit(E(y)). this means we should interpret specific predictions and confidence intervals from this model with caution, and instead only focus on interpretaiton of the brod functional forms. Its a necessary evil here, becuase we do not yet have the computational abiliy to fit the corMLPE correlation structure and a beta family. Preliminary tests with the data suggested that the pairwise correlation structure was a much alrger source of bias then transforming the response variabble,so that's the tradeoffwe went with. The bias from transforming the respone leads to bias in the estimates of variance.

 allows the model to be estimated additively.
 
```{r, echo= FALSE}
#this room plot shows a process of aggregation over time (mean room size increases)
ggplot(dat, aes(JSD)) +
  geom_histogram(bins = 30) + facet_wrap(~time)
# look at the plot of ws_mul, basically its all the links with low productivity that die out and the ones with high that survive!
```

```{r, echo = FALSE, fig.margin = TRUE}
ggplot(dat, aes(qlogis(JSD))) +
  geom_histogram(bins = 30) + facet_wrap(~time)
```


 
 Pairwise data are high powered, which is makes goodness of fit estimates are preferable over statistical significance testing, and we must be careful to properly estimate confidence intervals,

```{r}
fit_gam_JSD <- function(x){
  gamm(x, 
      method = 'ML', 
      family = gaussian(),
      correlation = corMLPE(form = ~from + to|time_fact),
      data = dat)
}
```


```{r gam-aic, cache = TRUE}
models <- c(
  "s(distance, bs = 'cr', k = 6) + time_fact",
  "s(distance, bs = 'cr', k = 6) +
   s(reof1, by = time_fact, bs = 'cs', k = 6) +
   s(reof2, by = time_fact, bs = 'cs', k = 6) +
   s(reof3, by = time_fact, bs = 'cs', k = 6) +
   s(reof4,  by = time_fact, bs = 'cs', k = 6) +
   s(reof5, by = time_fact, bs = 'cs', k = 6) +
   s(reof6, by = time_fact, bs = 'cs', k = 6) + time_fact"
) %>%
  paste('qlogis(JSD) ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(
    formula = map(formula_chr, as.formula),
    model = map(formula, fit_gam_JSD)
  ) %>%
  mutate(
    lme = map(model, ~ .$lme),
    gam = map(model, ~ .$gam),
    aic = map_dbl(lme, AIC)
  ) %>%
  arrange(aic)
```

```{r}
models
```
This may suggest areas of increased conflict, where the kind of social interaction through which pottery would have been exchanged would be rare, or other unmodeled costs of travel such as river crossings cultural taboos. 
```{r, eval = FALSE}
gam.check(models$gam[[1]])
```

We have moderate to strong concurvity
```{r, eval = FALSE}
concurvity(models$gam[[1]])
```

This represents that multiple transmission processes, with different relationships to distance, are acting together to generate the variation in the archaeological record. The two main inflection points in the decay curve are both consistent with previous work in the ecology of human mobility. Their is a plateau effect distance plays no appreciable role in interaction between sites within a day's walk of one another (johnson but cites olsson 1965 and crumley 1979, ariadne). This suggests that distance only becomes a deterring factor dealing in multiday trips. The second inflection point, at roughly 10 days is consistent with estimates of area needed to sustain the minimum viable breeding population for human groups (are they consistent with the figures of 50km-80km from rautman and drennen and others? johnson finds no marriages past 60km, maize to chaco papers also have distance).
Plateau of social interaction with communities who have daily face-to-face contact.

uses log transform
The Mogollon Rim acts as a severe bottleneck for spatial flows, with paths generally constrained to one of the three river systems, the Verde, Salt, and Gila, act to channel flow across this boundary. 

```{r dist-gam, cache = TRUE}
m1 <- gamm(qlogis(JSD) ~ s(distance, bs = 'cr', k = 6) + time_fact,
           method = 'REML', 
           family = gaussian(),
           correlation = corMLPE(form = ~from + to|time_fact),
           data = dat)
```

```{r eof-gam, cache = TRUE}
m2 <- gamm(qlogis(JSD) ~ s(distance, bs = 'cr', k = 6) +
             s(reof1, by = time_fact, bs = 'cs', k = 6) +
             s(reof2, by = time_fact, bs = 'cs', k = 6) +
             s(reof3, by = time_fact, bs = 'cs', k = 6) +
             s(reof4,  by = time_fact, bs = 'cs', k = 6) +
             s(reof5, by = time_fact, bs = 'cs', k = 6) +
             s(reof6, by = time_fact, bs = 'cs', k = 6) + time_fact,
           method = 'REML', 
           family = gaussian(),
           correlation = corMLPE(form = ~from + to|time_fact),
           data = dat)
```

```{r}
summary(m2$gam)
```

```{r, eval = FALSE}
gam.check(m2$gam)
concurvity(m2$gam)
```

```{r, echo = FALSE}
m1$gam %>% 
  getViz %>% 
  sm(1) %>%  
  plot(trans = plogis) + 
  geom_vline(xintercept = c(12, 100), linetype = 2) +
  l_ciPoly(mul = 5) + l_fitLine() + 
  scale_y_continuous(limits = c(0,1)) + 
  labs(x = 'Distance (hours)', y = 'Information flow')

ggsave('../figures/distance_function.pdf', width = 6, height = 4)
```


```{r fig.height = 6, fig.width = 6, echo = FALSE}
fun_dat <- bind_rows(
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1 = seq(0, .5, .01), reof2 = 0, reof3 = 0, reof4 =0, reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1 = 0, reof2 = seq(0, .5, .01), reof3 = 0, reof4 = 0, reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=seq(0,.5,.01), reof4=0, reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=0, reof4=seq(0,.5,.01), reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=0, reof4=0, reof5 = seq(0,.5,.01), reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=0, reof4=0, reof5 = 0, reof6 = seq(0,.5,.01)), .id = 'reof') %>%
  mutate(reof = paste0('REOF', reof))


m2$gam %>%
  predict.gam(newdata = fun_dat, type = 'response', se.fit = T) %>% 
  bind_cols %>%
  mutate(lower = fit - 2 * se.fit, upper = fit + 2 * se.fit) %>%
  mutate_at(vars(fit,lower, upper), plogis) %>% 
  bind_cols(fun_dat) %>%
  gather(reofs, value,reof1:reof6) %>%
  filter(parse_number(reofs) == parse_number(reof)) %>%
  mutate(reof = str_sub(reof, 2)) %>%
  ggplot(aes(value, fit, group = time_fact)) +  
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = reof), alpha = .3) +
  geom_line(aes(color = reof), size = 1.2) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0,1)) +
  scale_x_continuous(breaks = c(0, .5)) +
  theme_bw() +
  scale_color_viridis_d(guide = 'none') +
  scale_fill_viridis_d(guide = 'none') +
  facet_grid(reof ~ as.ordered(time_fact)) +
  theme_bw() +
  labs(x = 'Climatic Difference', y = 'Cultural Similarity')

ggsave('../figures/smooths.pdf', width = 6, height = 6)
```
 
 
```{r}
knitr::knit_exit()
```

how do the fits change over time?
```{r}
fit_gam_JSD <- function(x){
  gamm(x, method = 'ML', 
            family = gaussian(),
      correlation = corMLPE(form = ~from + to|time_fact),
      data = dat)
}
plan(multicore, workers = 2)

models <- c(
 "s(distance, bs = 'cr', k = 6) + time_fact",
       "s(distance, bs = 'cr', k = 6) +
          s(reof1, by = time_fact, bs = 'cs', k = 6) +
          s(reof2, by = time_fact, bs = 'cs', k = 6) +
       s(reof3, by = time_fact, bs = 'cs', k = 6) +
       s(reof4,  by = time_fact, bs = 'cs', k = 6) +
      s(reof5, by = time_fact, bs = 'cs', k = 6) +
       s(reof6, by = time_fact, bs = 'cs', k = 6) + time_fact"
   ) %>%
  paste('qlogis(JSD) ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = future_map(formula, fit_gam_JSD)) %>%
  mutate(lme = purrr::map(model, ~.$lme),
         gam = purrr::map(model, ~.$gam),
         aic = map_dbl(lme, AIC)) %>%
  arrange(aic)


t1 <- dat %>% group_by(time) %>% nest %>%
  mutate(dist_mod = map(data, ~gamm(qlogis(JSD) ~ s(distance, bs = 'cr', k = 6), method = 'ML', 
            family = gaussian(),
      correlation = corMLPE(form = ~from + to),
      data = .))) %>%
  mutate(reof_mod = map(data, ~gamm(qlogis(JSD) ~ s(distance, bs = 'cr', k = 6) +
          s(reof1, bs = 'cs', k = 6) +
          s(reof2,bs = 'cs', k = 6) +
       s(reof3, bs = 'cs', k = 6) +
       s(reof4, bs = 'cs', k = 6) +
      s(reof5,  bs = 'cs', k = 6) +
       s(reof6,  bs = 'cs', k = 6), method = 'ML', 
            family = gaussian(),
      correlation = corMLPE(form = ~from + to),
      data = .)))

t1 %>% 
  mutate(r2_dist = map_dbl(dist_mod, ~summary(.$gam)$r.sq),
         r2_reof = map_dbl(reof_mod, ~summary(.$gam)$r.sq)) %>%
  select(time, r2_dist, r2_reof) %>%
  gather(Model, value, 2:3) %>%
  ggplot(aes(time, value, color = Model)) +
  geom_line(size = 1.2) + 
  geom_point(size = 3) +
  scale_color_viridis_d(labels = c('Distance','Distance + EOF')) +
  scale_y_continuous(limits = c(.2, .5)) +
  labs(x = 'Time', y = 'Adjusted R-Squared') +
  theme_classic()

ggsave('../figures/r2_time.png', width = 6, height = 4)
```



```{r fig.width = 6, fig.height = 4}
#think about changing the residual plots back to magma so its easier to compare across them
a <- plt_dat %>%
arrange(JSD) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = JSD, color = JSD)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
   labs(subtitle = 'A') +
  theme_void() +
  coord_sf(datum = NA)

b <- plt_dat %>%
arrange(pred_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = pred_null, color = pred_null)) +
  scale_edge_alpha(guide = 'none') +
 scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
   labs(subtitle = 'B') +
  theme_void() +
  coord_sf(datum = NA)

c <- plt_dat %>%
arrange(res_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = res_null, color = res_null)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
   labs(subtitle = '') +
  theme_void() +
  coord_sf(datum = NA)

d <- plt_dat %>%
arrange(-res_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = -1 * res_null, color = res_null)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
  labs(subtitle = '') +
  theme_void() +
  coord_sf(datum = NA)

e <- plt_dat %>%
arrange(pred_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = pred_mod, color = pred_mod)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
   labs(subtitle = 'C') +
  theme_void() +
  coord_sf(datum = NA)

f <- plt_dat %>%
arrange(res_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = res_mod, color = res_mod)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) +
   labs(subtitle = '') +
  theme_void() +
  coord_sf(datum = NA)

g <- plt_dat %>%
arrange(-res_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = -1 * res_mod, color = res_mod)) +
  scale_edge_alpha(guide = 'none') +
   scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
  labs(subtitle = '') +
  theme_void() +
  coord_sf(datum = NA)

a+(b+c+d+ e+f+g + plot_layout(nrow = 2))
plt_dat %E>% as_tibble() %>% summary
ggsave('figures/residuals.png', width = 6, height = 4)
```

```{r fig.width = 6, fig.height = 6}
plt_dat %E>% as_tibble %>% summary
a <- plt_dat %>%
arrange(JSD) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = JSD, color = JSD)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
   labs(subtitle = 'A') +
  facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

b <- plt_dat %>%
arrange(pred_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = pred_null, color = pred_null)) +
  scale_edge_alpha(guide = 'none') +
 scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
    facet_wrap(~time, nrow = 1) +
   labs(subtitle = 'B') +
  theme_void() +
  coord_sf(datum = NA)

c <- plt_dat %>%
arrange(res_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = res_null, color = res_null)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
   labs(subtitle = '') +
        facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

d <- plt_dat %>%
arrange(-res_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = -1 * res_null, color = res_null)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
  labs(subtitle = '') +
        facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

e <- plt_dat %>%
arrange(pred_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = pred_mod, color = pred_mod)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
   labs(subtitle = 'C') +
    facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

f <- plt_dat %>%
arrange(res_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = res_mod, color = res_mod)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) +
   labs(subtitle = '') +
      facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

g <- plt_dat %>%
arrange(-res_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = -1 * res_mod, color = res_mod)) +
  scale_edge_alpha(guide = 'none') +
   scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
  labs(subtitle = '') +
      facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

a+(b+c+d+ e+f+g + plot_layout(nrow = 2))
a+ b + e + f + g + plot_layout(nrow = 5)
ggsave('figures/residuals.png', width = 6, height = 4)
```



We fit 3 models to predict the prsence of an interaction tie between two sites. The best performing model including distance, size, and reofs as predictors, with an aic of `r logistic_models[[1,4]]`. In second is the model with distance, size, and reofs as predictors, with an aic of `r logistic_models[[2,4]]`. Third is the base geographic model with distance and size, with an AIC of `r logistic_models[[3,4]]`. That said, even the best fitting model only captures 30% of the variance in tie presence. This means we are still missing key variables that drive the presence or absence of social relations.

# References {#references .unnumbered}
