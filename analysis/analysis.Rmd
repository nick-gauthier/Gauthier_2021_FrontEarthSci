---
title: "Supplemental Information"
subtitle: 'Analysis scripts for "Hydroclimate variability influence social interaction in the prehistoric American Southwest"'
author: "Nicolas Gauthier"
date: "Last knit on: `r format(Sys.time(), '%d %B, %Y')`"
#bibliography: bibliography.bibtex
output:
  tufte::tufte_handout:
    citation_package: natbib
    toc: true
    #highlight: pygments
    
  tufte::tufte_html:
    tufte_features: ["fonts", "italics"]
    toc: true
link-citations: yes
monofont: courier
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE)
```

# Introduction
\begin{fullwidth}
This RMarkdown document contains all analysis code used to produce the results and figures in the main text. This three-part analysis details the mapping of spatial and temporal hydroclimate patterns, the estimation of prehistoric social networks and travel routes from archaeological data, and the statistical network analysis comparing the archaeological and climatic data.
\end{fullwidth}

## Data sources
This R Markdown script requires raster data of the 100-year [observed Standardized Precipitation-Evapotranspiration Index](https://wrcc.dri.edu/wwdt/time/)^[https://wrcc.dri.edu/wwdt/time/], as well as [reconstructed SPEI maps from the past millennium](https://zenodo.org/record/1198817)^[https://zenodo.org/record/1198817], and the CGIAR version of the SRTM [digital elevation model](http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/)^[http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/], resampled to 250m. Finally, with permission, acquire the [Southwest Social Networks Database](http://www.southwestsocialnetworks.net/data.html)^[http://www.southwestsocialnetworks.net/data.html]. The Southwest Social Networks database contains sensitive site information, so its access is controlled. This analysis provides a preprocessed version of the dataset, with individual sites aggregated to 10km grid cells, to ensure reproducibility.

## Packages
Import the packages needed to reproduce the analysis.
```{r, message = FALSE}
library(raster) # raster data manipulation
library(ncdf4) # netcdf import
library(tidyverse) # data analysis and visualization
library(sf) # spatial data processing
library(philentropy) # entropy measures
library(tidygraph) # network analysis
library(broom) # PCA object handling
library(gdistance) # least-cost paths
library(mgcv) # GAM fitting
#devtools::install_github('nspope/corMLPE')
library(corMLPE) # correlation structure for the GAMM
```

Next import packages needed to reproduce the visualizations.
```{r, message = FALSE}
library(ggraph) # network visualization
library(mgcViz) # GAM plotting
#devtools::install_github('thomasp85/patchwork') 
library(patchwork)
```

## Study Area

First we define a bounding box covering much of the western United States to constrain the climate analyses, ranging between W124.5$^\circ$ and W-107$^\circ$ and N31$^\circ$ and N37.5$^\circ$. ^[Using a larger study area for the climate analysis allows us to sample a much wider range of climatic variability while still remaining within the broader western US climate zone. This ensures that both A) our statistical analyses will be more robust to sampling error and B) the results will be less sensitive to the exact location and dimensions of our study area.] Also define a smaller area in AZ and NM that will be the focus of the archaeological analysis.
      
```{r}
bbox_wus <- extent(c(-124.5, -102, 30, 42.1))
bbox_swsn <- extent(c(-113, -107, 31, 37.5)) 
```

```{r, echo = FALSE}
state_names <- c('arizona', 'new mexico', 'colorado', 
                 'california', 'utah', 'nevada')

states_wus <- maps::map('state', regions = state_names, 
                    fill = TRUE, plot = FALSE) %>% st_as_sf

states_swsn <- maps::map('state', regions = c('arizona', 'new mexico'), 
                    fill = TRUE, plot = FALSE) %>% st_as_sf

country <- maps::map('usa',fill = TRUE, plot = FALSE) %>% st_as_sf
```

```{r bbox_map, fig.margin=TRUE, echo = FALSE, fig.cap = 'Locations of the two study areas.'}
ggplot() + 
  geom_sf(data = states_wus) +
  geom_sf(data = country, fill = NA) +
  geom_sf(data = st_as_sfc(st_bbox(bbox_wus, crs = st_crs(4326))), fill = NA, color = 'red') +
  geom_sf(data = st_as_sfc(st_bbox(bbox_swsn, crs = st_crs(4326))), fill = NA, color = 'red') +
  theme_bw()
```

# Climate Analysis

First estimate robust spatial modes of hydroclimate variability in the Southwest US using present-day observations and reconstructions from the past millennium. Calculate the leading Empirical Orthogonal Functions of a ~100 year series of the summertime average Standardized Precipitation-Evapotranspiration Index. These patterns then undergo a varimax rotation to highlight more physically meaningful spatial patterns. Compare these patterns to those derived from a long-term reconstruction based that fuses several climate proxies with outputs from the CESM Last Millennium Ensemble. We confirm that the spatial patterns detected for the past 100 years have been robust over time and explain up to 83% of the variance in a full 1,000 year sequence of reconstructed drought dynamics.

## Climate Data
Import high resolution SPEI maps generated from PRISM data. Calculate the average summertime (JJA) SPEI value for each year.

```{r import_observations, warning=FALSE}
spei_obs <- list.files('data/PRISM', full.names = TRUE) %>%
  map(brick) %>%
  map(crop, bbox_wus) %>%  # crop to the study area bounding box
  reduce(`+`) %>% # average over the 3 months
  `/`(3) %>%
  `names<-`(1895:2017) %>% # add year names
  .[[-1]] # SPEI calculated on 12 month lag, so drop 1st year
```

Import the reconstructed SPEI fields from PHYDA.^[PHYDA uses an off-line data assimilation approach, with simulated SPEI from the CESM LME experiments as physically-consistent model priors and a network of tree rings, ice cores, and corals as assimilated observations. The ensemble Kalman filter uses this information, as well as the spatial covariances from the climate-model prior (e.g. teleconnections), to ``spread out'' information from the point-based proxies.]

```{r import_reconstructions, cache = TRUE}
recon_path <- 'data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc'
spei_recon <- brick(recon_path, varname = 'spei_mn') %>%
   .[[1100:1999]] %>% # extract years of interest
   rotate %>% # rotate longitudes to -180 to 180
   crop(bbox_wus, snap = 'near')
```

```{r, echo = FALSE, fig.margin = TRUE, fig.cap = 'Observed and reconstructed SPEI for summer 1985.'}
brick(c(spei_obs[[90]], resample(spei_recon[[886]], spei_obs, method = 'ngb'))) %>%
  `names<-`(c('Observed', 'Reconstructed')) %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(type, spei, 3:4) %>%
  ggplot(aes(x, y)) +
  geom_raster(aes(fill = spei)) +
  facet_wrap(~type, nrow = 2) +
  scale_fill_distiller(palette = 'Spectral', direction = 1,
                       limits = c(-3, 3),
                       guide = 'legend',
                       name = 'SPEI') +
  coord_quickmap() +
  theme_void()
```

```{r plot_variance_trend, echo = FALSE, message = FALSE, fig.margin=TRUE, fig.cap = 'Plotting the reconstruction time series reveals no clear trend in the mean. It does, however, show a pattern of increasing variance over time, which reflects the increasing number of proxies assimilated over time.'}
spei_recon %>% 
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, value, 3:902) %>%
  mutate(year = str_sub(year, 2),
         year = as.numeric(year)) %>%
  ggplot(aes(year, value)) +
  geom_line(alpha = .1, aes(group = interaction(x, y))) +
  scale_y_continuous(limits = c(-4,4), name = 'SPEI') +
  geom_smooth() +
  theme_bw()
```

## Drought and Flood Patterns

Calculate the (rotated) empirical orthogonal functions of the observed and reoconstructed SPEI data using PCA and varimax rotation. These represent coherent, recurring spatial patterns of drought and flood variability over our study area. First, define a function to reweight the SPEI data based on the latitude to acount for the areal distortion of each cell as latitude changes.

```{r}
area_weight <- function(x){
  names_x <- names(x)
  x %>%
    init('y') %>% # get a map of latitudes
    `*`(pi/180) %>% # convert to radians
    cos %>% # cosine
    sqrt %>%
    `*`(x) %>%
    `names<-`(names_x)
}
```

Now run a principal components analysis on the observed SPEI data. Much of the code that follows is adapted from the *wql* and *sinkr* packages.

```{r}
obs_pca <- spei_obs %>%
  area_weight %>% # reweight for latitudinal distoritions
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  prcomp(scale. = FALSE) # FALSE because SPEI is already normalized
```

Autocorrelation in the observed time series may bias our assessment of the principal components. Define a function to calculate the effective number of observations given the sample autocorrelation. 
```{R}
n_effective <- function(x){
  n <- nlayers(x)
  x %>%
    area_weight %>%
    as.data.frame(na.rm = TRUE) %>%
    t %>%
    as_tibble %>%
    gather(cell, value) %>%
    nest(data = c(value)) %>%
    mutate(rho = map_dbl(data, ~cor(.$value, lag(.$value), use = 'comp')),
           effective_n = n * (1 - rho^2) / (1 + rho^2)) %>% # from Bretherton et al 1999
    summarise(mean(effective_n)) %>%
    pull
}
```

Plot the results of the PCA. For the observations, we see that the leading 6 PCs explain 83% of the variance in the series, so we'll retain those for rotation. Truncating at 4 PCs would also be justified, but because we later rotate these PCs the results are less sensitive to the exact number of PCs retained (see below). 

```{r, echo = FALSE}
obs_eigs <- obs_pca %>%
  tidy(matrix = 'pcs') %>%
  mutate(eigenvalues = std.dev ^ 2,
         error = sqrt(2 / n_effective(spei_obs)),
         low =  eigenvalues * (1 - error) * 100 / sum(eigenvalues),
         hi = eigenvalues * (1 + error) * 100 / sum(eigenvalues),
         cumvar_line = hi + 0.02 * max(hi))
```

```{r, echo = FALSE}
recon_pca <- spei_recon %>%
  area_weight %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  prcomp(scale. = FALSE)

recon_eigs <- recon_pca %>% 
  tidy(matrix = 'pcs') %>%
  mutate(eigenvalues = std.dev ^ 2,
         error = sqrt(2 / n_effective(spei_recon)),
         low =  eigenvalues * (1 - error) * 100 / sum(eigenvalues),
         hi = eigenvalues * (1 + error) * 100 / sum(eigenvalues),
         cumvar_line = hi + 0.02 * max(hi))
```

```{r plot_variance_obs, fig.width = 5, fig.height = 4, echo = FALSE, fig.cap = 'Percent variance explained by the leading principal components of the observed SPEI record. PCs that are not well separated can be considered effective multiplets due to temporal autocorrelation, and should not be split in truncation.'}
obs_eigs %>% 
  mutate(separated = if_else(is.na(lag(low)), TRUE, hi < lag(low)),
                   separated_colors = cumsum(separated),
         weights = if_else(PC < 50, 0, 1))%>%
    filter(PC <= 12) %>%
ggplot(aes(x = PC, y = percent * 100)) +
  geom_errorbar(aes(x = PC, ymin = low, ymax = hi), width = 0.4) +
  geom_point(size = 2, aes(color = as.factor(separated_colors))) + 
  geom_text(aes(x = PC, y = cumvar_line, label = paste0(round(cumulative * 100, 0), '%')), size = 2.5, vjust = 0) +
  labs(x = "Principal Component", y = "Normalized Eigenvalue") + 
  geom_vline(xintercept = 6.5, linetype = 2, color = 'red', alpha = .7) +
  theme_bw() + 
  guides(color = F) + 
  scale_x_continuous(breaks = seq(0, 12, 2))
```


```{r plot_variance_phyda, fig.width = 5, fig.height = 4, fig.margin=TRUE, echo = FALSE, fig.cap = 'Percent variance explained by the principal components of the PHYDA reconstruction.'}
recon_eigs %>%
  mutate(separated = if_else(is.na(lag(low)), TRUE, hi < lag(low)),
          test = cumsum(separated),
         weights = if_else(PC < 3, 0, 1))%>%
  filter(PC <=12) %>%
ggplot(aes(x = PC, y = percent * 100)) +
  geom_errorbar(aes(x = PC, ymin = low, ymax = hi), width = 0.4) +
  geom_point(size = 3, aes(color = as.factor(test))) + 
  geom_text(aes(x = PC, y = cumvar_line, label = round(cumulative * 100, 0)), size = 2.5, vjust = 0) +
  labs(x = "PC", y = "Normalized Eigenvalue") +
  theme_bw() + 
  guides(color = FALSE) + 
  scale_x_continuous(breaks = seq(0, 12, 2))
```

Define a function to calculate the empirical orthogonal functions and rotated empirical orthogonal functions, as well as their associated PC amplitude time series. Save all relevant information in a list for later use.

```{r}
get_EOFs <- function(pc_object, eigs, rast_template, n_modes){
  eofs <- pc_object %>% # calculate unrotated EOFs
    tidy(matrix = 'variables') %>%
    filter(PC <= n_modes) %>%
    left_join(eigs[1:2], by = 'PC') %>%
    mutate(eof = value * std.dev,
           PC = as.character(PC)) %>%
    select(-std.dev, -value) 
  
  varim <- eofs %>% # varimax rotation
    pivot_wider(names_from = PC, values_from = eof) %>%
    column_to_rownames(var = 'column') %>%
    as.matrix %>%
    varimax
  rot_mat <- varim$rotmat # rotater
  
  reofs <- unclass(varim$loadings) %>%
    as_tibble(rownames = 'column') %>%
    pivot_longer(-column, names_to = 'PC', values_to = 'reof') %>%
    right_join(eofs, by = c('column', 'PC'))
  
  eof_amps <- pc_object$x %>%
    .[,1:n_modes] %>%
    scale %>%
    as_tibble(rownames = 'year', .name_repair = ~1:n_modes) %>%
    gather(PC, eof_amp, -year) %>%
    mutate(year = as.numeric(str_sub(year, 2)),
           PC = as.factor(paste0('PC', PC)))
  
  reof_amps <- pc_object$x %>%
    .[,1:n_modes] %>%
    scale %>%
    `%*%`(rot_mat) %>%
    as_tibble(rownames = 'year', .name_repair = ~1:n_modes) %>%
    gather(PC, reof_amp, -year) %>%
    mutate(year = as.numeric(str_sub(year, 2)),
           PC = as.factor(paste0('PC', PC)))
  
  reofs <- rast_template[[1]] %>%
    as.data.frame(xy = TRUE, na.rm = TRUE) %>% 
    .[1:2] %>%
    slice(rep(1:n(), times = n_modes)) %>%
    bind_cols(reofs, .) %>%
    select(-column) %>%
    mutate(PC = paste0('EOF', PC))
  
  list(EOFs = reofs, 
       amps = left_join(eof_amps, reof_amps, by = c('year', 'PC')), 
       rotater = rot_mat)
}
```

Use this function to calculate the EOFs and REOFs for the observations and reconstructions, retaining the 6 leading components in each case for rotation.

```{r calc_eofs}
n_modes <- 6
reof_obs <- get_EOFs(obs_pca, obs_eigs, spei_obs, n_modes)
reof_recon <- get_EOFs(recon_pca, recon_eigs, spei_recon, n_modes)
```

```{r reof_raster, warning = FALSE, echo = FALSE}
#Save the reof data as a raster, so we can crop it to the state boundaries
reof_raster <- reof_obs$EOFs %>%
  select(-eof) %>%
  spread(PC, reof)  %>%
  rasterFromXYZ %>%
  `crs<-`(value = '+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0')%>%
  crop(states_wus) %>%
  mask(states_wus)

# invert reof 6 for better visualization
reof_raster[[6]] <- reof_raster[[6]] * -1
```

```{r, include = FALSE}
writeRaster(reof_raster, '../output/reofs.tif', overwrite = TRUE)
```

```{r, include = FALSE, warning = FALSE}
eof_raster <- reof_obs$EOFs %>%
  select(-reof) %>%
  spread(PC, eof)  %>%
  rasterFromXYZ %>%
  `crs<-`(value = '+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0')%>%
  crop(states_wus) %>%
  mask(states_wus)
```

```{r, echo = FALSE, fig.margin = TRUE, fig.cap='Observed EOFs before rotation.'}
as.data.frame(eof_raster, xy = TRUE, na.rm = TRUE, long= TRUE) %>%
ggplot() +
  facet_wrap(~layer, ncol = 2) +
  geom_raster(aes(x, y, fill = value)) +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-.82, .82)) +
  theme_void() + 
    geom_sf(data = states_wus, color = 'black', fill = NA) +
  coord_sf(datum = NA) +
  theme(legend.position = "bottom")
```

```{r, echo = FALSE, fig.margin = TRUE, fig.cap='Reconstructed EOFs before rotation.'}
ggplot(reof_recon$EOFs, aes(x, y, fill = eof)) +
  facet_wrap(~PC) +
  geom_raster() +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-.72, .72)) +
  theme_void() + 
  coord_quickmap()
```

Map the resulting spatial patterns. The colors in these REOF maps are eigenvectors weighted by the sqare root of the associated eigenvalue, so these loadings represent the covariance between each grid cell and each amplitude (principal component). Visual comparison of the observed and reconstructed REOFs show they correspond to the same physical phenomena. Thus we can use the observed patterns as the high resolution patterns, and infer their temporal evolution from the reconstructions. The next section focuses on these amplitude time series, and we'll confirm the correlations between the observed and reconstructed patterns there.

```{r plot_obs_reof, echo = FALSE, fig.fullwidth = FALSE, fig.width = 6, fig.height = 7, fig.cap = 'Observed drought REOFs'}
reof_raster %>%
  as.data.frame(xy = TRUE, na.rm = TRUE, long = TRUE) %>%
ggplot() +
  geom_raster(aes(x, y, fill = value)) +
  scale_fill_viridis_c(option = 'magma', name = 'Correlation') +
  facet_wrap(~layer, ncol = 2) +
  theme_void() +
  geom_sf(data = states_wus, color = 'black', fill = NA) +
  coord_sf(datum = NA) +
  theme(legend.position = "bottom")

ggsave('../figures/reof_observed.pdf', height = 7, width = 6)
```

```{r, echo = FALSE}
## the reconstructed and observed REOFs/PCs don't come in the same order, this corrects that
reof_recon$amps <- reof_recon$amps %>%
  mutate(PC = case_when(PC == 'PC1' ~ 'PC2',
                        PC == 'PC2' ~ 'PC4',
                         PC == 'PC3' ~ 'PC5',
                         PC == 'PC4' ~ 'PC3',
                         PC == 'PC5' ~ 'PC1',
                         PC == 'PC6' ~ 'PC6')) %>%
  arrange(PC) %>%
  mutate(reof_amp = if_else(PC %in% c('PC2', 'PC5', 'PC6'), reof_amp * -1, reof_amp))

reof_recon$EOFs <- reof_recon$EOFs %>%
  mutate(PC = case_when(PC == 'EOF1' ~ 'EOF2',
                        PC == 'EOF2' ~ 'EOF4',
                         PC == 'EOF3' ~ 'EOF5',
                         PC == 'EOF4' ~ 'EOF3',
                         PC == 'EOF5' ~ 'EOF1',
                         PC == 'EOF6' ~ 'EOF6')) %>%
  arrange(PC)
```

```{r plot_recon_reofs, fig.fullwidth = FALSE, echo = FALSE, fig.cap = 'Reconstructed drought REOFs'}
reof_recon$EOFs %>%
  mutate(reof = if_else(PC %in% c('EOF2', 'EOF5', 'EOF6'), reof * -1, reof)) %>%
  ggplot() +
  geom_raster(aes(x,y,fill = reof)) +
  scale_fill_viridis_c(option = 'magma', name = 'Correlation') +
  facet_wrap(~PC) +
  coord_sf(datum = NA) +
  theme_void()
```



```{r, echo = FALSE, fig.margin = TRUE, fig.cap = 'Climate regionalization showing the dominant REOF for each location.'}
which.max(abs(reof_raster)) %>%
  as.data.frame(xy = T, na.rm = T)  %>%
  ggplot() +
  geom_raster(aes(x,y, fill = as.ordered(layer))) +
  scale_fill_viridis_d(name = '') +
  theme_void() +
  geom_sf(data = states_wus, color = 'black', fill = NA) +
  coord_sf(datum = NA)
```

## Drought and Flood Amplitudes

```{r, echo = FALSE, include=FALSE}
# Not shown.
# For each PC amplitude time series, plot out the wettest and driest years from the observations.
# From this we can tell that all but PC6 is of the right sign (high pc amplitude == high positive SPEI). 
# So next we multiply PC6 by -1 
reof_obs$amps %>%
  group_by(PC) %>%
  filter(reof_amp == max(reof_amp) | reof_amp == min(reof_amp)) %>%
  pull(year) %>%
  `-`(1895) %>%
  spei_obs[[.]] %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, SPEI, X1915:X2008) %>%
  mutate(year = factor(year, levels = paste0('X', c(1915, 1996 ,1987, 2011, 1934, 1984, 2014, 2017, 1947, 1963, 1977, 2008)))) %>%
  ggplot(aes(x, y, fill = SPEI)) +
  geom_raster() +
  facet_wrap(~year) +
  coord_quickmap() +
  scale_fill_distiller(palette = 'Spectral', limits = c(NA, 4), direction = 1) +
  theme_void()

reof_obs$amps <- reof_obs$amps %>%
  mutate(reof_amp = if_else(PC == 'PC6', reof_amp * -1, reof_amp))
```

Now plot the leading 6 PC amplitude time series.

```{r echo = FALSE, fig.width=6, fig.height=4, fig.cap = 'Observed SPEI amplitudes of the leading 6 rotated principal components.', warning = FALSE}
reof_obs$amps %>%
  group_by(PC) %>%
  mutate(amp_smooth = zoo::rollmeanr(reof_amp, k = 10, fill = NA)) %>%
ggplot(aes(year, reof_amp, color = PC)) +
    geom_line(alpha = .4) +
  geom_line(aes(y = amp_smooth), size = 1.2, na.rm = TRUE) +
  facet_wrap(~PC) + 
  geom_hline(yintercept = 0, linetype = 2, alpha= 1, color = 'black') + 
  scale_color_viridis_d(guide = FALSE) +
  theme_bw() +
  labs(x = 'Year', y = 'SPEI')

ggsave('../figures/pc_obs.pdf', width = 6, height = 4)
```

```{r, echo = FALSE, fig.margin = TRUE, fig.cap='Reconstructed SPEI amplitudes.'}
reof_recon$amps %>%
  filter(year >= 1896) %>%
  group_by(PC) %>%
  mutate(amp_smooth = zoo::rollmeanr(reof_amp, k = 10, fill = NA)) %>%
ggplot(aes(year, reof_amp, color = PC)) +
  geom_line(alpha = .4) +
  geom_line(aes(y = amp_smooth), size = 1.2, na.rm = TRUE) +
  facet_wrap(~PC) + 
  geom_hline(yintercept = 0, linetype = 2, alpha= 1, color = 'black') + 
  scale_color_viridis_d(guide = FALSE) +
  theme_bw() +
  labs(x = 'Year', y = 'SPEI')
```


Plot the time series (PCs) of each observed and reconstructed REOF against each other, to confirm that they correspond to the same patterns. In general we find strong linear fits, save for PC6 which seems to be more loosely related in the observations and reconstructions. Since this pattern represents variability related to the Colorado Plateau, perhaps we are seeing some topographically-induced variability that isn't well represented in the CESM LME prior used by PHYDA.

```{r, plot_amplitudes, fig.fullwidth=TRUE, fig.height=4, fig.width=10, echo = FALSE, fig.cap = 'Strong linear correlation between the observed and reconstructed EOFs', warning=FALSE}
amplitudes_modern <- reof_obs$amps %>%
  inner_join(reof_recon$amps, by = c('year', 'PC'),
            suffix = c('_obs', '_recon'))

ggplot(amplitudes_modern, aes(reof_amp_recon, reof_amp_obs)) +
  geom_point(aes(color = year)) +
  geom_smooth(method = 'lm') +
  scale_color_viridis_c() +
  facet_wrap(~PC, nrow = 1) +
  labs(title = 'Linear fits between observed and reconstructed amplitudes', 
       subtitle = '1896-1999',
       x = 'Reconstructed', y = 'Observed') +
  theme_minimal() +
  coord_fixed() +
  theme(legend.position = 'bottom')
```

How have the amplitudes of these patterns changed over time? Plot the reconstructed time series for each, along with an estimated trend line. Mark the timing of the "Great Drought" at ~1275.

```{r plot_recon_amplitudes, fig.width = 9, fig.height = 4, echo = FALSE, fig.fullwidth=TRUE, fig.cap='Reconstructed SPEI amplitudes during the period 1200CE-1450CE.'}
reof_recon$amps %>%
  mutate(amp_smooth = zoo::rollmeanr(reof_amp, k = 10, fill = NA)) %>%
  filter(between(year, 1200, 1450)) %>% 
ggplot(aes(year, reof_amp, group = PC)) +
  geom_line(aes(color = PC), alpha = .3) +
  geom_line(aes(y = amp_smooth, color = PC), size = 1.2) +
  scale_color_viridis_d(guide = FALSE) +
  facet_wrap(~PC) +
  theme_bw()+
  theme(legend.position = "bottom") + 
  geom_hline(yintercept = 0, color = 'black', linetype = 2) +
  geom_vline(xintercept = 1275, color = 'red', linetype = 2) +
  labs(x = 'Year', y = 'SPEI (Reconstructed)')
```

```{r, echo = FALSE, message=FALSE, warning = FALSE}
rm(spei_obs, spei_recon, obs_pca);invisible(gc())
```

# Archaeological Analysis

Estimate prehistoric social networks using archaeological proxies from the Southest Social Networks project.^[Go to http://www.southwestsocialnetworks.net for more information on this project and dataset.] First we import our archaeological site location data, and aggregate them to 10km grid cells for analysis. Then we estimate the flow of information between each pair of 10km grid cells using the frequency of different ceramic ware types and an information theoretic measure. Then we estimate the potential hiking speeds over the terrain of the study region, and use those speeds to estimate the likely travel times between each pair of sites.

## Mapping Archaeological Sites

Read in a csv file containing information for all of the archaeological sites in the database, including spatial location and number of rooms present from each time period. These will be the source of the the node-level data for the archaeological network. Because the precise locations of archaeological sites are sensitive information, most of this section is not run by default and the following sections use a preprocessed version of the **patches** object created below. To run this section, set *eval = TRUE* in the following code chunnks and acquire the site locations dataset (attributes_orig.csv) from the SWSN project directly.

```{r, message = FALSE}
# define the SWSN coordinate reference system
crs_utm <- '+proj=utm +zone=12 +datum=NAD27'
# read in the csv and turn into a spatial object
sites <- read_csv('data/attributes_orig.csv') %>%
  st_as_sf(coords = c('EASTING', 'NORTHING'), crs = crs_utm) %>%
  st_transform('+proj=longlat +datum=WGS84')
```

Next aggregate the sites into 10km x 10km patches -- the rough limit of raw material extraction around a site. This serves a practical purpose for the analysis -- coarse-graining the data to make the results less sensitive to how exactly "sites" are defined in the field and removing data artifacts due to periods of small-scale settlement dispersal and aggregation -- as well as keeping the exact location of sensitive archaeological sites hidden. First import the SRTM DEM we'll use as a reference and aggregate it to ~10km resolution.

```{r, echo = FALSE, eval = FALSE}
# This chunk simply creates the cropped version of the SRTM 250m DEM included 
# in the rest of the analysis. Download from http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/
# to reproduce the whole analysis, or just use the preprocessed SRTM_SWSN.tif
# file included with this notebook.
srtm <- raster('data/SRTM_W_250m_TIF/SRTM_W_250m.tif') %>% # import the SRTM DEM
  crop(bbox_swsn) # crop to study area
writeRaster(srtm, 'data/SRTM_SWSN.tif')
```

```{r dem-aggregation}
elev_patch <- raster('data/SRTM_SWSN.tif') %>%
  aggregate(fact = 40) # aggregate to ~10km resolution
```

Use this elev_patch raster as a template for our 10km grid cells. Get a list of unique site IDs.
```{r, eval = FALSE}
patch_ids <- sites %>%
  st_coordinates%>%
  cellFromXY(elev_patch, .) %>%
  as.character
```

Use these patch IDs to aggregate the site-level data. For each patch, get a list of all the sites contained therein and sum the room counts of all sites for each period.

```{r, eval = FALSE}
patches <- sites %>%
  mutate(., patch_id = patch_ids) %>%
  gather(period, rooms, P1room:P5room) %>%
  mutate(time = parse_number(period) * 50 + 1150) %>%
  group_by(patch_id, time) %>%
  summarise(site = list(SWSN_Site),
            rooms = sum(rooms)) %>%
  ungroup %>%
  spread(time, rooms) %>%
  cbind(., xyFromCell(elev_patch, as.numeric(.$patch_id))) %>%
  as.data.frame %>%
  st_as_sf(coords = c('x','y'), crs = 4326, remove = FALSE)
```

```{r, echo = FALSE, eval = FALSE}
saveRDS(patches, 'data/patches.RDS')
```

```{r, echo = FALSE}
patches <- readRDS('data/patches.RDS')
```

```{r, echo = FALSE, fig.cap = 'Total room counts for all periods from all archaeological sites within each 10km grid cell'}
patches %>%
  mutate(Rooms = X1200 + X1250 + X1300 + X1350 + X1400) %>%
  arrange(Rooms) %>%
  ggplot() +
  geom_sf(aes(size = Rooms, color = Rooms), show.legend = 'point') + 
  geom_sf(data = states_swsn, fill = NA, color = 'black') +
  scale_size_area(limits = c(0, 3000), breaks = seq(0, 3000, by = 500)) +
  scale_color_viridis(option = 'magma', limits = c(0, 3000), breaks = seq(0, 3000, by = 500), guide = 'legend') +
  coord_sf(datum = NA) +
  theme_void()  +
  theme(legend.position = c(1, 0), legend.justification = c(2.6,-.3))
```


## Social Network Reconstruction

Next we get the apportioned ceramic ware counts per site. These files are supplied by the SWSN project as a result from their internal apportioning scheme. Start by loading these preprocessed matrices, one for each time step, and combinding them into a single tibble.

```{r}
load('ware_matrices')
wares <- list(AD1200cer, AD1250cer, AD1300cer, 
              AD1350cer, AD1400cer) %>%
  map(rownames_to_column) %>%
  bind_rows(.id = 'time') %>%
  mutate(time = as.numeric(time) * 50 + 1150) %>%
  rename(site = rowname)
```

Visualize the ware types, and how their relative frequencies vary over time across the entire study area.
```{r, fig.fullwidth = FALSE, echo = FALSE, fig.height = 4, fig.width = 6, fig.cap = 'Changing proportions of each ware type over time.', warning = FALSE}
wares %>%
  gather(ware, count, 3:38) %>%
  group_by(ware, time) %>%
  summarise(Proportion = sum(count)) %>%
ggplot(aes(time, Proportion, fill = ware, group = ware)) +
  geom_bar(position='fill', stat = 'identity', color = 'black') +
  theme_bw()+
  theme(legend.key.size = unit(.4, 'cm'), legend.text=element_text(size=rel(0.5)))
```

Define a function to calculate the modified Jensen-Shannon divergence between each pair of assemblages, and turn the result into a social network for visualization and analysis.^[Setting unit = "log" in the JSD command means we're measuring information in nats, change to "log2" to measure in bits.]

```{r}
# maximum theoretical divergence given equal weights
theo_max <- -.5 * log(.5) - .5 * log(.5)
# function to calculate modified JSD
getJSD <- function(data, time){
  patch_ids <- pull(data, patch_id)
  data %>%
    select(-patch_id) %>%
    select_if(~!all(. == 0)) %>%
    as.matrix %>%
    # jensen-shannon divergence calculated here
    philentropy::distance(method = 'jensen-shannon', 
                          unit = 'log',
                          est.prob = 'empirical') %>%
    `/`(., theo_max) %>% # divide by theoretical max so [0-1]
    sqrt %>% # square root makes it a metric
    `-`(1,.) %>% # invert so high values == more similar
    `rownames<-`(patch_ids) %>%
    `colnames<-`(patch_ids) %>%
    replace(. == 0, 999) %>% # replace 0 values with 999 temporarily
    as_tbl_graph %E>% # convert to directed graph
    rename(JSD = weight) %>%
    mutate(JSD = if_else(JSD == 999, 0, JSD)) %>% # convert 999 values back to 0
    mutate(time = time) %>% # set the time period
    filter(!edge_is_loop()) %>% # remove self loops
    activate(nodes) 
}
```

For each 10km grid cell and each time period, combine the apportioned ceramic wares from all archaeological sites. Then apply the getJSD function defined above to each pair of grid cells to calculate the cultural similarity/information flow between the pair. The result is a **ggraph** network object with the estimated tie strength between each grid cell for each time period.

```{r, message = FALSE, warning = FALSE}
swsn <- patches %>%
  gather(time, rooms, X1200:X1400) %>%
  mutate(time = parse_number(time)) %>%
  as.data.frame %>%
  select(patch_id, site, time) %>%
  unnest(cols = c(site)) %>%
  right_join(wares, by = c('site', 'time')) %>%
  as.data.frame %>%
  select(-site) %>%
  group_by(time, patch_id) %>%
  summarise_all(sum, na.rm = T) %>%
  nest %>%
  mutate(net = map2(data, time, getJSD)) %>%
  pull(net) %>%
  reduce(graph_join, by = 'name') %>%
  left_join(patches, by = c('name' = 'patch_id'))
```

```{r fig.fullwidth = TRUE, fig.width = 8, fig.height = 4, echo = FALSE, warning=FALSE, fig.cap='Reconstructed social network, estimated with the modified Jensen-Shannon divergence between the ceramic ware frequencies at each pair of 10km landscape patches. Higher values represent greater information flow, which implies greater cultural similarity and a higher probability of social interaction.'}
swsn %E>% 
    filter(JSD > .001) %>%
    filter(from < to) %>%
    arrange(JSD) %>% 
ggraph('manual', x = x, y = y) +
  geom_edge_fan(aes( color = JSD)) +
  geom_sf(data = states_swsn, fill = NA, color = 'black') +
  facet_edges(~time, ncol = 3) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(name = 'Similarity', option = 'magma') +
  coord_sf(datum = NA) +
  theme_void() +
  theme(legend.position = c(1, 0), legend.justification = c(2.5,0))
```

```{r, fig.width= 10, fig.height = 5, fig.fullwidth = TRUE, echo = FALSE, warning = FALSE}
a <- swsn %N>%
  mutate(Rooms = X1200 + X1250 + X1300 + X1350 + X1400) %>%
  arrange(Rooms) %>%
  ggraph('manual', x = x, y = y) +
      geom_sf(data = states_swsn, fill = NA, color = 'black') +
  geom_node_point(aes(size = Rooms, color = Rooms)) +
  scale_size_area(limits = c(0, 3000), breaks = seq(0, 3000, by = 500)) +
  scale_color_viridis(option = 'magma', limits = c(0, 3000), breaks = seq(0, 3000, by = 500), guide = 'legend') +
  coord_sf(datum = NA) +
  theme_void()  +
  theme(legend.position = c(1, 0), legend.justification = c(2.6,-.3))

b <- swsn %E>% 
    filter(JSD > .001) %>%
    filter(from < to) %>%
    arrange(JSD) %>% 
ggraph('manual', x = x, y = y) +
  geom_edge_fan(aes( color = JSD)) +
  geom_sf(data = states_swsn, fill = NA, color = 'black') +
  facet_edges(~time, ncol = 2) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(name = 'Similarity', option = 'magma') +
  coord_sf(datum = NA) +
  theme_void() +
  theme(legend.position = c(1, 0), legend.justification = c(1.25, -1), legend.direction = 'horizontal')

c <- a + b + plot_layout(heights = c(1, 1)) + plot_annotation(tag_levels = 'A')
ggsave('swsn2.png', plot = c, height = 5, width = 10)
```

## Hiking Speeds and Least-cost distances

Next we calculate the impact of distance and terrain on the potential flow of people and information between the sites in the SWSN database. For this we need to use a higher-resolutio version of the SRTM digital elevation model used above to define the landscape patches. First import the elevation dataset and resample to 500m to cut down on computational time.
```{r}
elev_lcp <- raster('data/SRTM_SWSN.tif') %>%
  aggregate(fact = 2) # aggregate to ~500m resolution
```

Now we use the package *gdistance* to calculate the time it would take for a hiker to traverse this landscape on foot. This package uses a transition matrix approach, working with matrices that contain the "transmissiveness" (inverse costs) of travel from each cell to its immediate 16 neighbor cells. This allows for a sparse matrix representation, because there is no possible connection to non neighboring cells, which considerably reduces the computational burden. Calculate the absolute difference in elevation between each cell and its 16 neighbors, and use these values to create the sparse transition matrix.

```{r, cache = TRUE}
altDiff <- function(x){abs(x[2] - x[1])}
hd <- transition(elev_lcp, altDiff, 16, symm = TRUE)
```
Divide the height differences by the horizontal distances between cells, resulting in slopes. In rugged, mountainous terrain, slope is the key factor in influencing ease of travel.
```{r, cache = TRUE}
slope_c <- geoCorrection(hd, type = 'c')
```
Figure out which cells are adjacent to one another, queen's case.
```{r, cache = TRUE}
adj <- adjacent(elev_lcp, cells = 1:ncell(elev_lcp), directions = 16)
```

Use Tobler's hiking function to convert the slope-based transition matrix to one of perceived hiking speeds. 

```{r}
tobler_adjusted <- function(x){
  4 * exp(-3.5 * 2.15 * x) / 3.6 # 3.6 turns km/h into m/s
}
```

```{r, fig.margin = TRUE, echo = FALSE, fig.cap = "This is a modified version of Tobler's traditional function in two ways. First, a steep slopes have an extra penalty to reflect cognitive biases in how people perceive terrain. People tend to overestimate the steepness of steeper slopes, thereby discouraging travel along such slopes even when it is physically possible. Second, the function is made symmetric (and thus isotropic) by averaging uphill and downhill walking speeds. The focus here is on the physical routes that people traveled, and a good path connecting two places has to optimize for ease of travel in both directions."}
slope <- seq(-1, 1, .01)
qplot(slope, tobler_adjusted(abs(slope)) * 3.6, geom = 'line' ) +
  geom_vline(xintercept = 0, linetype = 2) +
  labs(x = 'Slope (rise over run)', y = 'Walking speed (km/h)') +
  theme_classic()
```

Use Tobler's hiking function to calculate walking speed from slope and update the transition matrix.
```{r, cache = TRUE}
speed_c <- slope_c
speed_c[adj] <- tobler_adjusted(slope_c[adj])
```
Divide by intercell distance again, resulting in the conductance matrix.
```{r, cache = TRUE}
conductance_c <- geoCorrection(speed_c, type = 'c')
```

```{r, echo = FALSE, fig.fullwidth = FALSE, fig.cap = 'Map of the conductance surface used to calculate the least cost network. Values correspond to the potential hiking speed over each point in space.'}
speed_c %>%
  raster %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
ggplot(aes(x, y, fill = layer * 3.6)) + # convert back to km/h
  geom_raster() +
  scale_fill_viridis_c(guide = 'legend', name = 'Walking\nspeed (km/h)', limits = c(NA,4)) +
  coord_quickmap() +
  labs(x = 'Longitude', y = 'Latitude') +
  theme_minimal()
```

```{r, echo = FALSE, message=FALSE, warning = FALSE}
rm(altDiff, hd, slope_c, speed_c, adj);invisible(gc())
```

Using the conductance matrix, calculate the full pairwise least cost distance matrix. The resulting least-cost network will be used as a proxy measure for the constraints on moving both people and bulk goods across the landscape, and thus the topographic affordances for social exchange.

```{r least-cost-distances, cache = TRUE}
distances <- swsn %>%
  as_tibble %>%
  st_as_sf %>%
  st_coordinates%>%
  costDistance(conductance_c, .) %>% # least cost distances
  as.matrix %>% 
  `colnames<-`(as_tibble(swsn)$name) %>%
  as_tibble %>%
  mutate(from_patch = as_tibble(swsn)$name) %>%
  gather(to_patch, distance, -from_patch) %>%
  mutate(distance = distance / 3600)
```


# Network Analysis

With the climatic and archaeological data preprocessed, its time to fit the statistical network model.

## Data Preparation

Import the data produced in the other analyses to add to the sites data.
```{r}
reof_resamp <- reof_raster %>%
  crop(bbox_swsn) %>%
  resample(elev_patch)
```

```{r, fig.margin = TRUE, eval = FALSE}
plot(reof_resamp, )
pairs(reof_resamp)
```


Now that we have all the data imported and preprocessed, let's put it all together into a data frame.

```{r data_preparation, warning = FALSE, message = FALSE}
swsn2 <- swsn %N>%
  # extract the reof data at the network node level
  mutate(reof1 = raster::extract(reof_resamp, st_coordinates(geometry))[,1],
         reof2 = raster::extract(reof_resamp, st_coordinates(geometry))[,2],
         reof3 = raster::extract(reof_resamp, st_coordinates(geometry))[,3],
         reof4 = raster::extract(reof_resamp, st_coordinates(geometry))[,4],
         reof5 = raster::extract(reof_resamp, st_coordinates(geometry))[,5],
         reof6 = raster::extract(reof_resamp, st_coordinates(geometry))[,6]) %E>%
  # for each edge, get the name of the from and to nodes
  mutate(from_patch = .N()$name[from],
         to_patch = .N()$name[to]) %>%
  # combine with the least-cost distances
  left_join(distances, by = c('from_patch', 'to_patch')) %>%
  # calculate the absolute distance between the reofs along each edge
  mutate(reof1 = abs(.N()$reof1[from] - .N()$reof1[to]),
         reof2 = abs(.N()$reof2[from] - .N()$reof2[to]),
         reof3 = abs(.N()$reof3[from] - .N()$reof3[to]),
         reof4 = abs(.N()$reof4[from] - .N()$reof4[to]),
         reof5 = abs(.N()$reof5[from] - .N()$reof5[to]),
         reof6 = abs(.N()$reof6[from] - .N()$reof6[to])) %>%
  # remove redundant edges
  mutate(from_tmp = pmin(from, to), to_tmp = pmax(from, to)) %>%
  group_by(from_tmp, to_tmp, time) %>%
  sample_n(1) %>%
  ungroup %>%
  select(-from_tmp, -to_tmp) %>%
  convert(to_undirected)
```

Convert the network edgelist into a tibble for modeling.
```{r}
dat <- swsn2 %E>%
  as_tibble %>%
  mutate(JSD = if_else(JSD > .999, .999, JSD)) %>%
  mutate(from = as.factor(from), 
         to = as.factor(to), 
         time_fact = as.factor(time)) %>%
  filter(JSD >= .001)
```

## Spatial Interaction Modeling
 
Fit a spatial interaction model to the data using a generalized additive mixed model. First define a function to fit a GAMM to the data, given an arbitrary model specification. We'll use this function to compare the AIC of models fit with EOFs + distance to those with distance alone as a predictor. Use the corMLPE correlation structure to model node-level correlation which may influence the edge data.^[Pairwise data are high powered, which is makes goodness-of-fit estimates preferable to significance testing. Thus set method = 'ML' here to ensure unbiased AIC estimates for model selection. Later use method = 'REML' when exploring the actual estimated functions. The corMLPE correlation structure also helpse here, as well as helping to properly estimate confidence intervals.]

```{r}
fit_gam_JSD <- function(x){
  gamm(x, 
      method = 'ML', 
      family = gaussian(),
      correlation = corMLPE(form = ~from + to|time_fact),
      data = dat)
}
```

Now map this GAMM formulat to a list of model specifications, extracting the AIC from each model fit and ordering the result accordingly. Not that the Jensen-Shannon values are logit transformed before modeling (using qlogis()) to normalize the response data.^[Note that E(logit(y)) >= logit(E(y)). It would be preferable to run the model as a beta regression without logit-transformation, but it is not currently possible to fit the corMLPE in a GAMM correlation structure with the beta family. Tests suggest that the pairwise correlation structure was a larger source of bias than transforming the response variable, so this was a necessary tradeoff.]

```{r gam-aic, cache = TRUE}
models <- c(
  "s(distance, bs = 'cr', k = 6) + time_fact",
  "s(distance, bs = 'cr', k = 6) +
   s(reof1, by = time_fact, bs = 'cs', k = 6) +
   s(reof2, by = time_fact, bs = 'cs', k = 6) +
   s(reof3, by = time_fact, bs = 'cs', k = 6) +
   s(reof4,  by = time_fact, bs = 'cs', k = 6) +
   s(reof5, by = time_fact, bs = 'cs', k = 6) +
   s(reof6, by = time_fact, bs = 'cs', k = 6) + time_fact"
) %>%
  paste('qlogis(JSD) ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(
    formula = map(formula_chr, as.formula),
    model = map(formula, fit_gam_JSD)
  ) %>%
  mutate(
    lme = map(model, ~ .$lme),
    gam = map(model, ~ .$gam),
    aic = map_dbl(lme, AIC)
  ) %>%
  arrange(aic)
```

The best performing model includes distance and REOFs as predictors and had an AIC of `r models[[1,6]]`. In second is the model with distance alone, with an AIC of `r models[[2,6]]`. The REOF + distance model has an adjusted R-squared of `r summary(models[[1,5]])$r.sq`, and the distance only model has an adjusted R-squared of `r summary(models[[2,5]])$r.sq`.

```{r, eval = FALSE}
# additional GAM diagnostics, not run by default
summary(models$gam[[1]])
gam.check(models$gam[[1]])
concurvity(models$gam[[1]])
```


```{r, fig.margin = TRUE, echo = FALSE, cache = TRUE, fig.cap= 'How does the model performance change over time? Rerun the above analysis for each time period separately, and plot the adjusted R-squared for each model at each time period. We see that the REOF models most increase the variance explained over the distance-only model after the period of drought and migration.'}
dat %>%
  group_by(time) %>%
  nest %>%
  mutate(dist_mod = map(
    data,
    ~ gamm(
      qlogis(JSD) ~ s(distance, bs = 'cr', k = 6),
      method = 'ML',
      family = gaussian(),
      correlation = corMLPE(form = ~ from + to),
      data = .
    )
  )) %>%
  mutate(reof_mod = map(
    data,
    ~ gamm(
      qlogis(JSD) ~ s(distance, bs = 'cr', k = 6) +
        s(reof1, bs = 'cs', k = 6) +
        s(reof2, bs = 'cs', k = 6) +
        s(reof3, bs = 'cs', k = 6) +
        s(reof4, bs = 'cs', k = 6) +
        s(reof5,  bs = 'cs', k = 6) +
        s(reof6,  bs = 'cs', k = 6),
      method = 'ML',
      family = gaussian(),
      correlation = corMLPE(form = ~ from + to),
      data = .
    )
  )) %>%
  mutate(
    r2_dist = map_dbl(dist_mod, ~ summary(.$gam)$r.sq),
    r2_reof = map_dbl(reof_mod, ~ summary(.$gam)$r.sq)
  ) %>%
  select(time, r2_dist, r2_reof) %>%
  gather(Model, value, 2:3) %>%
  ggplot(aes(time, value, color = Model)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  scale_color_viridis_d(labels = c('Distance', 'Distance + EOF')) +
  scale_y_continuous(limits = c(.2, .5)) +
  labs(x = 'Time', y = 'Adjusted R-Squared') +
  theme_classic()

ggsave('../figures/r2_time.png', width = 6, height = 4)
```

Now refit the above GAMMs using method = 'REML'. This is the preferred way to do so, but the 'ML' method had to be used above for the AIC model comparison. First fit the distance-only model and plot the resulting distance-decay function.
```{r dist-gam, cache = TRUE}
m1 <- gamm(qlogis(JSD) ~ s(distance, bs = 'cr', k = 6) + time_fact,
           method = 'REML', 
           family = gaussian(),
           correlation = corMLPE(form = ~from + to|time_fact),
           data = dat)
```

```{r, echo = FALSE}
m1$gam %>% 
  getViz %>% 
  sm(1) %>%  
  plot(trans = plogis) + 
  geom_vline(xintercept = c(12, 100), linetype = 2) +
  l_ciPoly(mul = 5) + l_fitLine() + 
  scale_y_continuous(limits = c(0,1)) + 
  labs(x = 'Distance (hours)', y = 'Information flow')

ggsave('../figures/distance_function.pdf', width = 6, height = 4)
```

Now fit the REOF + distance model, and plot the resulting functional forms.
```{r eof-gam, cache = TRUE}
m2 <- gamm(qlogis(JSD) ~ s(distance, bs = 'cr', k = 6) +
             s(reof1, by = time_fact, bs = 'cs', k = 6) +
             s(reof2, by = time_fact, bs = 'cs', k = 6) +
             s(reof3, by = time_fact, bs = 'cs', k = 6) +
             s(reof4,  by = time_fact, bs = 'cs', k = 6) +
             s(reof5, by = time_fact, bs = 'cs', k = 6) +
             s(reof6, by = time_fact, bs = 'cs', k = 6) + time_fact,
           method = 'REML', 
           family = gaussian(),
           correlation = corMLPE(form = ~from + to|time_fact),
           data = dat)
```



```{r fig.height = 6, fig.width = 6, echo = FALSE}
fun_dat <- bind_rows(
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1 = seq(0, .5, .01), reof2 = 0, reof3 = 0, reof4 =0, reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1 = 0, reof2 = seq(0, .5, .01), reof3 = 0, reof4 = 0, reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=seq(0,.5,.01), reof4=0, reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=0, reof4=seq(0,.5,.01), reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=0, reof4=0, reof5 = seq(0,.5,.01), reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=0, reof4=0, reof5 = 0, reof6 = seq(0,.5,.01)), .id = 'reof') %>%
  mutate(reof = paste0('REOF', reof))


m2$gam %>%
  predict.gam(newdata = fun_dat, type = 'response', se.fit = T) %>% 
  bind_cols %>%
  mutate(lower = fit - 2 * se.fit, upper = fit + 2 * se.fit) %>%
  mutate_at(vars(fit,lower, upper), plogis) %>% 
  bind_cols(fun_dat) %>%
  gather(reofs, value,reof1:reof6) %>%
  filter(parse_number(reofs) == parse_number(reof)) %>%
  mutate(reof = str_sub(reof, 2)) %>%
  ggplot(aes(value, fit, group = time_fact)) +  
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = reof), alpha = .3) +
  geom_line(aes(color = reof), size = 1.2) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0,1)) +
  scale_x_continuous(breaks = c(0, .5)) +
  theme_bw() +
  scale_color_viridis_d(guide = 'none') +
  scale_fill_viridis_d(guide = 'none') +
  facet_grid(reof ~ as.ordered(time_fact)) +
  theme_bw() +
  labs(x = 'Climatic Difference', y = 'Cultural Similarity')

ggsave('../figures/smooths.pdf', width = 6, height = 6)
```
 
 
```{r, eval = FALSE, echo = FALSE}
# more diagnostics not run by default
summary(m2$gam)
gam.check(m2$gam)
concurvity(m2$gam)
```


