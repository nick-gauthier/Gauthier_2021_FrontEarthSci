---
title: "Supplemental Information"
subtitle: 'Analysis scripts for "Hydroclimate variability influence social interaction in the prehistoric American Southwest"'
author: "Nicolas Gauthier"
date: "Last knit on: `r format(Sys.time(), '%d %B, %Y')`"
#bibliography: bibliography.bibtex
output:
  tufte::tufte_handout:
    citation_package: natbib
    toc: true
    #highlight: pygments
    
  tufte::tufte_html:
    tufte_features: ["fonts", "italics"]
    toc: true
link-citations: yes
monofont: courier
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE)
```

# Introduction
\begin{fullwidth}
This RMarkdown document contains all analysis code used to produce the results and figures in the main text.

In this two-part analysis we first 
\end{fullwidth}

## Data sources
This R Markdown script requires raster data of [observed present-day temperature and precipitation](http://www.prism.oregonstate.edu/normals/)^[http://www.prism.oregonstate.edu/normals/] 30-year averages and 100-year time series of the [Standardized Precipitation-Evapotranspiration Index](https://wrcc.dri.edu/wwdt/time/)^[https://wrcc.dri.edu/wwdt/time/]. We'll also need reconstructed SPEI rasters^[https://zenodo.org/record/1198817]. Finally, get the CGIAR version of the SRMTM [digital elevation model](http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/)^[http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/], resampled to 250m. With permission, also acquire the [Southwest Social Networks Database](http://www.southwestsocialnetworks.net/data.html)^[http://www.southwestsocialnetworks.net/data.html].

## Packages
```{r, message = FALSE}
library(raster) # raster data manipulation
library(tidyverse) # data analysis and visualization
library(sf)
library(philentropy)
library(tidygraph) # network analysis
library(ggraph) # network visualization
library(broom)
library(gdistance)
library(mgcv) # GAM fitting
library(mgcViz) # GAM plotting
library(gratia)

#devtools::install_github('nspope/corMLPE')
library(corMLPE)
library(furrr)

#devtools::install_github('thomasp85/patchwork') 
library(patchwork)
```

# Climate Analysis: Drought Patterns

First we estimate present and past climate patterns.

## Study Area

First we define a boundary box covering much of the western United States, ranging between W$124.5\deg$ and W$-107\deg$ and N$31\deg$ and N$37.5\deg$, which we'll use to constrain all subsequent climate analyses. This study area is significantly larger than the one we'll define in the next section for the network analysis. This allows us to sample a much wider range of climatic variability, while still remaining within the broader western US climate zone. This ensures that both A) our statistical analyses will be more robust to sampling error and B) the results will be less sensitive to the exact location and dimensions of our study area.
      
```{r}
bbox_wus <- extent(c(-124.5, -102, 30, 42.1))
bbox_swsn <- extent(c(-113, -107, 31, 37.5))
```

```{r, echo = FALSE}
state_names <- c('arizona', 'new mexico', 'colorado', 
                 'california', 'utah', 'nevada')

states_wus <- maps::map('state', regions = state_names, 
                    fill = TRUE, plot = FALSE) %>% st_as_sf

states_swsn <- maps::map('state', regions = c('arizona', 'new mexico'), 
                    fill = TRUE, plot = FALSE) %>% st_as_sf

country <- maps::map('usa',fill = TRUE, plot = FALSE) %>% st_as_sf
```


```{r bbox_map, fig.margin=TRUE, echo = FALSE, fig.cap = 'Locations of 2 bounding boxes.'}
ggplot() + 
  geom_sf(data = states_wus) +
  geom_sf(data = country, fill = NA) +
  geom_sf(data = st_as_sfc(st_bbox(bbox_wus, crs = st_crs(4326))), fill = NA, color = 'red') +
  geom_sf(data = st_as_sfc(st_bbox(bbox_swsn, crs = st_crs(4326))), fill = NA, color = 'red') +
  theme_bw()
```

## Climate Data
Import high resolution SPEI maps generated from PRISM data. Calculate the average summertime (JJA) SPEI value for each year.

```{r import_observations, warning=FALSE}
# calculate average JJA SPEI
spei_obs <- list.files('data/PRISM', full.names = TRUE) %>%
  map(brick) %>%
  map(crop, bbox_wus) %>%  # crop to the study area bounding box
  reduce(`+`) %>%
  `/`(3) %>%
  `names<-`(1895:2017) %>% # add year names
  .[[-1]] # SPEI calculated on 12 month lag, so drop 1st year
```

Import the reconstructed SPEI fields from PHYDA. PHYDA uses a novel off-line data assimilation approach, using simulated SPEI from the CESM LME experiments as physically-consistent model priors and a network of tree rings, ice cores, and corals as assimilated observations. The ensemble Kalman filter uses this information, as well as the spatial covariances from the climate-model prior (e.g. teleconnections), to ``spread out'' information from the point-based proxies to generate physically optimal extrapolations beyond the proxy locations.

```{r import_reconstructions, cache = TRUE}
spei_recon <- brick('data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc',
                    varname = 'spei_mn') %>%
   .[[1100:1999]] %>% # extract years of interest
   rotate %>% # rotate longitudes to -180 to 180
   crop(bbox_wus, snap = 'near')
```

Let's compare the reconstructions for the summer of 1985.
```{r, echo = FALSE, fig.cap = 'Observed and reconstructed SPEI for summer 1985.'}
brick(c(spei_obs[[90]], resample(spei_recon[[886]], spei_obs, method = 'ngb'))) %>%
  `names<-`(c('Observed', 'Reconstructed')) %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(type, spei, 3:4) %>%
  ggplot(aes(x, y)) +
  geom_raster(aes(fill = spei)) +
  facet_wrap(~type) +
  scale_fill_distiller(palette = 'Spectral', direction = 1,
                       limits = c(-3, 3),
                       guide = 'legend') +
  coord_quickmap() +
  theme_void()
```

```{r plot_variance_trend, echo = FALSE, message = FALSE, fig.margin=TRUE, fig.cap = 'Plotting out the time series of the reconstruction reveals no clear trend in the mean. It does, however, show a pattern of increasing variance over time, which is entirely a function of the changing number of proxies used in the data assimilation approach.'}
spei_recon %>% 
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, value, 3:902) %>%
  mutate(year = str_sub(year, 2),
         year = as.numeric(year)) %>%
  ggplot(aes(year, value)) +
  geom_line(alpha = .1, aes(group = interaction(x, y))) +
  scale_y_continuous(limits = c(-5,5)) +
  geom_smooth() +
  ggtitle('Standardized Precipitation-Evapotranspiration Index in the American Southwest',
          'June-August, 12 month lag') + 
  theme_bw()
```

## Empirical Orthogonal Functions

We calculate the leading Empirical Orthogonal Functions of a ~100 year series of the summertime average Standardized Precipitation-Evapotranspiration Index. These patterns then undergo a varimax rotation to highlight more physically meaningful spatial patterns. We compare these patterns to those derived from a long term (~2ka) reconstruction based on data assimilation and CESM LME. We confirm that the spatial patterns detected for the past 100 years have been robust over time and explain up to 85% of the variance in a full 1,000 year sequence of reconstructed drought dynamics.

Although the particular choices of variables, resolution, domain size, truncation level, were informed by theory and tested to as to minimize sensitivity to particular choices, different researchers could still generate equally reasonable results given  different input parameters.

This is a function to reweight our observations based on the latitude, to acount for the areal distortion of each cell as latitude changes.

```{r}
area_weight <- function(x){
  names_x <- names(x)
  x %>%
    init('y') %>% # get a map of latitudes
    `*`(pi/180) %>% # convert to radians
    cos %>%
    sqrt %>%
    `*`(x) %>%
    `names<-`(names_x)
}
```

Principal components analysis of observation and recon data

```{r}
# these are adpated from wql and sinkr
obs_pca <- spei_obs %>%
  area_weight %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  prcomp(scale. = FALSE) # use (scale. = FALSE) because spei is already normalized and rescaled
```

Now we calculate the rotated empirical orthogonal functions for both the observed and reconstructed drought maps. For the observations, we see that the leading 5 eofs explain 85% of the variance in the series, so we'll retain those for rotation.

This function calculates the effective observations in an autocorrelated time series of rasters.

```{R}
n_effective <- function(x){
  n <- nlayers(x)
  x %>%
    area_weight %>%
    as.data.frame(na.rm = TRUE) %>%
    t %>%
    as_tibble %>%
    gather(cell, value) %>%
    nest(data = c(value)) %>%
    mutate(rho = map_dbl(data, ~cor(.$value, lag(.$value), use = 'comp')),
           effective_n = n * (1 - rho^2) / (1 + rho^2)) %>% # from Bretherton et al 1999
    summarise(mean(effective_n)) %>%
    pull
}
```

we want to look for separation in the error bars. eofs that are not well separated can be considered effective multiplets, and hsould not be split in truncation.

```{r}
obs_eigs <- obs_pca %>%
  tidy(matrix = 'pcs') %>%
  mutate(eigenvalues = std.dev ^ 2,
         error = sqrt(2 / n_effective(spei_obs)),
         low =  eigenvalues * (1 - error) * 100 / sum(eigenvalues),
         hi = eigenvalues * (1 + error) * 100 / sum(eigenvalues),
         cumvar_line = hi + 0.02 * max(hi))
```

Repeat all of the above for the reconstructions too.
```{r}
recon_pca <- spei_recon %>%
  area_weight %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  prcomp(scale. = FALSE)

recon_eigs <- recon_pca %>% 
  tidy(matrix = 'pcs') %>%
  mutate(eigenvalues = std.dev ^ 2,
         error = sqrt(2 / n_effective(spei_recon)),
         low =  eigenvalues * (1 - error) * 100 / sum(eigenvalues),
         hi = eigenvalues * (1 + error) * 100 / sum(eigenvalues),
         cumvar_line = hi + 0.02 * max(hi))
```

```{r plot_variance_obs, fig.width = 5, fig.height = 4, echo = FALSE, fig.cap = 'Variance explained'}
obs_eigs %>% 
  mutate(separated = if_else(is.na(lag(low)), TRUE, hi < lag(low)),
                   separated_colors = cumsum(separated),
         weights = if_else(PC < 50, 0, 1))%>%
    filter(PC <= 12) %>%
ggplot(aes(x = PC, y = percent * 100)) +
  geom_errorbar(aes(x = PC, ymin = low, ymax = hi), width = 0.4) +
  geom_point(size = 2, aes(color = as.factor(separated_colors))) + 
  geom_text(aes(x = PC, y = cumvar_line, label = paste0(round(cumulative * 100, 0), '%')), size = 2.5, vjust = 0) +
  labs(x = "Principal Component", y = "Normalized Eigenvalue") + 
  geom_vline(xintercept = 6.5, linetype = 2, color = 'red', alpha = .7) +
  theme_bw() + 
  guides(color = F) + 
  scale_x_continuous(breaks = seq(0, 12, 2))
```


the overlaps mean these are effective multiplets. the real eof can be some linear combination of these. This won't in practice impact our successive results, as long as we don't truncate within these multplets, but only between them.
using the log linear test, keep 7 of recon and 6 for obs
Look at the variances for the eofs of each field, to inform truncation. Let's retain the leading four modes from PHYDA

```{r plot_variance_phyda, fig.width = 5, fig.height = 4, fig.margin=TRUE, echo = FALSE, fig.cap = 'Variance explained'}
recon_eigs %>%
  mutate(separated = if_else(is.na(lag(low)), TRUE, hi < lag(low)),
          test = cumsum(separated),
         weights = if_else(PC < 3, 0, 1))%>%
  filter(PC <=12) %>%
ggplot(aes(x = PC, y = percent * 100)) +
  geom_errorbar(aes(x = PC, ymin = low, ymax = hi), width = 0.4) +
  geom_point(size = 3, aes(color = as.factor(test))) + 
  geom_text(aes(x = PC, y = cumvar_line, label = round(cumulative * 100, 0)), size = 2.5, vjust = 0) +
  labs(x = "PC", y = "Normalized Eigenvalue") +
  theme_bw() + 
  guides(color = FALSE) + 
  scale_x_continuous(breaks = seq(0, 12, 2))
```

Repeat for the prism observations. Again, we'll retain the leading four modes.


It looks like there is some autocorrelation in phyda but not in the observations, will have to adjust in the future.

```{r}
get_EOFs <- function(pc_object, eigs, rast_template, n_modes){
  eofs <- pc_object %>%
    tidy(matrix = 'variables') %>%
    filter(PC <= n_modes) %>%
    left_join(eigs[1:2], by = 'PC') %>%
    mutate(eof = value * std.dev,
           PC = as.character(PC)) %>%
    select(-std.dev, -value) 
  
  varim <- eofs %>% 
    pivot_wider(names_from = PC, values_from = eof) %>%
    column_to_rownames(var = 'column') %>%
    as.matrix %>%
    varimax # varimax rotation
  
  rot_mat <- varim$rotmat
  
  reofs <- unclass(varim$loadings) %>%
    as_tibble(rownames = 'column') %>%
    pivot_longer(-column, names_to = 'PC', values_to = 'reof') %>%
    right_join(eofs, by = c('column', 'PC'))
  
  eof_amps <- pc_object$x %>%
    .[,1:n_modes] %>%
    scale %>%
    as_tibble(rownames = 'year', .name_repair = ~1:n_modes) %>%
    gather(PC, eof_amp, -year) %>%
    mutate(year = as.numeric(str_sub(year, 2)),
           PC = as.factor(paste0('PC', PC)))
  
  reof_amps <- pc_object$x %>%
    .[,1:n_modes] %>%
    scale %>%
    `%*%`(rot_mat) %>%
    as_tibble(rownames = 'year', .name_repair = ~1:n_modes) %>%
    gather(PC, reof_amp, -year) %>%
    mutate(year = as.numeric(str_sub(year, 2)),
           PC = as.factor(paste0('PC', PC)))
  
  reofs <- rast_template[[1]] %>%
    as.data.frame(xy = TRUE, na.rm = TRUE) %>% 
    .[1:2] %>%
    slice(rep(1:n(), times = n_modes)) %>%
    bind_cols(reofs, .) %>%
    select(-column) %>%
    mutate(PC = paste0('EOF', PC))
  
  list(EOFs = reofs, 
       amps = left_join(eof_amps, reof_amps, by = c('year', 'PC')), 
       rotater = rot_mat)
}
```

Now, calculate the eofs for both fields, retaining the 6 leading components in each case for rotation

```{r calc_eofs}
n_modes <- 6
```

```{r}
reof_obs <- get_EOFs(obs_pca, obs_eigs, spei_obs, n_modes)
reof_recon <- get_EOFs(recon_pca, recon_eigs, spei_recon, n_modes)
```



We see that the leading 3 eigenvectors from the observations and reconstructions are a good match, including their ordering. 4-6 look good too

```{r reof_raster, warning = FALSE}
#Save the reof data as a raster, so we can crop it to the state boundaries
reof_raster <- reof_obs$EOFs %>%
  select(-eof) %>%
  spread(PC, reof)  %>%
  rasterFromXYZ %>%
  `crs<-`(value = '+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0')%>%
  crop(states_wus) %>%
  mask(states_wus)

# invert reof 6 for better visualization
reof_raster[[6]] <- reof_raster[[6]] * -1
```

```{r, include = FALSE}
writeRaster(reof_raster, '../output/reofs.tif', overwrite = TRUE)
```

```{r, include = FALSE, warning = FALSE}
eof_raster <- reof_obs$EOFs %>%
  select(-reof) %>%
  spread(PC, eof)  %>%
  rasterFromXYZ %>%
  `crs<-`(value = '+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0')%>%
  crop(states_wus) %>%
  mask(states_wus)
```



Let's map out the spatial and temporal patterns in the REOFs. First we'll look at the spatial patterns. It looks like the observed and reconstructed datasets reveal very similar spatial patterns for the 4 leading modes.

```{r plot_obs_reof, echo = FALSE, fig.fullwidth = TRUE, fig.width = 6, fig.height = 7, fig.cap = 'Observed drought REOFs'}
reof_raster %>%
  as.data.frame(xy = TRUE, na.rm = TRUE, long = TRUE) %>%
ggplot() +
  geom_raster(aes(x, y, fill = value)) +
  scale_fill_viridis_c(option = 'magma', name = 'Correlation') +
  facet_wrap(~layer, ncol = 2) +
  theme_void() +
  geom_sf(data = states_wus, color = 'black', fill = NA) +
  coord_sf(datum = NA) +
  theme(legend.position = "bottom")

ggsave('../figures/reof_observed.pdf', height = 7, width = 6)
```

```{r, echo = FALSE, fig.margin = FALSE, fig.caption = 'Climate regionalization.'}
which.max(abs(reof_raster)) %>%
  as.data.frame(xy = T, na.rm = T)  %>%
  ggplot() +
  geom_raster(aes(x,y, fill = as.ordered(layer))) +
  scale_fill_viridis_d(name = 'Dominant EOF') +
  theme_void() +
  geom_sf(data = states_wus, color = 'black', fill = NA) +
  coord_sf(datum = NA)
```

```{r, echo = FALSE, fig.margin = TRUE, fig.cap='Observed Empirical Orthogonal Functions before rotation.'}
as.data.frame(eof_raster, xy = TRUE, na.rm = TRUE, long= TRUE) %>%
ggplot() +
  facet_wrap(~layer, ncol = 2) +
  geom_raster(aes(x, y, fill = value)) +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-.82, .82)) +
  theme_void() + 
    geom_sf(data = states_wus, color = 'black', fill = NA) +
  coord_sf(datum = NA) +
  theme(legend.position = "bottom")
```

```{r, echo = FALSE, fig.margin = TRUE, fig.cap='Reconstructed Empirical Orthogonal Functions before rotation.'}
ggplot(reof_recon$EOFs, aes(x, y, fill = eof)) +
  facet_wrap(~PC) +
  geom_raster() +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-.72, .72)) +
  theme_void() + 
  coord_quickmap()
```

The colors here are eigenvectors weighted by the sqare root of the associated eigenvalue, so these loadings represent the covariance between each grid cell and each amplitude (principal component). A key assumption here is that the REOFs calculated from both the observations and reconstructions correspond to the same physical phenomena. That way we can just use the observed patterns as the high resolution patterns, and infer their temporal evolution from the reconstructions. This means we don't have to downscale the reconstructed reofs to use them, as otherwise we'd have to deal with issues of spatial represntativeness and non-overlap. Now we can be ensured that these are the same signals.

```{r, echo = FALSE}
## todo, clean up all the inversions and put them here
# now the reofs for the recons don't match, fix:
reof_recon$amps <- reof_recon$amps %>%
  mutate(PC = case_when(PC == 'PC1' ~ 'PC2',
                        PC == 'PC2' ~ 'PC4',
                         PC == 'PC3' ~ 'PC5',
                         PC == 'PC4' ~ 'PC3',
                         PC == 'PC5' ~ 'PC1',
                         PC == 'PC6' ~ 'PC6')) %>%
  arrange(PC) %>%
  mutate(reof_amp = if_else(PC %in% c('PC2', 'PC5', 'PC6'), reof_amp * -1, reof_amp))

reof_recon$EOFs <- reof_recon$EOFs %>%
  mutate(PC = case_when(PC == 'EOF1' ~ 'EOF2',
                        PC == 'EOF2' ~ 'EOF4',
                         PC == 'EOF3' ~ 'EOF5',
                         PC == 'EOF4' ~ 'EOF3',
                         PC == 'EOF5' ~ 'EOF1',
                         PC == 'EOF6' ~ 'EOF6')) %>%
  arrange(PC)
```

```{r plot_recon_reofs, fig.fullwidth = TRUE, echo = FALSE, fig.cap = 'Reconstructed drought REOFs'}
reof_recon$EOFs %>%
  mutate(reof = if_else(PC %in% c('EOF2', 'EOF5', 'EOF6'), reof * -1, reof)) %>%
  ggplot() +
  geom_raster(aes(x,y,fill = reof)) +
    scale_fill_viridis_c(option = 'magma') +
  facet_wrap(~PC) +
  coord_sf(datum = NA) +
  theme_void()
```




To confirm this, let's plot the time series (PCs) of each observed and reconstructed REOF against each other, to confirm that they correspond to the same patterns. Look at the correlations between the modes.

## Drought Amplitudes




```{r, echo = FALSE, include=FALSE}
# Not shown.
# For each PC amplitude time series, plot out the wettest and driest years from the observations.
# From this we can tell that all but PC6 is of the right sign (high pc amplitude == high positive SPEI). 
# So next we multiply PC6 by -1 
reof_obs$amps %>%
  group_by(PC) %>%
  filter(reof_amp == max(reof_amp) | reof_amp == min(reof_amp)) %>%
  pull(year) %>%
  `-`(1895) %>%
  spei_obs[[.]] %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, SPEI, X1915:X2008) %>%
  mutate(year = factor(year, levels = paste0('X', c(1915, 1996 ,1987, 2011, 1934, 1984, 2014, 2017, 1947, 1963, 1977, 2008)))) %>%
  ggplot(aes(x, y, fill = SPEI)) +
  geom_raster() +
  facet_wrap(~year) +
  coord_quickmap() +
  scale_fill_distiller(palette = 'Spectral', limits = c(NA, 4), direction = 1) +
  theme_void()
```

```{r, echo = FALSE}
reof_obs$amps <- reof_obs$amps %>%
  mutate(reof_amp = if_else(PC == 'PC6', reof_amp * -1, reof_amp))
```


```{r echo = FALSE, fig.width=6, fig.height=4, warning = FALSE}
reof_obs$amps %>%
  group_by(PC) %>%
  mutate(amp_smooth = zoo::rollmeanr(reof_amp, k = 10, fill = NA)) %>%
ggplot(aes(year, reof_amp, color = PC)) +
    geom_line(alpha = .4) +
  geom_line(aes(y = amp_smooth), size = 1.2) +
  facet_wrap(~PC) + 
  geom_hline(yintercept = 0, linetype = 2, alpha= 1, color = 'black') + 
  scale_color_viridis_d(guide = FALSE) +
  theme_bw() +
  labs(x = 'Year', y = 'SPEI')

ggsave('../figures/pc_obs.pdf', width = 6, height = 4)
```

```{r, echo = FALSE, fig.margin = TRUE}
reof_recon$amps %>%
  filter(year >= 1896) %>%
  group_by(PC) %>%
  mutate(amp_smooth = zoo::rollmeanr(reof_amp, k = 10, fill = NA)) %>%
ggplot(aes(year, reof_amp, color = PC)) +
  geom_line(alpha = .4) +
  geom_line(aes(y = amp_smooth), size = 1.2) +
  facet_wrap(~PC) + 
  geom_hline(yintercept = 0, linetype = 2, alpha= 1, color = 'black') + 
  scale_color_viridis_d(guide = FALSE) +
  theme_bw() +
  labs(x = 'Year', y = 'SPEI')
```


Now let's fit these models in practice. First create a data frame of past amplitudes.
```{r past_amplitudes}
amplitudes_modern <- reof_obs$amps %>%
  inner_join(reof_recon$amps, by = c('year', 'PC'),
            suffix = c('_obs', '_recon'))
amplitudes_past <- reof_recon$amps %>%
  select(-reof_amp) %>%
  rename(amplitude_recon = eof_amp) %>%
  group_by(PC) %>%
  nest(recons = c(year, amplitude_recon))
```

Look for the correlation between the amplitude time series to make sure we have a good match between the two sets of patterns.
```{r, plot_amplitudes, fig.fullwidth=TRUE, fig.height=4, fig.width=10, echo = FALSE, fig.cap = 'Strong linear correlation between the observed and reconstructed EOFs'}
#do the eofs match?
ggplot(amplitudes_modern, aes(reof_amp_recon, reof_amp_obs)) +
  geom_point(aes(color = year)) +
  geom_smooth(method = 'lm') +
  scale_color_viridis_c() +
  facet_wrap(~PC, nrow = 1) +
  labs(title = 'Linear fits between observed and reconstructed amplitudes', subtitle = '1896-1999',
       x = 'Reconstructed', y = 'Observed') +
  theme_minimal() +
  coord_fixed() +
  theme(legend.position = 'bottom')
```


How have the amplitudes of these patterns changed over time? Let's plot the reconstructed time series for each, along with an estimated trend line.
```{r plot_recon_amplitudes, fig.width = 9, fig.height = 4, echo = FALSE, fig.fullwidth=TRUE}
reof_recon$amps %>%
    mutate(amp_smooth = zoo::rollmeanr(reof_amp, k = 10, fill = NA)) %>%
  filter(between(year, 1200, 1450)) %>% 
ggplot(aes(year, reof_amp, group = PC)) +
  geom_line(aes(color = PC), alpha = .3) +
  geom_line(aes(y = amp_smooth, color = PC), size = 1.2) +
  scale_color_viridis_d(guide = FALSE) +
  facet_wrap(~PC) +
  theme_bw()+
  theme(legend.position = "bottom") +  geom_hline(yintercept = 0, color = 'black', linetype = 2)+  geom_vline(xintercept = 1275, color = 'red', linetype = 2) +
  labs(x = 'Year', y = 'SPEI (Reconstructed)')
```

These results point to REOFs 2 and 3 as having major shocks at around 1300 AD.
```{r, eval = FALSE, fig.margin=TRUE, fig.cap='Are there any time periods with marked signals?'}
reof_recon$amps %>%
  mutate(period = floor((year / 50)) * 50) %>%
  filter(between(period, 1200, 1400)) %>%
  ggplot(aes((reof_amp), as.factor(period),fill = ..x.., height = ..density..)) +
    ggridges::geom_density_ridges_gradient(stat = 'density', scale = 1.5)+
  facet_wrap(~PC) +
  scale_fill_viridis_c(guide = F) +
  geom_vline(xintercept= 0, linetype = 2) +
  ggridges::theme_ridges(grid = FALSE, center_axis_labels = TRUE)
```


```{r, echo = FALSE, message=FALSE, warning = FALSE}
rm(spei_obs, spei_recon, obs_pca);gc()
```

# Network Analysis: Spatial Interaction Modeling

First generate an estimate of the prehistoric social network using archaeological proxies from the Southest Social Networks project.^[Go to http://www.southwestsocialnetworks.net for more information on this project and dataset] First we read in a csv file containing information for all of the archaeological sites in the database, including spatial location and number of rooms present from each time period. These will become the node-level data for the network.

```{r}
crs_utm <- '+proj=utm +zone=12 +datum=NAD27'
```

```{r, message = FALSE}
sites <- read_csv('data/attributes_orig.csv') %>%
  st_as_sf(coords = c('EASTING', 'NORTHING'), crs = crs_utm) %>%
  st_transform('+proj=longlat +datum=WGS84')
```

Next we get the apportioned ceramic ware counts per site.
```{r}
load('ware_matrices')
wares <- list(AD1200cer, AD1250cer, AD1300cer, 
              AD1350cer, AD1400cer) %>%
  map(rownames_to_column) %>%
  bind_rows(.id = 'time') %>%
  mutate(time = as.numeric(time) * 50 + 1150) %>%
  rename(site = rowname)
```

```{r, fig.fullwidth = TRUE, echo = FALSE}
wares %>%
  gather(ware, count, 3:38) %>%
  group_by(ware, time) %>%
  summarise(Proportion = sum(count)) %>%
ggplot(aes(time, Proportion, fill = ware, group = ware)) +
  geom_bar(position='fill', stat = 'identity', color = 'black') +
  theme_bw()
```
Next we aggregate the sites into 10x10km patches. This is because an area of about 10 km radius from a site is the limit of raw material resources for ceramics (TODO cite Arnold1985). So 5km is roughly the limit of day to day social interaction, 10km is raw material procurement for ceramics, and 18 km is for day's journey round trip (TODO cite varien). translated to square units in equivalent areas, that's 3km square cells for regular face-to-face interaction and intensive agriculture. 12km for food and non food resources, and 32km grids for day's trip there and back. So 12 is the most reasonable. First import and crop the raster we'll use as a reference.
```{r}
elev <- raster('~/gdrive/Data/SRTM_W_250m_TIF/SRTM_W_250m.tif') %>% # import the SRTM DEM
  crop(bbox_swsn) %>% # crop to study area
  aggregate(fact = 40) # aggregate to 10km resolution
```

```{r, echo = FALSE}
#from https://stackoverflow.com/a/46846474
calculate_mode <- function(x) {
  uniqx <- unique(x)
  uniqx[which.max(tabulate(match(x, uniqx)))]
}
```

```{r}
patches <- sites %>%
  mutate(., patch_id = as.character(cellFromXY(elev, st_coordinates(.)))) %>%
  gather(period, rooms, P1room:P5room) %>%
  mutate(time = parse_number(period) * 50 + 1150) %>%
  group_by(patch_id, time) %>%
  summarise(Macro = calculate_mode(Macro),
            site = list(SWSN_Site),
            rooms = sum(rooms)) %>%
  ungroup %>%
  spread(time, rooms) %>%
  cbind(., xyFromCell(elev, as.numeric(.$patch_id))) %>%
  as.data.frame %>%
  st_as_sf(coords = c('x','y'), crs = 4326, remove = FALSE)
```

```{r}
theo_max <- -.5 * log(.5) - .5 * log(.5) # maximum theoretical divergence given equal weights
```

```{r}
getJSD <- function(data, time){
  patch_ids <- pull(data, patch_id)
  data %>%
    select(-patch_id) %>%
    select_if(~!all(. == 0)) %>%
    as.matrix %>%
    philentropy::distance(method = 'jensen-shannon', est.prob = 'empirical') %>%
    `/`(., theo_max) %>%
    sqrt %>%
    `-`(1,.) %>%
    `rownames<-`(patch_ids) %>%
    `colnames<-`(patch_ids) %>%
    replace(. == 0, 999) %>% # replace 0 values with 999 temporarily
    as_tbl_graph %E>% # convert to directed graph
    rename(JSD = weight) %>%
    mutate(JSD = if_else(JSD == 999, 0, JSD)) %>% # convert 999 values back to 0
    mutate(time = time) %>% # set the time period
    filter(!edge_is_loop()) %>%
  activate(nodes) 
}
```

```{r}
swsn <- patches %>%
  gather(time, rooms, X1200:X1400) %>%
  mutate(time = parse_number(time)) %>%
  as.data.frame %>%
  select(patch_id, site, time) %>%
  unnest(cols = c(site)) %>%
  right_join(wares, by = c('site', 'time')) %>%
  as.data.frame %>%
  select(-site) %>%
  group_by(time, patch_id) %>%
  summarise_all(sum, na.rm = T) %>%
  nest %>%
  mutate(net = map2(data, time, getJSD)) %>%
  pull(net) %>%
  reduce(graph_join, by = 'name') %>%
  left_join(patches, by = c('name' = 'patch_id'))
```

```{r, echo = FALSE}
swsn %N>%
  mutate(Rooms = X1200 + X1250 + X1300 + X1350 + X1400) %>%
  arrange(Rooms) %>%
  ggraph('manual', x = x, y = y) +
      geom_sf(data = states_swsn, fill = NA, color = 'black') +
  geom_node_point(aes(size = Rooms, color = Rooms)) +
  scale_size_area(limits = c(0, 3000), breaks = seq(0, 3000, by = 500)) +
  scale_color_viridis(option = 'magma', limits = c(0, 3000), breaks = seq(0, 3000, by = 500), guide = 'legend') +
  coord_sf(datum = NA) +
  theme_void()  +
  theme(legend.position = c(1, 0), legend.justification = c(2.6,-.3))
```


```{r fig.fullwidth = TRUE, fig.width = 9, fig.height = 6, echo = FALSE}
swsn %E>% 
    filter(JSD > .001) %>%
    filter(from < to) %>%
    arrange(JSD) %>% 
ggraph('manual', x = x, y = y) +
  geom_edge_fan(aes( color = JSD)) +
  geom_sf(data = states_swsn, fill = NA, color = 'black') +
  facet_edges(~time, ncol = 3) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(name = 'Similarity', option = 'magma') +
  coord_sf(datum = NA) +
  theme_void() +
  theme(legend.position = c(1, 0), legend.justification = c(2.5,0))
```

```{r,echo=TRUE, fig.margin = TRUE, fig.width=4, fig.height = 8}
swsn %E>% 
    filter(JSD > .001) %>%
    filter(from < to) %>%
    mutate(JSD = qlogis(if_else(JSD > .999, .999, JSD))) %>%
    arrange(JSD) %>% 
ggraph('manual', x = x, y = y) +
  geom_edge_fan(aes( color = JSD)) +
  geom_sf(data = states_swsn, fill = NA, color = 'black') +
  facet_edges(~time, ncol = 2) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(name = 'Similarity', option = 'magma') +
  coord_sf(datum = NA) +
  theme_void() +
  theme(legend.position = c(1, 0), legend.justification = c(2.5,0))
```


```{r, fig.width= 10, fig.height = 5, fig.fullwidth = TRUE, echo = FALSE}
a <- swsn %N>%
  mutate(Rooms = X1200 + X1250 + X1300 + X1350 + X1400) %>%
  arrange(Rooms) %>%
  ggraph('manual', x = x, y = y) +
      geom_sf(data = states_swsn, fill = NA, color = 'black') +
  geom_node_point(aes(size = Rooms, color = Rooms)) +
  scale_size_area(limits = c(0, 3000), breaks = seq(0, 3000, by = 500)) +
  scale_color_viridis(option = 'magma', limits = c(0, 3000), breaks = seq(0, 3000, by = 500), guide = 'legend') +
  coord_sf(datum = NA) +
  theme_void()  +
  theme(legend.position = c(1, 0), legend.justification = c(2.6,-.3))

b <- swsn %E>% 
    filter(JSD > .001) %>%
    filter(from < to) %>%
    arrange(JSD) %>% 
ggraph('manual', x = x, y = y) +
  geom_edge_fan(aes( color = JSD)) +
  geom_sf(data = states_swsn, fill = NA, color = 'black') +
  facet_edges(~time, ncol = 2) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(name = 'Similarity', option = 'magma') +
  coord_sf(datum = NA) +
  theme_void() +
  theme(legend.position = c(1, 0), legend.justification = c(1.25, -1), legend.direction = 'horizontal')

c <- a + b + plot_layout(heights = c(1, 1)) + plot_annotation(tag_levels = 'A')
ggsave('swsn2.png', plot = c, height = 5, width = 10)
```

Comparing the two approaches show that the using the coarse grained patches approach reduces the number of long distance strong connections. This is likely do to the fact htat we're removing the impact of small sites with only one or two ware types, which can introduce spurious high correlations.

# Terrain Analysis: Least Cost Distances

Next we calculate the impact of rugged terrain on the potential flow of people and information between the sites in the SWSN database. For this we need to use the SRTM digital elevation model. From the height data in the DEM, we calculate slope, accounting for cognitive biases people have when assessing the steepness of high slopes (people tend to exaggerate slopes above a certain threshold). From this map of perceived slope, we calculate "perceived walking speeds", using Tobler's hiking function. We uses these perceived walking speeds as a measure of the perceived, symmetric costs of traveling between two locations on the landscape, and from these estimate the least cost paths from every site to every site. From there, we extract the 15 nearest neighbors of each site in each time period, and use this nearest neighbor network as a spatial network, from which to estimate new travel costs (limited to paths along this nearest neighbor network).

We focus on the perceived time costs of travel. People will consistently overestimate slopes, so the perceived cognitive costs of moving across the landscape will be greater. In rugged, mountainous terrain, slope is the key factor in influencing ease of travel. We focus specifically on paths, or the physical routes that people traveled. A good path connecting two places has to optimize for ease of travel in both directions. The resulting least-cost network will be used as a proxy measure for the constraints on moving both people and bulk goods across the landscape, and thus the topographic affordances for social exchange.

## Digital Elevation Model

First import the elevation dataset, crop to the study area, and resample.

```{r elevation}
elev_lcp <- raster('~/gdrive/Data/SRTM_W_250m_TIF/SRTM_W_250m.tif') %>% # import the SRTM DEM
  crop(bbox_swsn) %>% # crop to study area
  aggregate(fact = 2) # aggregate to ~500m resolution
```

## Hiking Speeds

Now we use the package *gdistance* to calculate the time it would take for a hiker to traverse this landscape on foot. Tobler's hiking function allows us to convert terrain slope to expected walking speed. We modify this function to account for cognitive bias in the perception of slope, which causes people to overestimate the steepness of steeper slopes. 

The maximum walking speed of 5 assumes on path movement.
```{r}
tobler_adjusted <- function(x){
  4 * exp(-3.5 * 2.15 * x) / 3.6 # 3.6 turns km/h into m/s
}
```

```{r, fig.margin = TRUE, echo = FALSE, fig.cap = "Tobler's hiking function"}
slope <- seq(-1, 1, .01)
qplot(slope, tobler_adjusted(abs(slope)) * 3.6, geom = 'line' ) +
  geom_vline(xintercept = 0, linetype = 2) +
  labs(x = 'Slope (rise over run)', y = 'Walking speed (km/h)') +
  theme_classic()
```

This package uses a transition matrix approach, whereby we work with matrices that contain the costs or transmissiveness of travel from each cell to its immediate 16 neighbor cells. This allows for a sparse matrix representation, because there is no possible connection to non neighboring cells, which considerably reduces the computational burden.

We're representing paths, not necessarily routes, and so focus on symmetric costs. This is an additional modification to tobler.

Calculate the absolute difference in elevation between each cell and its 16 neighbors.
```{r, cache = TRUE}
altDiff <- function(x){abs(x[2] - x[1])}
hd <- transition(elev_lcp, altDiff, 16, symm = TRUE)
```
Divide the height differences by the horizontal distances between cells, resulting in slopes.
```{r, cache = TRUE}
slope_c <- geoCorrection(hd, type = 'c')
```
Figure out which cells are adjacent to one another, queen's case.
```{r, cache = TRUE}
adj <- adjacent(elev_lcp, cells = 1:ncell(elev_lcp), directions = 16)
```
Use Tobler's hiking function to calculate walking speed from *cognitive* slope.
```{r, cache = TRUE}
speed_c <- slope_c
speed_c[adj] <- tobler_adjusted(slope_c[adj])
```
Divide by intercell distance again, resulting in the conductance matrix.
```{r, cache = TRUE}
conductance_c <- geoCorrection(speed_c, type = 'c')
```

```{r, echo = FALSE, fig.fullwidth = TRUE, fig.cap = 'Conductance map.'}
speed_c %>%
  raster %>%
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
ggplot(aes(x, y, fill = layer * 3.6)) + # convert back to km/h
  geom_raster() +
  scale_fill_viridis_c(guide = 'legend', name = 'Walking\nspeed (km/h)', limits = c(NA,4)) +
  coord_quickmap() +
  labs(x = 'Longitude', y = 'Latitude') +
  theme_minimal()
```

```{r, echo = FALSE, message=FALSE, warning = FALSE}
rm(altDiff, hd, slope_c, speed_c, adj);gc()
```

## Least-cost Distances

Using the conductance matrix, calculate the full pairwise least cost distance matrix.
The points of sites are originally in utm zone 12, so reproject to latlong

```{r least-cost-distances, cache = TRUE}
distances <- swsn %>%
  as_tibble %>%
  st_as_sf %>%
  st_coordinates%>%
  costDistance(conductance_c, .) %>% # least cost distances
  as.matrix %>% 
  `colnames<-`(as_tibble(swsn)$name) %>%
  as_tibble %>%
  mutate(from_patch = as_tibble(swsn)$name) %>%
  gather(to_patch, distance, -from_patch) %>%
  mutate(distance = distance / 3600)
```


# Network analysis


Import the data produced in the other analyses to add to the sites data.
```{r}
reof_resamp <- reof_raster %>%
  crop(bbox_swsn) %>%
  resample(elev)
```

```{r, fig.margin = TRUE, echo = FALSE}
plot(reof_resamp)
```

```{r, fig.margin = TRUE, echo = FALSE}
pairs(reof_resamp)
```


## Data Preparation

Now that we have all the data imported and preprocessed, let's put it all together into a data frame.

Define a function to calculate the relative difference between two numbers. We use this becuase it is a measure of multiplicative distance, rather than additive such as euclidean distance, which makes the values more robust to deviations.

```{r data_preparation}
swsn2 <- swsn %N>%
  mutate(reof1 = raster::extract(reof_resamp, st_coordinates(geometry))[,1],
         reof2 = raster::extract(reof_resamp, st_coordinates(geometry))[,2],
         reof3 = raster::extract(reof_resamp, st_coordinates(geometry))[,3],
         reof4 = raster::extract(reof_resamp, st_coordinates(geometry))[,4],
         reof5 = raster::extract(reof_resamp, st_coordinates(geometry))[,5],
         reof6 = raster::extract(reof_resamp, st_coordinates(geometry))[,6]) %E>%
  mutate(from_patch = .N()$name[from],
         to_patch = .N()$name[to]) %>%
  left_join(distances, by = c('from_patch', 'to_patch')) %>%
  mutate(reof1 = abs(.N()$reof1[from] - .N()$reof1[to]),
         reof2 = abs(.N()$reof2[from] - .N()$reof2[to]),
         reof3 = abs(.N()$reof3[from] - .N()$reof3[to]),
         reof4 = abs(.N()$reof4[from] - .N()$reof4[to]),
         reof5 = abs(.N()$reof5[from] - .N()$reof5[to]),
         reof6 = abs(.N()$reof6[from] - .N()$reof6[to])) %>%
  mutate(from_tmp = pmin(from, to), to_tmp = pmax(from, to)) %>%
  group_by(from_tmp, to_tmp, time) %>%
  sample_n(1) %>%
  ungroup %>%
  select(-from_tmp, -to_tmp) %>%
  convert(to_undirected)
```



```{r,echo = FALSE, fig.fullwidth = TRUE}
r1 <- swsn2 %E>%
  arrange(reof1) %>%
  filter(reof1 > .1) %>%
ggraph('manual', x = x, y = y ) +
  geom_edge_fan(aes(alpha = reof1, color = reof1)) +
  scale_edge_alpha(guide = 'none', limits = c(0,1)) +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
  #scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(0,1)) +
    facet_edges(~time, ncol = 1) +
  coord_quickmap() +
  theme_void()
r2 <- swsn2 %E>%
  arrange(reof2) %>%
    arrange(reof2) %>%
  filter(reof2 > .1) %>%
ggraph('manual', x = x, y = y ) +
  geom_edge_fan(aes(alpha = reof2, color = reof2)) +
  scale_edge_alpha(guide = 'none', limits = c(0,1)) +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
  #scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(0,1)) +
    facet_edges(~time, ncol = 1) +
  coord_quickmap() +
  theme_void()
r3 <- swsn2 %E>%
  arrange(reof3) %>%
    arrange(reof3) %>%
  filter(reof3 > .1) %>%
ggraph('manual', x = x, y = y ) +
  geom_edge_fan(aes(alpha = reof3, color = reof3)) +
  scale_edge_alpha(guide = 'none', limits = c(0,1)) +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
  #scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(0,1)) +
    facet_edges(~time, ncol = 1) +
  coord_quickmap() +
  theme_void()
r4 <- swsn2 %E>%
  arrange(reof4) %>%
    arrange(reof4) %>%
  filter(reof4 > .1) %>%
ggraph('manual', x = x, y = y ) +
  geom_edge_fan(aes(alpha = reof4, color = reof4)) +
  scale_edge_alpha(guide = 'none', limits = c(0,1)) +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
  #scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(0,1)) +
    facet_edges(~time, ncol = 1) +
  coord_quickmap() +
  theme_void()
r5 <- swsn2 %E>%
  arrange(reof5) %>%
    arrange(reof5) %>%
  filter(reof5 > .1) %>%
ggraph('manual', x = x, y = y ) +
  geom_edge_fan(aes(alpha = reof5, color = reof5)) +
  scale_edge_alpha(guide = 'none', limits = c(0,1)) +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
  #scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(0,1)) +
    facet_edges(~time, ncol = 1) +
  coord_quickmap() +
  theme_void()
r6 <- swsn2 %E>%
  arrange(reof6) %>%
  arrange(reof6) %>%
  filter(reof6 > .1) %>%
  ggraph('manual', x = x, y = y ) +
  geom_edge_fan(aes(alpha = reof6, color = reof6)) +
  scale_edge_alpha(guide = 'none', limits = c(0,1)) +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
  #scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(0,1)) +
    facet_edges(~time, ncol = 1) +
  coord_quickmap() +
  theme_void()

r1 + r2 + r3 + r4 + r5 + r6 + plot_layout(nrow = 1)
```

```{r}
dat <- swsn2 %E>%
  as_tibble %>%
  mutate(JSD = if_else(JSD > .999, .999, JSD)) %>%
  mutate(from = as.factor(from), 
         to = as.factor(to), 
         time_fact = as.factor(time)) %>%
  filter(JSD >= .001)
```


jensen inequality, E(logit(y)) >=logit(E(y)). this means we should interpret specific predictions and confidence intervals from this model with caution, and instead only focus on interpretaiton of the brod functional forms. Its a necessary evil here, becuase we do not yet have the computational abiliy to fit the corMLPE correlation structure and a beta family. Preliminary tests with the data suggested that the pairwise correlation structure was a much alrger source of bias then transforming the response variabble,so that's the tradeoffwe went with. The bias from transforming the respone leads to bias in the estimates of variance.

 allows the model to be estimated additively.
 
```{r, echo= FALSE}
#this room plot shows a process of aggregation over time (mean room size increases)
ggplot(dat, aes(JSD)) +
  geom_histogram() + facet_wrap(~time)
# look at the plot of ws_mul, basically its all the links with low productivity that die out and the ones with high that survive!
```
```{r, echo = FALSE, fig.margin = TRUE}
ggplot(dat, aes(qlogis(JSD))) +
  geom_histogram() + facet_wrap(~time)
```


 
 Pairwise data are high powered, which is makes goodness of fit estimates are preferable over statistical significance testing, and we must be careful to properly estimate confidence intervals,

```{r}
fit_gam_JSD <- function(x){
  gamm(x, 
      method = 'ML', 
      family = gaussian(),
      correlation = corMLPE(form = ~from + to|time_fact),
      data = dat)
}
```


```{r, cache = TRUE}
plan(multisession, workers = 2)

models <- c(
 "s(distance, bs = 'cr', k = 6) + time_fact",
       "s(distance, bs = 'cr', k = 6) +
          s(reof1, by = time_fact, bs = 'cs', k = 6) +
          s(reof2, by = time_fact, bs = 'cs', k = 6) +
       s(reof3, by = time_fact, bs = 'cs', k = 6) +
       s(reof4,  by = time_fact, bs = 'cs', k = 6) +
      s(reof5, by = time_fact, bs = 'cs', k = 6) +
       s(reof6, by = time_fact, bs = 'cs', k = 6) + time_fact"
   ) %>%
  paste('qlogis(JSD) ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = future_map(formula, fit_gam_JSD)) %>%
  mutate(lme = purrr::map(model, ~.$lme),
         gam = purrr::map(model, ~.$gam),
         aic = map_dbl(lme, AIC)) %>%
  arrange(aic)
```

```{r}
models
```
This may suggest areas of increased conflict, where the kind of social interaction through which pottery would have been exchanged would be rare, or other unmodeled costs of travel such as river crossings cultural taboos. 
```{r, eval = FALSE}
gam.check(models$gam[[1]])
```

We have moderate to strong concurvity
```{r}
concurvity(models$gam[[1]])
```

%This represents that multiple transmission processes, with different relationships to distance, are acting together to generate the variation in the archaeological record. The two main inflection points in the decay curve are both consistent with previous work in the ecology of human mobility. Their is a plateau effect distance plays no appreciable role in interaction between sites within a day's walk of one another (johnson but cites olsson 1965 and crumley 1979, ariadne). This suggests that distance only becomes a deterring factor dealing in multiday trips. The second inflection point, at roughly 10 days is consistent with estimates of area needed to sustain the minimum viable breeding population for human groups (are they consistent with the figures of 50km-80km from rautman and drennen and others? johnson finds no marriages past 60km, maize to chaco papers also have distance).
%Plateau of social interaction with communities who have daily face-to-face contact.

%uses log transform
%The Mogollon Rim acts as a severe bottleneck for spatial flows, with paths generally constrained to one of the three river systems, the Verde, Salt, and Gila, act to channel flow across this boundary. 


```{r, cache = TRUE}
m1 <- gamm(qlogis(JSD) ~ s(distance, bs = 'cr', k = 6) + time_fact,
           method = 'REML', 
           family = gaussian(),
           correlation = corMLPE(form = ~from + to|time_fact),
           data = dat)
```

```{r, cache = TRUE}
m2 <- gamm(qlogis(JSD) ~ s(distance, bs = 'cr', k = 6) +
             s(reof1, by = time_fact, bs = 'cs', k = 6) +
             s(reof2, by = time_fact, bs = 'cs', k = 6) +
             s(reof3, by = time_fact, bs = 'cs', k = 6) +
             s(reof4,  by = time_fact, bs = 'cs', k = 6) +
             s(reof5, by = time_fact, bs = 'cs', k = 6) +
             s(reof6, by = time_fact, bs = 'cs', k = 6) + time_fact,
           method = 'REML', 
           family = gaussian(),
           correlation = corMLPE(form = ~from + to|time_fact),
           data = dat)
```
```{r}
summary(m1$gam)
summary(m2$gam)
```

```{r, eval = FALSE}
gam.check(m2$gam)
concurvity(m2$gam)
```

```{r, echo = FALSE}
m1$gam %>% 
  getViz %>% 
  sm(1) %>%  
  plot(trans = plogis) + 
  geom_vline(xintercept = c(12, 100), linetype = 2) +
  l_ciPoly(mul = 5) + l_fitLine() + 
  scale_y_continuous(limits = c(0,1)) + 
  labs(x = 'Distance (hours)', y = 'Information flow')

ggsave('../figures/distance_function.pdf', width = 6, height = 4)
```


```{r fig.height = 6, fig.width = 6, echo = FALSE}
fun_dat <- bind_rows(
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1 = seq(0, .5, .01), reof2 = 0, reof3 = 0, reof4 =0, reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1 = 0, reof2 = seq(0, .5, .01), reof3 = 0, reof4 = 0, reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=seq(0,.5,.01), reof4=0, reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=0, reof4=seq(0,.5,.01), reof5 = 0, reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=0, reof4=0, reof5 = seq(0,.5,.01), reof6 = 0),
  expand.grid(distance = 100, time_fact = factor(c('1200','1250','1300','1350','1400')), 
              reof1=0,reof2=0,reof3=0, reof4=0, reof5 = 0, reof6 = seq(0,.5,.01)), .id = 'reof') %>%
  mutate(reof = paste0('REOF', reof))


m2$gam %>%
  predict.gam(newdata = fun_dat, type = 'response', se.fit = T) %>% 
  bind_cols %>%
  mutate(lower = fit - 2 * se.fit, upper = fit + 2 * se.fit) %>%
  mutate_at(vars(fit,lower, upper), plogis) %>% 
  bind_cols(fun_dat) %>%
  gather(reofs, value,reof1:reof6) %>%
  filter(parse_number(reofs) == parse_number(reof)) %>%
  mutate(reof = str_sub(reof, 2)) %>%
  ggplot(aes(value, fit, group = time_fact)) +  
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = reof), alpha = .3) +
  geom_line(aes(color = reof), size = 1.2) +
  scale_y_continuous(breaks = c(0, 1), limits = c(0,1)) +
  scale_x_continuous(breaks = c(0, .5)) +
  theme_bw() +
  scale_color_viridis_d(guide = 'none') +
  scale_fill_viridis_d(guide = 'none') +
  facet_grid(reof ~ as.ordered(time_fact)) +
  theme_bw() +
  labs(x = 'Climatic Difference', y = 'Cultural Similarity')

ggsave('../figures/smooths.pdf', width = 6, height = 6)
```
 
 
```{r}
knitr::knit_exit()
```

how do the fits change over time?
```{r}
fit_gam_JSD <- function(x){
  gamm(x, method = 'ML', 
            family = gaussian(),
      correlation = corMLPE(form = ~from + to|time_fact),
      data = dat)
}
plan(multicore, workers = 2)

models <- c(
 "s(distance, bs = 'cr', k = 6) + time_fact",
       "s(distance, bs = 'cr', k = 6) +
          s(reof1, by = time_fact, bs = 'cs', k = 6) +
          s(reof2, by = time_fact, bs = 'cs', k = 6) +
       s(reof3, by = time_fact, bs = 'cs', k = 6) +
       s(reof4,  by = time_fact, bs = 'cs', k = 6) +
      s(reof5, by = time_fact, bs = 'cs', k = 6) +
       s(reof6, by = time_fact, bs = 'cs', k = 6) + time_fact"
   ) %>%
  paste('qlogis(JSD) ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = future_map(formula, fit_gam_JSD)) %>%
  mutate(lme = purrr::map(model, ~.$lme),
         gam = purrr::map(model, ~.$gam),
         aic = map_dbl(lme, AIC)) %>%
  arrange(aic)


t1 <- dat %>% group_by(time) %>% nest %>%
  mutate(dist_mod = map(data, ~gamm(qlogis(JSD) ~ s(distance, bs = 'cr', k = 6), method = 'ML', 
            family = gaussian(),
      correlation = corMLPE(form = ~from + to),
      data = .))) %>%
  mutate(reof_mod = map(data, ~gamm(qlogis(JSD) ~ s(distance, bs = 'cr', k = 6) +
          s(reof1, bs = 'cs', k = 6) +
          s(reof2,bs = 'cs', k = 6) +
       s(reof3, bs = 'cs', k = 6) +
       s(reof4, bs = 'cs', k = 6) +
      s(reof5,  bs = 'cs', k = 6) +
       s(reof6,  bs = 'cs', k = 6), method = 'ML', 
            family = gaussian(),
      correlation = corMLPE(form = ~from + to),
      data = .)))

t1 %>% 
  mutate(r2_dist = map_dbl(dist_mod, ~summary(.$gam)$r.sq),
         r2_reof = map_dbl(reof_mod, ~summary(.$gam)$r.sq)) %>%
  select(time, r2_dist, r2_reof) %>%
  gather(Model, value, 2:3) %>%
  ggplot(aes(time, value, color = Model)) +
  geom_line(size = 1.2) + 
  geom_point(size = 3) +
  scale_color_viridis_d(labels = c('Distance','Distance + EOF')) +
  scale_y_continuous(limits = c(.2, .5)) +
  labs(x = 'Time', y = 'Adjusted R-Squared') +
  theme_classic()

ggsave('../figures/r2_time.png', width = 6, height = 4)
```



```{r fig.width = 6, fig.height = 4}
#think about changing the residual plots back to magma so its easier to compare across them
a <- plt_dat %>%
arrange(JSD) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = JSD, color = JSD)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
   labs(subtitle = 'A') +
  theme_void() +
  coord_sf(datum = NA)

b <- plt_dat %>%
arrange(pred_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = pred_null, color = pred_null)) +
  scale_edge_alpha(guide = 'none') +
 scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
   labs(subtitle = 'B') +
  theme_void() +
  coord_sf(datum = NA)

c <- plt_dat %>%
arrange(res_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = res_null, color = res_null)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
   labs(subtitle = '') +
  theme_void() +
  coord_sf(datum = NA)

d <- plt_dat %>%
arrange(-res_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = -1 * res_null, color = res_null)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
  labs(subtitle = '') +
  theme_void() +
  coord_sf(datum = NA)

e <- plt_dat %>%
arrange(pred_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = pred_mod, color = pred_mod)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
   labs(subtitle = 'C') +
  theme_void() +
  coord_sf(datum = NA)

f <- plt_dat %>%
arrange(res_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = res_mod, color = res_mod)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) +
   labs(subtitle = '') +
  theme_void() +
  coord_sf(datum = NA)

g <- plt_dat %>%
arrange(-res_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = -1 * res_mod, color = res_mod)) +
  scale_edge_alpha(guide = 'none') +
   scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
  labs(subtitle = '') +
  theme_void() +
  coord_sf(datum = NA)

a+(b+c+d+ e+f+g + plot_layout(nrow = 2))
plt_dat %E>% as_tibble() %>% summary
ggsave('figures/residuals.png', width = 6, height = 4)
```

```{r fig.width = 6, fig.height = 6}
plt_dat %E>% as_tibble %>% summary
a <- plt_dat %>%
arrange(JSD) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = JSD, color = JSD)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
   labs(subtitle = 'A') +
  facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

b <- plt_dat %>%
arrange(pred_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = pred_null, color = pred_null)) +
  scale_edge_alpha(guide = 'none') +
 scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
    facet_wrap(~time, nrow = 1) +
   labs(subtitle = 'B') +
  theme_void() +
  coord_sf(datum = NA)

c <- plt_dat %>%
arrange(res_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = res_null, color = res_null)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
   labs(subtitle = '') +
        facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

d <- plt_dat %>%
arrange(-res_null) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = -1 * res_null, color = res_null)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
  labs(subtitle = '') +
        facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

e <- plt_dat %>%
arrange(pred_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = pred_mod, color = pred_mod)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_viridis(option = 'magma', guide = 'none', limits = c(0,1)) +
   labs(subtitle = 'C') +
    facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

f <- plt_dat %>%
arrange(res_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = res_mod, color = res_mod)) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) +
   labs(subtitle = '') +
      facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

g <- plt_dat %>%
arrange(-res_mod) %>%
ggraph(x = x, y = y) +
  geom_edge_fan(aes(alpha = -1 * res_mod, color = res_mod)) +
  scale_edge_alpha(guide = 'none') +
   scale_edge_color_distiller(palette = 'RdYlBu', guide = 'none', name = 'JSD', limits = c(-1,1)) + 
  labs(subtitle = '') +
      facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_sf(datum = NA)

a+(b+c+d+ e+f+g + plot_layout(nrow = 2))
a+ b + e + f + g + plot_layout(nrow = 5)
ggsave('figures/residuals.png', width = 6, height = 4)
```



We fit 3 models to predict the prsence of an interaction tie between two sites. The best performing model including distance, size, and reofs as predictors, with an aic of `r logistic_models[[1,4]]`. In second is the model with distance, size, and reofs as predictors, with an aic of `r logistic_models[[2,4]]`. Third is the base geographic model with distance and size, with an AIC of `r logistic_models[[3,4]]`. That said, even the best fitting model only captures 30% of the variance in tie presence. This means we are still missing key variables that drive the presence or absence of social relations.

# References {#references .unnumbered}
