---
title: "Supplemental Information"
subtitle: 'Analysis scripts for "Drought Variability and the Robustness of Agrarian Social Networks"'
author: "Nicolas Gauthier"
date: "Last knit on: `r format(Sys.time(), '%d %B, %Y')`"
#bibliography: bibliography.bibtex
output:
  tufte::tufte_handout:
    citation_package: natbib
    toc: true
    highlight: pygments
  tufte::tufte_html:
    tufte_features: ["fonts", "italics"]
    toc: true
link-citations: yes
monofont: courier
---

# Introduction
\begin{fullwidth}
This RMarkdown document contains all analysis code used to produce the results and figures in the main text.

In this two-part analysis we first 
\end{fullwidth}

## Data sources
This R Markdown script requires raster data of [observed present-day temperature and precipitation](http://www.prism.oregonstate.edu/normals/)^[http://www.prism.oregonstate.edu/normals/] 30-year averages and 100-year time series of the [Standardized Precipitation-Evapotranspiration Index](https://wrcc.dri.edu/wwdt/time/)^[https://wrcc.dri.edu/wwdt/time/]. We'll also need reconstructed SPEI rasters^[https://zenodo.org/record/1198817]. Finally, get the CGIAR version of the SRMTM [digital elevation model](http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/)^[http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/], resampled to 250m. With permission, also acquire the [Southwest Social Networks Database](http://www.southwestsocialnetworks.net/data.html)^[http://www.southwestsocialnetworks.net/data.html].

## Packages

Load the packages used in the analyses.

```{r packages, message = FALSE}
library(raster) # raster data manipulation
library(tidyverse) # data analysis and plotting
library(sf)
library(tidygraph) # network analysis
library(ggraph) # network plotting
library(ggridges)

library(mgcv) # GAM fitting
library(mgcViz) # GAM plotting

#devtools::install_github('nspope/corMLPE')
library(corMLPE)
library(furrr)
```

```{r include = FALSE}
theme_set(theme_bw())
```

# Network Analysis: Spatial Interaction Modeling

Now we move on to the archaeological social network proxies.^[Go to http://www.southwestsocialnetworks.net for more information on this dataset and the project] First we read in a csv file containing information for all of the archaeological sites in the database, including spatial location and number of rooms present from each time period. These will become the node-level data for the network.

```{r}
crs_utm <- '+proj=utm +zone=12 +datum=NAD27'
```

```{r, message = FALSE}
sites <- read_csv('data/attributes_orig.csv') %>%
  st_as_sf(coords = c('EASTING', 'NORTHING'), crs = crs_utm) %>%
  st_transform('+proj=longlat +datum=WGS84')
```

```{r, echo = FALSE}
knitr::kable(
  sites[1:3,c(1:4,8:10)], caption = 'A subset of the node level data.'
)
```

Next import the edge data. Our data come in the form of similarity matrices, one for each time step. We need to define a function for importing and processing a single time step of the edge datasets. This function imports the csv, turns it into a matrix, and then a directed graph. 

```{r read_swsn}
read_swsn <- function(net, time){
  read.csv(net, row.names = 1, check.names = FALSE) %>% 
    as.matrix %>% # convert to a matrix
    replace(. == 0, 999) %>% # replace 0 values with 999 temporarily
    as_tbl_graph(directed = TRUE) %E>% # convert to directed graph
    mutate(weight = if_else(weight == 999, 0, weight)) %>% # convert 999 values back to 0
    rename(similarity = weight) %>% # rename the edge weights
    filter(!edge_is_loop()) %>% # remove self loops
    mutate(time = 1150 + 50 * time) %>%  # set the time period
    activate('nodes') # activate node data
}
```

Row normalization allows us to compare the edges without the node effects, measuring the similarities onthe same scale.

Next we map this function over a lists of simlaritity matrices. Get a list of the similarity files, map the above function over them, reduce to a single graph.  Use reduce and graph join to combiine all of these networks into a single netwtork, retaining the edge weights for each  separate period.

Start by getting a list of the similarity file names, map the read_swsn function over these files, combine into a single graph using the reduce function, and then join to the site database.

```{r swsn_import}
swsn <- list.files('data/Sim', full.names = TRUE) %>%
  imap(read_swsn) %>%
  reduce(graph_join, by = 'name') %>%
  left_join(sites, by = c('name' = 'SWSN_Site')) 
```

```{r echo = FALSE, fig.margin = TRUE, fig.cap = 'Example 6x6 simililarity matrix. Note the symmetry'}
sim_tmp <- read.csv('data/Sim/AD1200sim.csv', row.names = 1, check.names = FALSE) %>%
  as.matrix %>%
  .[1:3,1:3]
knitr::kable(sim_tmp, caption = 'Example similarity matrix')
```

```{r}
states <- maps::map('state', regions = c('arizona', 'new mexico'), 
                    fill = TRUE, plot = FALSE) %>%
  st_as_sf
```

```{r network_plot, echo = FALSE}
# igraph is very particular about how it wants the points for plotting,
# requires a data frame with x and y columns
pts <- swsn %N>% # use swsn instead of sites to get the order right
  pull(geometry) %>%
  st_coordinates %>%
  as_tibble %>%
  rename(x = X, y = Y)

plot_swsn <- function(net, ncols = 3){
  net %E>%
    filter(similarity > 0) %>%
    filter(from < to) %>%
    arrange(similarity) %>%
  ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = similarity, color = similarity)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time, ncol = ncols) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'legend', name = 'Similarity') +
  coord_sf(datum = NA) +
  theme_void() +
  theme(legend.position = c(1, 0), legend.justification = c(2.5, 0))
}

plot_swsn(swsn, ncols = 2)
ggsave('figures/similarity_network.tiff', width = 7)
```

## Data Extraction

Import the data produced in the toher analyses to add to the sites data.

```{r}
reof_raster <- brick('output/reofs.tif')  %>%
  `names<-`(., paste0('reof', 1:nlayers(.)))
eof_raster <- brick('output/eofs.tif')  %>%
  `names<-`(., paste0('eof', 1:nlayers(.)))

reof_dat <- reof_raster %>%
  raster::extract(sites, df = TRUE) %>%
  cbind(sites$SWSN_Site, .) %>%
  select(-ID) %>%
  rename(name = `sites$SWSN_Site`)
eof_dat <- eof_raster %>%
  raster::extract(sites, df = TRUE) %>%
  cbind(sites$SWSN_Site, .) %>%
  select(-ID) %>%
  rename(name = `sites$SWSN_Site`)
```

```{r, echo = FALSE}
eof_raster %>%
  crop(states) %>%
  mask(states) %>%
  as.data.frame(xy = TRUE, na.rm = TRUE, long = TRUE) %>%
  ggplot() +
  geom_raster(aes(x, y, fill = value)) +
  scale_fill_distiller(palette = 'BrBG', limits = c(-1, 1)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_wrap(~layer) +
  coord_sf(datum = NA) +
  theme_void()
```


```{r, message = FALSE}
distances <- read_csv('output/distances.csv')
```


## Data Preparation

Now that we have all the data imported and preprocessed, let's put it all together into a data frame.

Define a function to calculate the relative difference between two numbers. We use this becuase it is a measure of multiplicative distance, rather than additive such as euclidean distance, which makes the values more robust to deviations.


```{r, eval = FALSE}
  mutate(dist_geo = geosphere::distGeo(matrix(c(.N()$lon[from], .N()$lat[from]), ncol = 2), 
                            matrix(c(.N()$lon[to], .N()$lat[to]), ncol = 2)) / 1000)
```


```{r data_preparation}
swsn_reof <- swsn %E>%
  filter(from < to) %>%
  convert(to_undirected) %>%
  morph(to_split, time, split_by = 'edges') %N>% # split into subgraphs
  mutate(degree = centrality_degree(weights = similarity)) %E>%
  mutate(size = .N()$degree[from] * .N()$degree[to]) %N>%
  select(-degree) %>%
  unmorph %>%
  left_join(eof_dat) %>%
  left_join(reof_dat) %E>%
  mutate(from_site = .N()$name[from], 
         to_site = .N()$name[to]) %>%
  left_join(distances) %>%
  select(-c(from_site:to_site)) %>%
  mutate(
    rooms = case_when(
      time == 1200 ~ sqrt(.N()$P1room[from] * .N()$P1room[to]),
      time == 1250 ~ sqrt(.N()$P2room[from] * .N()$P2room[to]),
      time == 1300 ~ sqrt(.N()$P3room[from] * .N()$P3room[to]),
      time == 1350 ~ sqrt(.N()$P4room[from] * .N()$P4room[to]),
      time == 1400 ~ sqrt(.N()$P5room[from] * .N()$P5room[to])),
  reof1 = abs(.N()$reof1[from] - .N()$reof1[to]),
  reof2 = abs(.N()$reof2[from] - .N()$reof2[to]),
  reof3 = abs(.N()$reof3[from] - .N()$reof3[to]),
  reof4 = abs(.N()$reof4[from] - .N()$reof4[to]),
  #reof5 = abs(.N()$reof5[from] - .N()$reof5[to]),
 # reof6 = abs(.N()$reof6[from] - .N()$reof6[to]),
  eof1 = abs(.N()$eof1[from] - .N()$eof1[to]),
  eof2 = abs(.N()$eof2[from] - .N()$eof2[to]),
  eof3 = abs(.N()$eof3[from] - .N()$eof3[to]),
  eof4 = abs(.N()$eof4[from] - .N()$eof4[to]),
  #eof5 = abs(.N()$eof5[from] - .N()$eof5[to]),
 # eof6 = abs(.N()$eof6[from] - .N()$eof6[to]),
  macro_same = .N()$Macro[from] == .N()$Macro[to],
  micro_same = .N()$Micro[from] == .N()$Micro[to])
```

```{r}
dat <- swsn_reof %E>%
  as_tibble %>%
  mutate(from = as.factor(from), to = as.factor(to), time = as.factor(time)) %>%
  mutate(tie = similarity > 0) %>%
  mutate(similarity = if_else(similarity > .999, .999, similarity))
```

```{r}
  ggplot(dat, aes(log(rooms))) +
  geom_histogram() + facet_wrap(~time)
 ggplot(dat, aes(size)) +
  geom_histogram() + facet_wrap(~time)
  ggplot(dat, aes((distance))) +
  geom_histogram() + facet_wrap(~time)
```
```{r}
fit_gam_similarity <- function(x, y){
  gam(x, method = 'ML', 
      family = betar(),
     # correlation = corMLPE(form = ~from + to),
      data = filter(dat, tie == TRUE))
}
plan(multicore, workers = 3)

geo_models <- c(
  "s(distance, bs = 'cr')+ time",
  "s(size, bs = 'cr') + time",
 "s(distance, bs = 'cr') + s(size, bs = 'cr') + time",
    "s(distance, bs = 'cr') + s(size, bs = 'cr') + time + s(eof1, bs = 'cs') + s(eof2, bs = 'cs') + 
        s(eof3,  bs = 'cs') + s(eof4, bs = 'cs')",
     "s(distance, bs = 'cr') + s(size, bs = 'cr') + time + s(reof1, bs = 'cs') + s(reof2, bs = 'cs') + 
        s(reof3,  bs = 'cs') + s(reof4, bs = 'cs')"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = future_map(formula, fit_gam_similarity)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
summary(geo_models$model[[1]])
plot(geo_models$model[[1]], scale = -1, trans = plogis)
```



We want to compare a couple different ways of fitting the model
```{r}
test_models <- c(
 "s(distance, bs = 'cr') + s(size, bs = 'cr') + time",
 "s(distance, bs = 'cr') + s(log(size), bs = 'cr') + time",
 "te(distance, size) + time",
 "te(distance, log(size)) + time",
 "s(distance, bs = 'cr') + log(size) + time",
 "s(distance, bs = 'cr') + size + time"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = future_map(formula, fit_gam_similarity)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)

plot(test_models$model[[2]], scheme = 1)
summary(test_models$model[[1]])
gam.check(test_models$model[[1]])
```

```{r}
m1 <- dat %>%
  filter(tie == TRUE) %>%
  bam(similarity ~ s(log(size), by = time) + time, data = ., select = TRUE, discrete = FALSE, family = betar)
plot(m1, scale= 0)
gam.check(m1)
summary(m1)

swsn_reof %E>% 
  filter(similarity > 0) %>%
  mutate(pred = predict(test_models$model[[1]], newdata = as_tibble(.), type = 'response')) %>%
  mutate(res = similarity - pred) %>%
  arrange(pred) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = pred, color = pred)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_sf(datum = NA) +
  theme_void()
swsn_reof %E>% 
  filter(similarity > 0) %>%
  mutate(pred = predict(test_models$model[[2]], newdata = as_tibble(.), type = 'response')) %>%
  mutate(res = similarity - pred) %>%
  arrange(pred) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = pred, color = pred)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_sf(datum = NA) +
  theme_void()
swsn_reof %E>% 
  filter(similarity > 0) %>%
  mutate(pred = predict(geo_models$model[[1]], newdata = as_tibble(.), type = 'response')) %>%
  mutate(res = similarity - pred) %>%
  arrange(pred) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = pred, color = pred)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_sf(datum = NA) +
  theme_void()
```

```{r}
m2 <- dat %>%
  filter(tie == TRUE) %>%
  bam(similarity ~ s(distance) + 
        s(size) + s(rooms) + 
        s(reof1, time, bs = 'fs') + s(reof2, time, bs = 'fs') + 
        s(reof3, time, bs = 'fs') + s(reof4, time, bs = 'fs') + 
        s(reof5, time, bs = 'fs') + s(reof6, time, bs = 'fs'), 
      data = ., select = TRUE, discrete = TRUE, nthreads = 2, family = betar)
plot(m2, trans = plogis)
gam.check(m2)
summary(m2)

swsn_reof %E>% 
  filter(similarity > 0) %>%
  mutate(pred = predict(m2, newdata = as_tibble(.), type = 'response')) %>%
  arrange(pred) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = pred, color = pred)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_sf(datum = NA) +
  theme_void()
```

```{r}
test1 <- swsn_reof %>%
  filter(similarity > 0,
         from < to) #%>%

test1 %>%
  as_tibble() %>%
  select(from, to, time, reof1:reof6) %>%
  gather(reof, value, reof1:reof6) %>%
  sample_frac(.5) %>%
 as_tbl_graph() %E>%
  arrange(value) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = value, color = value)) +
  facet_graph(time ~ reof, col_type = 'edge') +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_quickmap() +
  theme_void()
```
```{r}
test1 %>%
  as_tibble() %>%
  select(from, to, time, reof1_from:reof_) %>%
  gather(reof, value, reof1:reof6) %>%
  sample_frac(.5) %>%
 as_tbl_graph() %E>%
  arrange(value) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = value, color = value)) +
  facet_graph(time ~ reof, col_type = 'edge') +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_quickmap() +
  theme_void()
```
We use a time factor variable, in part, to allow for the varying amplitudes of each 
```{r}
swsn_norm <- swsn_reof %>%
  morph(to_split, time, split_by = 'edges') %N>% # split into subgraphs
  mutate(outflow = centrality_degree(weights = similarity, mode = 'out')) %E>%
  #mutate(trans_weight = similarity / .N()$outflow[from]) %>%
  mutate(rel_size = .N()$outflow[from] * .N()$outflow[to],
         test = similarity * sum(similarity) / rel_size) %N>%
  select(-outflow) %>%
  unmorph %E>%
  filter(similarity > 0) %>%
  filter(from < to) %>%
  mutate(test = (test - 1) / (test + 1), test = (test+ 1) / 2)

mod1 <- swsn_norm %E>%
  as_tibble %>%
  mutate(from = as.factor(from), to = as.factor(to), time = as.factor(time)) %>%
  mutate(tie = similarity > 0) %>%
  mutate(time_num = as.numeric(time)) %>%
  mutate(similarity = if_else(similarity > .999, .999, similarity)) %>%
  filter(tie == TRUE) %>% 
  bam(test ~ s(distance, bs = 'cr', k = 6) +
         te(reof1, time_num, k = c(6, 4), fx = c(F,T)) +
         te(reof2, time_num, k = c(6, 4), fx = c(F,T)) +
         te(reof3, time_num, k = c(6, 4), fx = c(F,T)) +
         te(reof4, time_num, k = c(6, 4), fx = c(F,T)) +
         te(reof5, time_num, k = c(6, 4), fx = c(F,T)) +
         te(reof6, time_num, k = c(6, 4), fx = c(F,T)),
        #s(as.factor(to), as.factor, bs = 're') + time,
      data = .,
      method = 'fREML',
      select = TRUE,
      discrete = TRUE,
      nthreads = 2,
      family = betar)
summary(mod1)

plot(mod1, scheme = 2, trans = plogis)
```

```{r}
mod2 <- swsn_norm %E>%
  as_tibble %>%
  mutate(from = as.factor(from), to = as.factor(to), time = as.factor(time)) %>%
  mutate(tie = similarity > 0) %>%
  mutate(time_num = as.numeric(time)) %>%
  mutate(similarity = if_else(similarity > .999, .999, similarity)) %>%
  filter(tie == TRUE) %>% 
  bam(test ~ s(distance, bs = 'cr', k = 4),# +
       #  s(reof1, bs = 'cr', k = 6) +
       #    s(reof2, bs = 'cr', k = 6) +
       #   s(reof3, bs = 'cr', k = 6) +
       #   s(reof4, bs = 'cr', k = 6) +
       # s(reof5, bs = 'cr', k = 6) +
       #  s(reof6, bs = 'cr', k = 6) +
       #   te(reof1, time_num, k = c(4, 4), fx = c(F,T)) +
       #   te(reof2, time_num, k = c(4, 4), fx = c(F,T)) +
       #   te(reof3, time_num, k = c(4, 4), fx = c(F,T)) +
       #   te(reof4, time_num, k = c(4, 4), fx = c(F,T)) +
       #   te(reof5, time_num, k = c(4, 4), fx = c(F,T)) +
       #   te(reof6, time_num, k = c(4, 4), fx = c(F,T)),
        #s(as.factor(to), as.factor, bs = 're') + time,
      data = .,
      method = 'fREML',
      select = TRUE,
      discrete = TRUE,
      nthreads = 2,
      family = betar)
summary(mod2)
plot(mod2, trans = plogis, scale = 0)
```

```{r, echo = FALSE}
knitr::kable(
  dat[1:6,], caption = 'A subset of the edge level data.'
)
```

```{r}
swsn_reof %E>%
  filter(similarity > 0) %>%
  filter(from < to) %>%
  arrange(similarity) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = similarity, color = similarity)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_sf(datum = NA) +
  theme_void()

swsn_reof %E>%
  filter(similarity > 0) %>%
  filter(from < to) %>%
  mutate(similarity = if_else(similarity > .999, .999, similarity)) %>%
  mutate(similarity = qlogis(similarity)) %>%
  arrange(similarity) %>% 
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = similarity, color = similarity)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_sf(datum = NA) +
  theme_void()


swsn_reof %E>%
  filter(similarity > 0) %>%
  filter(from < to) %>%
  mutate(time_num = as.numeric(time)) %>%
  mutate(., pred = predict(mod2, as_tibble(.))) %>% pull(pred)
  arrange(pred) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = pred, color = pred)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_sf(datum = NA) +
  theme_void()
```

The distribution for the final time step is very different from the others.
Let's just isolate the distributions from the first time period. Clearly these are non gaussian, so modeling them as such would be inappropriate. log and sqrt transforms seem similarly inappropriate.

Since we'll be using a logit link in the regressions, check the logit transform

This is why we need to model them separately.
```{r}
ggplot(dat %>%
  mutate(similarity = if_else(similarity == 1, 1 - .001, if_else(similarity == 0, .001, similarity))), aes(x = qlogis(similarity), y = as.factor(time), fill = ..x.., height = ..density..)) +
  geom_density_ridges_gradient(stat = 'density', scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')
ggplot(filter(dat, similarity > 0 & similarity < 1), aes(x = qlogis(similarity), y = as.factor(time), fill = ..x.., height = ..density..)) +
  geom_density_ridges_gradient(stat = 'density', scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')
```


By adding a rug plot, we can see that this is likely because the small number of edges sampled overall in the final period. Possibly this has additional meaning due to something like the salado phenomenon, by which ceramic assemblage similarities take on a different meaning?

We can confirm this by looking at the number of edges from each time period.
```{r}
dat %>%
  group_by(time) %>%
  count
```


```{r, eval = FALSE, echo = FALSE}
test <- recon_amplitudes %>%
  filter(between(period, 1200, 1400)) %>%
  group_by(eof_num, period) %>% 
  summarise(preds = mean(preds))
eof_data <- raster::extract(reof_raster, pts, df=TRUE) %>%
  gather(eof_num, value, 2:(n_modes + 1)) %>%
  full_join(test) %>%
  mutate(key = paste(eof, period, sep = '_'),
            value = value * amplitude) %>%
  select(ID, key, value) %>%
  spread(key, value, -1)
```

## Distance modeling

Our data have a lot of zeros in them. To do this, we'll actually fit two models: a logistic regression for the presence or absence of any material culture similarity between two sites, and a beta regression for the intensity of the interaction when it does exist. 

First we predict the presence of a tie via logistic regression. Should we include size as a default predictor in the model? It doesn't increase the deviance explaned at all, but AIC says yes

Now we write out a list of potential model structures, and fit gams using all of them. Calculate the AICs for each formula, link function combination, and reorder the amount from the lowest AIC
```{r}
ctrl <- gam.control(nthreads = 2)

fit_gam_presence <- function(x){
  gam(x, method = 'ML', 
         family = binomial(link = 'logit'),
         control = ctrl,
         data = dat)
}
```

```{r logistic_models, cache = TRUE}
logistic_models <- c(
  "s(distance, bs = 'tp', m = 2) +
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) + 
  time + macro_same",
  "s(distance, bs = 'tp', m = 2) + 
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) +
  s(pc1, bs = 'ts', m = 2) + s(pc1, by = time, bs = 'ts', m = 1) + 
  s(pc2, bs = 'ts', m = 2) + s(pc2, by = time, bs = 'ts', m = 1) +
  s(pc3, bs = 'ts', m = 2) + s(pc3, by = time, bs = 'ts', m = 1) + 
  s(pc4, bs = 'ts', m = 2) + s(pc4, by = time, bs = 'ts', m = 1) +
  time + macro_same",
  "s(distance, bs = 'tp', m = 2) + 
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) +
  s(reof1, bs = 'ts', m = 2) + s(reof1, by = time, bs = 'ts', m = 1) + 
  s(reof2, bs = 'ts', m = 2) + s(reof2, by = time, bs = 'ts', m = 1) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, by = time, bs = 'ts', m = 1) + 
  s(reof4, bs = 'ts', m = 2) + s(reof4, by = time, bs = 'ts', m = 1) + 
  time + macro_same"
   ) %>%
  paste('tie ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = future_map(formula, fit_gam_presence)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```

```{r}
logistic_models
```

We fit 3 models to predict the prsence of an interaction tie between two sites. The best performing model including distance, size, and reofs as predictors, with an aic of `r logistic_models[[1,4]]`. In second is the model with distance, size, and reofs as predictors, with an aic of `r logistic_models[[2,4]]`. Third is the base geographic model with distance and size, with an AIC of `r logistic_models[[3,4]]`. That said, even the best fitting model only captures 30% of the variance in tie presence. This means we are still missing key variables that drive the presence or absence of social relations.

```{r}
plot(logistic_models[[1,3]], scale= 0)
summary(logistic_models[[1,3]])
gam.check(logistic_models[[1,3]])
```

Next model the interaction intensities.


Define a function that, when applied to a dataset and a gam formula, will fit a gam of that formula.
```{r fit_gam}
fit_gam_similarity <- function(x, y){
  gamm(x, method = 'ML', 
      family = gaussian(),
      correlation = corMLPE(form = ~from + to),
      data = y)
}

model_list <- c(
  "s(distance, bs = 'cr')",
  "s(distance, bs = 'cr') + s(pc1, bs = 'cs') + s(pc2, bs = 'cs') + 
    s(pc3, bs = 'cs') + s(pc4, bs = 'cs')",
  "s(distance, bs = 'cr') + s(reof1, bs = 'cs') + s(reof2,  bs = 'cs') + 
    s(reof3, bs = 'cs') + s(reof4, bs = 'cs')") %>%
  paste('qlogis(similarity) ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula))
  


  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula)) 
         model = purrr::map(formula, fit_gam_similarity)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)

test <- dat %>%
  filter(similarity > 0) %>%
  mutate(similarity = if_else(similarity >= .999, 0.999, similarity)) %>%
  group_by(time) %>%
  sample_frac(.5) %>%
  nest

similarity_models <- map_df(seq_len(5), ~model_list) %>%
  mutate(time = levels(dat$time) %>% rep(3)) %>%
  left_join(test) %>%
  mutate(model = purrr::map2(formula, data, ~fit_gam_similarity(.x,.y)))
```
```{r}
t1 <- similarity_models %>%
  mutate(aic = map_dbl(model, ~AIC(.$lme)))%>%
  arrange(aic)

plot(similarity_models[[3, 5]]$gam, trans = plogis)
summary(t1[[6,5]]$gam)
similarity_models[[3,5]]

```


```{r}
t1[[14,5]]$gam %>% getViz %>% plot %>% print(pages=1) 
t1[[11,5]]$gam %>% getViz %>% plot %>% print(pages=1)
t1[[7,5]]$gam %>% getViz %>% plot %>% print(pages=1)
t1[[4,5]]$gam %>% getViz %>% plot %>% print(pages=1)
t1[[1,5]]$gam %>% getViz %>% plot %>% print(pages=1)

t1[[13,5]]$gam %>% getViz %>% plot %>% print(pages=1)
t1[[10,5]]$gam %>% getViz %>% plot %>% print(pages=1)
t1[[8,5]]$gam %>% getViz %>% plot %>% print(pages=1)
t1[[4,5]]$gam %>% getViz %>% plot %>% print(pages=1)
t1[[2,5]]$gam %>% getViz %>% plot %>% print(pages =1)
```

```{r}
test <- dat %>%
  filter(similarity > 0) %>%
  mutate(similarity = if_else(similarity > .999, .999, similarity)) %>%
  #filter(time == 1350) %>%
 # filter(time %in% c(1250, 1300, 1350, 1400)) %>%
  #group_by(time) %>%
  sample_frac(.6) %>%
  mutate(time_num = as.numeric(time)) 

test1 <- gamm(qlogis(similarity) ~ s(distance, bs = 'cr'),
              method = 'REML',
              correlation = corMLPE(form = ~from + to),
              data = test)

summary(test1$gam)
plot(test1$gam, trans = plogis, residuals = T)
swsn_reof %E>%
  filter(similarity > 0) %>%
  filter(from < to) %>%
  mutate(., pred = plogis(predict(test1$gam, as_tibble(.)))) %>%
  arrange(pred) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = pred, color = pred)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_sf(datum = NA) +
  theme_void()
swsn_reof %E>%
  filter(similarity > 0) %>%
  filter(from < to) %>%
  mutate(., pred = predict(test1$gam, as_tibble(.))) %>%
  mutate(res = similarity - pred) %>%
  arrange(res) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = res, color = res)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_sf(datum = NA) +
  theme_void()
swsn_reof %E>%
  filter(similarity > 0) %>%
  filter(from < to) %>%
  mutate(., pred = plogis(predict(test1$gam, as_tibble(.)))) %>%
  mutate(res = similarity - pred) %>%
  arrange(-res) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = res, color = res)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none') +
  coord_sf(datum = NA) +
  theme_void()
```

```{r}
test3 <- gamm(qlogis(similarity) ~ s(distance, bs = 'cr', k = 6)   +
         s(log(size), bs = 'cs', k = 6) +
         s(reof1, bs = 'cs', k = 6) +
         s(reof2, bs = 'cs', k = 6) +
         s(reof3, bs = 'cs', k = 6) +
         s(reof4, bs = 'cs', k = 6) +
         s(reof5, bs = 'cs', k = 6) +
         s(reof6, bs = 'cs', k = 6) +
         s(reof7, bs = 'cs', k = 6) +
         s(reof8, bs = 'cs', k = 6) +
         s(reof9, bs = 'cs', k = 6) + 
          time + macro_same + micro_same,
         method = 'REML',
         correlation = corMLPE(form = ~from + to),
         data = test)
summary(test3$gam)
plot(test3$gam,residuals = T)
swsn_reof %E>%
  filter(similarity > 0) %>%
  filter(from < to) %>%
  mutate(., pred = plogis(predict(test3$gam, as_tibble(.)))) %>%
  arrange(pred) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = pred, color = pred)) +
  geom_sf(data = states, fill = NA, color = 'black') +
  facet_edges(~time) +
  scale_edge_alpha(guide = 'none') +
  scale_edge_color_distiller(palette = 'Spectral', guide = 'none', limits = c(0,1)) +
  coord_sf(datum = NA) +
  theme_void()

```


```{r}
test3 <- gamm(qlogis(similarity) ~ s(distance, bs = 'cr', k = 6)   +
       # s(log(size), bs = 'cs', k = 6) +
         s(log(reof1), bs = 'cs', k = 6) +
         s(log(reof2), bs = 'cs', k = 6) +
         s(log(reof3), bs = 'cs', k = 6) +
         s(log(reof4), bs = 'cs', k = 6) +
         s(log(reof5), bs = 'cs', k = 6) +
         s(log(reof6), bs = 'cs', k = 6) +
         s(log(reof7), bs = 'cs', k = 6) +
         s(log(reof8), bs = 'cs', k = 6) +
         s(log(reof9), bs = 'cs', k = 6) + 
          time,
         method = 'REML',
         correlation = corMLPE(form = ~from + to),
         data = test)

test2 <- gamm(qlogis(similarity) ~ s(distance, time_num, k = c(6, 4), fx = c(F,T))   +
        te(log(size), time_num, k = c(6, 4), fx = c(F,T)) +
         te(log(reof1), time_num, k = c(6, 4), fx = c(F,T)) +
         te(log(reof2), time_num, k = c(6, 4), fx = c(F,T)) +
         te(log(reof3), time_num, k = c(6, 4), fx = c(F,T)) +
         te(log(reof4), time_num, k = c(6, 4), fx = c(F,T)) +
         te(log(reof5), time_num, k = c(6, 4), fx = c(F,T)) +
         te(log(reof6), time_num, k = c(6, 4), fx = c(F,T)) +
         te(log(reof7), time_num, k = c(6, 4), fx = c(F,T)) +
         te(log(reof8), time_num, k = c(6, 4), fx = c(F,T)) +
         te(log(reof9), time_num, k = c(6, 4), fx = c(F,T)),
         method = 'ML',
         correlation = corMLPE(form = ~from + to|time),
         data = test)

test6
test4 <- gamm(qlogis(similarity) ~ s(distance, bs = 'cr', k = 6)   +
         log(reof1)+
         log(reof2)+
         log(reof3) +
         log(reof4)+
         log(reof5) +
         log(reof6)+
         log(reof7)+
         log(reof8) +
         log(reof9),
         method = 'ML',
         correlation = corMLPE(form = ~from + to),
         data = test)

gam.check(test1$gam)
summary(test3$gam)
anova(test2$lme, test1$lme, test3$lme)
plot(test2$gam, pages = 1, scale = )
plot(test1$gam, pages = 1, scale = 0)
```
```{r}
test1 <- gamm(qlogis(similarity) ~  s(distance, bs = 'cr', m = 2, k = 4) + s(distance, by = time, bs = 'cs', m = 1, k = 4) + 
  s(reof1, bs = 'cr', m = 2, k = 4) + s(reof1, by = time, bs = 'cs', m = 1, k = 4) + 
  s(reof2, bs = 'cr', m = 2, k = 4) + s(reof2, by = time, bs = 'cs', m = 1, k = 4) +
  s(reof3, bs = 'cr', m = 2, k = 4) + s(reof3, by = time, bs = 'cs', m = 1, k = 4) + 
  s(reof4, bs = 'cr', m = 2, k = 4) + s(reof4, by = time, bs = 'cs', m = 1, k = 4) +
  s(reof5, bs = 'cr', m = 2, k = 4) + s(reof5, by = time, bs = 'cs', m = 1, k = 4) + 
  s(reof6, bs = 'cr', m = 2, k = 4) + s(reof6, by = time, bs = 'cs', m = 1, k = 4) +
  s(reof7, bs = 'cr', m = 2, k = 4) + s(reof7, by = time, bs = 'cs', m = 1, k = 4) + 
  s(reof8, bs = 'cr', m = 2, k = 4) + s(reof8, by = time, bs = 'cs', m = 1, k = 4) +
  s(reof9, bs = 'cr', m = 2, k = 4) + s(reof9, by = time, bs = 'cs', m = 1, k = 4) + 
  time, method = 'REML', 
      family = gaussian(),
      correlation = corMLPE(form = ~from + to),
      data = test)

t5 <-gls(qlogis(similarity) ~ time + log(distance) + reof1, correlation = corMLPE(form = ~from + to|time), data = test, method = 'ML')

test %>%
  modelr::add_predictions(t4) %>%
  mutate(pred = plogis(pred)) %>%
  ggplot(aes(distance, pred, color = time)) + geom_point(aes(y = similarity), alpha = .1) + geom_line() +geom_smooth()
test %>%
  modelr::add_predictions(t4) %>%
  mutate(pred = pred) %>%
  mutate(res = qlogis(similarity) - pred) %>%
  ggplot(aes(log(reof2), res, color = time)) + geom_point(alpha = .1) + geom_smooth()
AIC(t1,t2,t3, t4)
summary(t4)
plot(test$distance, predict(t4, newd))

test2 <- gamm(qlogis(similarity) ~ time + pair,
  method = 'REML', 
      family = gaussian(),
      correlation = corMLPE(form = ~from + to),
      data = test)

test3 <- gamm(qlogis(similarity) ~  te(distance, time_num) +
  te(pc1, time_num) +
  te(pc2, time_num) +
  te(pc3, time_num) +
  te(pc4, time_num),
  method = 'REML', 
      family = gaussian(),
      correlation = corMLPE(form = ~from + to|time),
      data = test)

gam.check(test2$gam)

plot(test2$gam, scheme = 1)
 test1$gam %>% getViz %>% plot(trans = plogis) %>% print(pages =1)
test1$gam %>% predict.gam(newdata = tibble(distance = 0, reof1=seq(0,.6,.01),reof2=0,reof3=0, reof4=0), type = 'response', se.fit = T) %>% bind_cols %>%
  mutate(lower = fit - 2 * se.fit, upper = fit + 2 * se.fit) %>%
 mutate_at(vars(fit,lower, upper), plogis) %>% ggplot(aes(seq(0,.6,.01), fit)) +  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .2) +
geom_line() +
  scale_y_continuous(limits = c(0,1))
gc()
```


```{r}
devtools::install_github("gavinsimpson/gratia")
library(gratia)
evaluate_smooth(t1[[4,5]]$gam, 's(distance)')  %>% draw()
evaluate_smooth(t1[[4,5]]$gam, 's(distance)') %>% 
  mutate(est_trans = plogis(est),
         se_trans = est_trans * (1-est_trans)* se) %>%
  mutate(lower = est - 2 * se, upper = est + 2 * se) %>%
   mutate(lower_trans = est_trans - 2 * se_trans, upper_trans = est_trans + 2 * se_trans) %>%
 mutate_at(vars(lower, upper), plogis) %>%
  ggplot(aes(x = distance)) +
  geom_ribbon(aes(y = est_trans, ymin = lower, ymax = upper), alpha = .2, color = 'red') +
   geom_ribbon(aes(y = est_trans, ymin = lower_trans, ymax = upper_trans), alpha = .2, color = 'blue') +
  geom_line(aes(y = est_trans))
#draw(t1[[4,5]]$gam) 

t1[[4,5]]$gam %>% getViz %>% sm(1) %>%  plot(trans = plogis) + l_fitLine(colour = "red") + l_ciLine(mul = 5, colour = "blue", linetype = 2)

predict.gam
plot(qlogis(1:10))


plot.mgcv.smooth.1D
```
```{r}


predict(t1[[2,5]]$gam %>% predict.gam(newdata = tibble(distance = 0:7000), exclude = 's(pc1)', type = 'response', newdata.guaranteed = FALSE)
, newdata = tibble(distance = seq(min(test$distance), max(test$distance), 1000), x0 = 0, x1 = 0, x3 = 0),type = 'response', se.fit = TRUE ) %>% bind_cols %>%  
  mutate(lower = fit - 2 * se.fit, upper = fit + 2 * se.fit) %>%
 mutate_at(vars(fit,lower, upper), plogis) %>% ggplot(aes(seq(min(test$distance), max(test$distance), 1000), fit)) +   geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .2) +
geom_line()


draw(bm2)
```
for our plots, its important to back transform the confidence intervals: https://stats.stackexchange.com/questions/163824/different-ways-to-produce-a-confidence-interval-for-odds-ratio-from-logistic-reg 
```{r}
beta_models <- c(
  "s(distance, bs = 'tp', m = 2) +
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) + 
  time + macro_same",
  "s(distance, bs = 'tp', m = 2) + 
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) +
  s(pc1, bs = 'ts', m = 2) + s(pc1, by = time, bs = 'ts', m = 1) + 
  s(pc2, bs = 'ts', m = 2) + s(pc2, by = time, bs = 'ts', m = 1) +
  s(pc3, bs = 'ts', m = 2) + s(pc3, by = time, bs = 'ts', m = 1) + 
  s(pc4, bs = 'ts', m = 2) + s(pc4, by = time, bs = 'ts', m = 1) +
  time + macro_same",
  "s(distance, bs = 'tp', m = 2) + 
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) +
  s(reof1, bs = 'ts', m = 2) + s(reof1, by = time, bs = 'ts', m = 1) + 
  s(reof2, bs = 'ts', m = 2) + s(reof2, by = time, bs = 'ts', m = 1) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, by = time, bs = 'ts', m = 1) + 
  s(reof4, bs = 'ts', m = 2) + s(reof4, by = time, bs = 'ts', m = 1) + 
  time + macro_same"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam_similarity)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```

```{r}
beta_models
plot(beta_models[[1,3]])
```

Now we write out a list of potential model structures, and fit gams using all of them. Calculate the AICs for each formula, link function combination, and reorder the amount from the lowest AIC
```{r geo_models}
geo_models <- c(
    "s(distance, bs = 'tp') + s(time, bs = 're', k = 5)",
    "s(distance, m = 2) + s(distance, time, bs = 'fs', m = 2)",
    "s(distance, time, bs = 'fs', m = 2)",
    "s(distance, by = time, bs = 'tp', m = 2) + s(time, bs = 're', k = 5)",
    "s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + s(time, bs = 're', k = 5)"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam_similarity)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```

Best fitting size model has log of distance and size, second best has log of sie but not distance
```{r}
size_models <-  c(
      "s(distance, by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5)",
    "s(distance, by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5) + s(size, by = time, bs = 'ts', m = 2)",
    "s(distance, by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5) + s(log(size), by = time, bs = 'ts', m = 2)",
      "s(log(distance), by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5)",
    "s(log(distance), by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5) + s(size, by = time, bs = 'ts', m = 2)",
    "s(log(distance), by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5) + s(log(size), by = time, bs = 'ts', m = 2)"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)

odels
summary(odels[[1,3]])
plot(odels[[1,3]])
```

Here we can see that the best model includes distance, centrality, and size in a purely additive structure with a logit link function. This model gives an adjusted R2 of ~70%.
```{r}
geo_models
```

Let's visualize the model fits.
```{r geo_models_output}
gam.check(geo_models[[1,3]])
plot(geo_models[[1,3]])
summary(geo_models[[1,3]])
geo_models[[1,3]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)
```


```{r}
#devtools::install_github('nspope/corMLPE')
library(corMLPE)
library(tidyverse)
library(mgcv)
test2 <-  dat %>%
  filter(similarity > 0) %>%
  mutate(similarity = if_else(similarity >= .999, 0.999, similarity)) %>%
  group_by(time)%>%
  sample_n(886) %>%
 # filter(time %in% c(1250,1300, 1350, 1400)) %>%
  gamm(qlogis(similarity) ~ te(distance, as.numeric(time), k = 4),
  # s(reof1, by = time, bs = 'ts', m = 2) + 
   #s(reof2, by = time, bs = 'ts', m = 2) +
   #s(reof3, by = time, bs = 'ts', m = 2) + 
  #s(reof4, by = time, bs = 'ts', m = 2), 
         correlation = corMLPE(form = ~from + to|time),
  method = 'REML',
         data = .)

test <-  dat %>%
  filter(similarity > 0) %>%
  mutate(similarity = if_else(similarity >= .999, 0.999, similarity)) %>%
  group_by(time)%>%
  sample_frac(.) %>%
  gamm(qlogis(similarity) ~  s(distance, bs = 'cr') + time +
  s(pc1, bs = 'cs') + 
  s(pc2, bs = 'cs') + 
   s(pc3, bs = 'cs') +
    s(pc4,  bs = 'cs') +
  s(reof1, bs = 'cs') + 
   s(reof2,  bs = 'cs') +
    s(reof3,  bs = 'cs') + 
   s(reof4, bs = 'cs'), 
         correlation = corMLPE(form = ~from + to|time),
  method = 'REML',
         data = .)
test

test <-  dat %>%
  filter(similarity > 0) %>%
  mutate(similarity = if_else(similarity >= .999, 0.999, similarity)) %>%
filter(time = 1300) %>%
  gamm(qlogis(similarity) ~  s(distance, bs = 'cr'), 
         correlation = corMLPE(form = ~from + to|time),
  method = 'REML',
         data = .)

plot(test$gam, trans = plogis)
plot(test2$gam, scheme = 1)
summary(qlogis(dat$similarity))
plot(test$gam)
summary(test$gam)

concurvity(test$gam)

test <- lme(qlogis(similarity) ~ distance, random = ~distance|time, correlation = corMLPE(form = ~from + to | time), data = dat %>%
  mutate(similarity = if_else(similarity == 1.0, 1 - .0001, if_else(similarity == 0, .0001, similarity)), from = as.character(from), to = 
    as.character(to)))
```


So let's select this model as our baseline geographic model.

```{r}
expand.grid(distance = seq(0, 100000, by = 100), 
            time = as.factor(c(1200, 1250, 1300, 1350, 1400)), 
            macro_same = c(T, F), micro_same = c(T,F),
            size = 500,
            pc1 = 0, pc2 = 0, pc3 = 0, pc4 = 0, reof1 = 0, reof2 = 0, reof3 = 0, reof4 = 0) %>%
  mutate(., pred = predict(test$gam, ., type = 'response')) %>%
  ggplot(aes(distance, plogis(pred), group = time, color = time)) +
  facet_grid(macro_same ~ micro_same) +
  geom_line()

expand.grid(distance = 100000, 
            time = as.factor(c(1200, 1250, 1300, 1350, 1400)), 
            macro_same = F, micro_same = F,
            size = 500,
            pc1 = seq(0,1,.01), pc2 = 0, pc3 = 0, pc4 = 0, reof1 = 0, reof2 = 0, reof3 = 0, reof4 = 0) %>%
  mutate(., pred = predict(mod, ., type = 'response')) %>%
  ggplot(aes(pc1, pred, group = time, color = time)) +
  facet_grid(macro_same ~ micro_same) +
  geom_line()

expand.grid(distance = 100000, 
            time = as.factor(c(1200, 1250, 1300, 1350, 1400)), 
            macro_same = F, micro_same = F,
            size = 1000,
            pc1 = 0, pc2 = 0, pc3 = 0, pc4 = 0, reof1 = 0, reof2 = 0, reof3 = 0, reof4 = seq(0,.5,.01)) %>%
  mutate(., pred = predict(all_models[[1,3]], ., type = 'response')) %>%
  ggplot(aes(reof4, pred, group = time, color = time)) +
  facet_grid(macro_same ~ micro_same) +
  geom_line()
```

```{r}
mod <- bam(similarity ~ 
  s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + 
  s(size, bs = 'ts', m = 2) + s(size, by = time, bs = 'ts', m = 1) + 
  s(pc1, bs = 'ts', m = 2) + s(pc1, by = time, bs = 'ts', m = 1) + 
  s(pc2, bs = 'ts', m = 2) + s(pc2, by = time, bs = 'ts', m = 1) +
  s(pc3, bs = 'ts', m = 2) + s(pc3, by = time, bs = 'ts', m = 1) + 
  s(pc4, bs = 'ts', m = 2) + s(pc4, by = time, bs = 'ts', m = 1) +
  s(reof1, bs = 'ts', m = 2) + s(reof1, by = time, bs = 'ts', m = 1) + 
  s(reof2, bs = 'ts', m = 2) + s(reof2, by = time, bs = 'ts', m = 1) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, by = time, bs = 'ts', m = 1) + 
  s(reof4, bs = 'ts', m = 2) + s(reof4, by = time, bs = 'ts', m = 1) +
  micro_same + macro_same + s(time, bs = 're', k = 5),
  family = betar(eps = 0.0001),
  nthreads = 3,
  discrete = TRUE,
  data = dat)

summary(mod)
plot(mod)
```
```{r}
concurvity(geo_models[[1,3]])
concurvity(mod)
```

```{r}
mod3 <- bam(similarity ~ 
   s(distance, time, bs = 'fs', m = 2) + 
   s(log(size), time, bs = 'fs', m = 2) + 
   s(reof1, time, bs = 'fs', m = 2) + 
   s(reof2, time, bs = 'fs', m = 2) +
   s(reof3, time, bs = 'fs', m = 2) + 
  s(reof4, time, bs = 'fs', m = 2),
  family = betar(eps = 0.0001),
  nthreads = 3,
  discrete = TRUE,
  data = dat)

summary(mod3)
plot(mod3)
```


```{r}
mod2 <- bam(similarity ~ 
   s(distance, by = time, bs = 'cr') + 
   s(log(size), by = time, bs = 'cr') + 
   s(reof1, by = time, bs = 'cr') + 
   s(reof2, by = time, bs = 'cr') +
   s(reof3, by = time, bs = 'cr') + 
  s(reof4, by = time, bs = 'cr') +
    s(time, bs = 're', k = 5),
  family = betar(eps = 0.0001),
  select = TRUE,
  discrete = TRUE,
  nthreads = 3,
  data = dat)
dat
summary(mod2)
plot(mod2, residuals = TRUE)
gam.check(mod2)
```
```{r}
all_models <- c(
    "s(distance, bs = 'ts') + s(time, bs = 're', k = 5)",
    "s(distance, bs = 'ts', m = 2) + s(distance, time, bs = 'fs', m = 2)",
    "s(distance, time, bs = 'fs', m = 2)",
    "s(distance, by = time, bs = 'tp', m = 2) + s(time, bs = 're', k = 5)",
    "s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + s(time, bs = 're', k = 5)",
    "s(distance, by = time, bs = 'ts', m = 2) + 
   s(pc1, by = time, bs = 'ts', m = 2) + 
   s(pc2, by = time, bs = 'ts', m = 2) +
   s(pc3, by = time, bs = 'ts', m = 2) + 
  s(pc4, by = time, bs = 'ts', m = 2) +
  s(time, bs = 're', k = 5)",
 "s(distance, time, bs = 'fs', m = 2) + 
   s(pc1, time, bs = 'fs', m = 2) + 
   s(pc2, time, bs = 'fs', m = 2) +
   s(pc3, time, bs = 'fs', m = 2) + 
  s(pc4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + 
  s(pc1, bs = 'ts', m = 2) + s(pc1, by = time, bs = 'ts', m = 1) + 
  s(pc2, bs = 'ts', m = 2) + s(pc2, by = time, bs = 'ts', m = 1) +
  s(pc3, bs = 'ts', m = 2) + s(pc3, by = time, bs = 'ts', m = 1) + 
  s(pc4, bs = 'ts', m = 2) + s(pc4, by = time, bs = 'ts', m = 1) +
 s(time, bs = 're', k = 5)",
  "s(distance, bs = 'ts', m = 2) + s(distance, time, bs = 'fs', m = 2) + 
  s(pc1, bs = 'ts', m = 2) +   s(pc1, time, bs = 'fs', m = 2) + 
  s(pc2, bs = 'ts', m = 2) +  s(pc2, time, bs = 'fs', m = 2) +
  s(pc3, bs = 'ts', m = 2) +     s(pc3, time, bs = 'fs', m = 2) +
  s(pc4, bs = 'ts', m = 2) +  s(pc4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'ts') + 
  s(pc1, bs = 'ts') + 
  s(pc2, bs = 'ts') + 
  s(pc3, bs = 'ts') + 
  s(pc4, bs = 'ts') + 
  s(time, bs = 're', k = 5)",
     "s(distance, by = time, bs = 'ts', m = 2) + 
   s(reof1, by = time, bs = 'ts', m = 2) + 
   s(reof2, by = time, bs = 'ts', m = 2) +
   s(reof3, by = time, bs = 'ts', m = 2) + 
  s(reof4, by = time, bs = 'ts', m = 2) + 
  s(time, bs = 're', k = 5)",
 "s(distance, time, bs = 'fs', m = 2) + 
   s(reof1, time, bs = 'fs', m = 2) + 
   s(reof2, time, bs = 'fs', m = 2) +
   s(reof3, time, bs = 'fs', m = 2) + 
  s(reof4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + 
  s(reof1, bs = 'ts', m = 2) + s(reof1, by = time, bs = 'ts', m = 1) + 
  s(reof2, bs = 'ts', m = 2) + s(reof2, by = time, bs = 'ts', m = 1) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, by = time, bs = 'ts', m = 1) + 
  s(reof4, bs = 'ts', m = 2) + s(reof4, by = time, bs = 'ts', m = 1) + 
 s(time, bs = 're', k = 5)",
  "s(distance, bs = 'ts', m = 2) + s(distance, time, bs = 'fs', m = 2) + 
  s(reof1, bs = 'ts', m = 2) +  s(reof1, time, bs = 'fs', m = 2) + 
  s(reof2, bs = 'ts', m = 2) +  s(reof2, time, bs = 'fs', m = 2) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, time, bs = 'fs', m = 2) + 
  s(reof4, bs = 'ts', m = 2) +  s(reof4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'ts') + 
  s(reof1, bs = 'ts') + 
  s(reof2, bs = 'ts') + 
  s(reof3, bs = 'ts') + 
  s(reof4, bs = 'ts') +
  s(time, bs = 're', k = 5)"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```
```{r}
saveRDS(all_models, 'all_models')
```

```{r}
all_models <- readRDS('all_models')
all_models
```
```{r}
summary(all_models[[1,3]]);summary(all_models[[6,3]])
plot(all_models[[2,3]], scale = -1)
```


```{r}
    "s(distance, by = time, bs = 'ts', m = 2) + 
   s(pc1, by = time, bs = 'ts', m = 2) + 
   s(pc2, by = time, bs = 'ts', m = 2) +
   s(pc3, by = time, bs = 'ts', m = 2) + 
  s(pc4, by = time, bs = 'ts', m = 2) +
   s(reof1, by = time, bs = 'ts', m = 2) + 
   s(reof2, by = time, bs = 'ts', m = 2) +
   s(reof3, by = time, bs = 'ts', m = 2) + 
  s(reof4, by = time, bs = 'ts', m = 2) + 
  s(time, bs = 're', k = 5)",
 "s(distance, time, bs = 'fs', m = 2) + 
   s(pc1, time, bs = 'fs', m = 2) + 
   s(pc2, time, bs = 'fs', m = 2) +
   s(pc3, time, bs = 'fs', m = 2) + 
  s(pc4, time, bs = 'fs', m = 2) +
   s(reof1, time, bs = 'fs', m = 2) + 
   s(reof2, time, bs = 'fs', m = 2) +
   s(reof3, time, bs = 'fs', m = 2) + 
  s(reof4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + 
  s(pc1, bs = 'ts', m = 2) + s(pc1, by = time, bs = 'ts', m = 1) + 
  s(pc2, bs = 'ts', m = 2) + s(pc2, by = time, bs = 'ts', m = 1) +
  s(pc3, bs = 'ts', m = 2) + s(pc3, by = time, bs = 'ts', m = 1) + 
  s(pc4, bs = 'ts', m = 2) + s(pc4, by = time, bs = 'ts', m = 1) +
  s(reof1, bs = 'ts', m = 2) + s(reof1, by = time, bs = 'ts', m = 1) + 
  s(reof2, bs = 'ts', m = 2) + s(reof2, by = time, bs = 'ts', m = 1) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, by = time, bs = 'ts', m = 1) + 
  s(reof4, bs = 'ts', m = 2) + s(reof4, by = time, bs = 'ts', m = 1) + 
 s(time, bs = 're', k = 5)",
  "s(distance, bs = 'ts', m = 2) + s(distance, time, bs = 'fs', m = 2) + 
  s(pc1, bs = 'ts', m = 2) +   s(pc1, time, bs = 'fs', m = 2) + 
  s(pc2, bs = 'ts', m = 2) +  s(pc2, time, bs = 'fs', m = 2) +
  s(pc3, bs = 'ts', m = 2) +     s(pc3, time, bs = 'fs', m = 2) +
  s(pc4, bs = 'ts', m = 2) +  s(pc4, time, bs = 'fs', m = 2) +
  s(reof1, bs = 'ts', m = 2) +  s(reof1, time, bs = 'fs', m = 2) + 
  s(reof2, bs = 'ts', m = 2) +  s(reof2, time, bs = 'fs', m = 2) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, time, bs = 'fs', m = 2) + 
  s(reof4, bs = 'ts', m = 2) +  s(reof4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'ts') + 
  s(pc1, bs = 'ts') + 
  s(pc2, bs = 'ts') + 
  s(pc3, bs = 'ts') + 
  s(pc4, bs = 'ts') + 
  s(reof1, bs = 'ts') + 
  s(reof2, bs = 'ts') + 
  s(reof3, bs = 'ts') + 
  s(reof4, bs = 'ts') +
  s(time, bs = 're', k = 5)"
```

```{r}
ggplot(dat, aes(abs(reof1_from - reof1_to), similarity)) +
  geom_point(alpha = .1, aes(color = time)) +
    scale_color_brewer(palette = 'Spectral', direction = 1) +
  geom_smooth(aes(group = time, color = time)) +
  geom_smooth() +
  theme_minimal()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(abs(reof4_from - reof4_to), resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_brewer(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(abs(size_from - size_to), resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_brewer(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(abs(pc4_from - pc4_to), resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_brewer(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```


## Cultural costs
Higher than average similarity within macroregions, but not significantly when you control for distance. Looks like there are a lot of outliers withhigh simliarity between macroregion that we're not yet capturing. the difference shrinks when we control for difference, because the "same macrogregion" will covary with distance

```{r}
ggplot(dat, aes(macro_same, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(macro_same, distance)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Distance') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(macro_same, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
ggplot(dat, aes(macro_from, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(macro_to, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()
```

```{r}
dat %>%
  na.omit %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
ggplot(aes(macro_from, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
ggplot(dat, aes(micro_same, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share microregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(micro_same, distance)) +
  geom_boxplot() +
  labs(title = '', x = 'Share microregion?', y = 'Distance') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(micro_same, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
ggplot(dat, aes(micro_from, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(micro_to, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()


dat %>%
  na.omit %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
ggplot(aes(micro_to, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
geo_models
```

## Average climmate models
Now we can look weather average climate conditions have any predictive power for link strength.
```{r clim_mods}
clim_models <- c(
      "s(distance, bs = 'cr') + s(size_from, size_to) +
          s(reof1_from, reof1_to) + s(reof2_from, reof2_to) + 
          s(reof3_from, reof3_to) + s(reof4_from, reof4_to) + s(to, bs = 're')",
      "s(distance, bs = 'cr') + s(size_from, size_to) + 
          s(pc1_from, pc1_to) + s(pc2_from, pc2_to) + 
          s(pc3_from, pc3_to) + s(pc4_from, pc4_to) + s(to, bs = 're')"
      ) %>%
  paste('rel_sim ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```
```{r}
clim_models
```

```{r}
dat %>%
  na.omit %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
ggplot(aes(macro_from == macro_to, resid)) +
  geom_point(alpha = .1) +
  geom_smooth() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```
So think about the models to fit. micro same, macro same, and micro and macro same. repeat for micro/macro from/to pairs. Nest all that in same not same. then compare with no distance, distance, and distance + centrality

```{r}
clim_models
```

Let's visualize the model fits.
```{r clim_models_output}
clim_models[[1,3]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)

clim_models[[1,3]] %>% plot(scheme = 1)

gam.check(clim_models[[2,3]])
```
```{r}
clim_models2 <- c(
        "s(distance, bs = 'cr')",
      #"s(distance, bs = 'cr') + s(size_from, bs = 'cr', id = 1) + s(size_to, bs = 'cr', id = 1)",
       #     "s(distance, bs = 'cr') + s((size_from * size_to), bs = 'cr')",
      #"s(distance, bs = 'cr') + s((size_from + size_to), bs = 'cr')",
      #"s(distance, bs = 'cr') + s(abs(size_from - size_to), bs = 'cr')",
      "s(distance, bs = 'cr') + s(abs(eof1_from - eof1_to), bs = 'cr') + s(abs(eof2_from - eof2_to), bs = 'cr') + s(abs(eof3_from - eof3_to), bs = 'cr') + s(abs(eof4_from - eof4_to), bs = 'cr')",
      "s(distance, bs = 'cr') + s(abs(reof1_from - reof1_to), bs = 'cr') + s(abs(reof2_from - reof2_to), bs = 'cr') + s(abs(reof3_from - reof3_to), bs = 'cr') + s(abs(reof4_from - reof4_to), bs = 'cr')",
      "s(distance, bs = 'cr') + s(abs(pc1_from - pc1_to), bs = 'cr') + s(abs(pc2_from - pc2_to), bs = 'cr') + s(abs(pc3_from - pc3_to), bs = 'cr') + s(abs(pc4_from - pc4_to), bs = 'cr')"
           #"s(distance, bs = 'cr') + s(abs(reof1_from - reof1_to), bs = 'cr') + s(abs(reof2_from - reof2_to), bs = 'cr') + s(abs(reof3_from - reof3_to), bs = 'cr') + s(abs(reof4_from - reof4_to), bs = 'cr') + s(abs(pc1_from - pc1_to), bs = 'cr') + s(abs(pc2_from - pc2_to), bs = 'cr') + s(abs(pc3_from - pc3_to), bs = 'cr') + s(abs(pc4_from - pc4_to), bs = 'cr')"
      ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```

```{r}
clim_models2
```

Let's visualize the model fits.
```{r clim_models_output2}
clim_models2[[2,3]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)

clim_models2[[1,3]] %>% plot(scheme = 1, residuals = T)
clim_models2[[2,3]] %>% plot(scheme = 1, residuals = T)

summary(clim_models2[[1,3]])
gam.check(clim_models2[[1,3]])
```


Interesting. Looks like there is increased similarity between locations with similar precipitation patterns or different temperature patterns. I should:
1. compare temperature and elevation regressions alone to see which is better
2. use unrotated EOFs, maybe winter ones too?
3. Use PCA of bioclim variables

## Variability models
```{r include = FALSE, eval = F}
  #  "ti(distance) + ti(size) + ti(distance, size)",
  #  "te(distance, size, k = 10)"
 #"s(distance, bs = 'cr') + s(abs(eof1), bs = 'cr', k = 15) + s(abs(eof2), bs = 'cr') + s(abs(eof3), bs = 'cr', k = 15) + s(abs(eof4), bs = 'cr', k = 15)",
#    "s(distance, bs = 'cr') + s(sqrt(abs(eof1)), bs = 'cr', k = 15) + s(abs(eof2), bs = 'cr') + s(abs(eof3), bs = 'cr', k = 15) + s(abs(eof4), bs = 'cr', #k = 15)"
```

```{r eval = FALSE}
mod5 <- gam(fractional_similarity ~ s(distance, bs = 'cr') +
              s(eof1, bs = 'cr') +
              s(eof2, bs = 'cr') +
              s(eof3, bs = 'cr') +
              s(eof4, bs = 'cr'),
            select = TRUE,
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)

mod6 <- gam(similarity ~ s(distance, bs = 'cr') +
              s(abs(eof1), bs = 'cr') +
              s(abs(eof2), bs = 'cr') +
              s(abs(eof3), bs = 'cr') +
              s(abs(eof4), bs = 'cr'),
            select = TRUE,
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)

summary(mod1)
plot(mod1)
gam.check(mod1)
```


```{r eval = FALSE}
swsn %E>%
  filter(centrality_from >= centrality_to) %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
  filter(resid > .25 | resid < -.25) %>%
  mutate(fac = if_else(resid > 0, 'pos', 'neg')) %>%
  ggraph('manual', node.positions = pts) +
  geom_edge_link(aes(alpha = abs(resid), color = resid)) +
  facet_graph(time ~ fac, col_type = 'edge') +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  scale_edge_colour_distiller(palette = 'RdBu', limits = c(-1,1)) +
  coord_equal() +
  theme_graph()
```
```{r eval = FALSE}
swsn %N>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[6,2]], type = 'response')) %>%
  filter(resid > .25 | resid < -.25) %>%
  mutate(fac = if_else(resid > 0, 'pos', 'neg')) %>%
  ggraph('manual', node.positions = pts) +
  geom_edge_link(aes(alpha = abs(resid), color = resid)) +
  facet_graph(time ~ fac, col_type = 'edge') +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  scale_edge_colour_distiller(palette = 'RdBu', limits = c(-1,1)) +
  coord_equal() +
  theme_graph()
```
Looks like distance is correlated with residuals
```{r eval = FALSE}
dat %>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  as_tibble %>%
  qplot(distance, resid, data = ., geom = 'point', alpha = I(.1), color = as.factor(time))
```
```{r eval = FALSE}
dat %>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  as_tibble %>%
  qplot(abs(eof1), resid, data = ., geom = 'point', alpha = I(.1), color = as.factor(time))
```
```{r eval = FALSE}
swsn %E>%
    filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  filter(time == 1) %N>%
  mutate(centrality = centrality_eigen(weights = similarity)) %E>%
  mutate(cent = .N()$centrality[from] * .N()$centrality[to]) %>%
  as_tibble %>%
  qplot(cent, resid, data = ., geom = 'point', alpha = I(.1))
```

```{r eval = FALSE}
swsn %E>%
    filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  filter(time == 3) %N>%
  mutate(centrality = centrality_eigen(weights = similarity)) %E>%
  mutate(cent = .N()$centrality[from] + .N()$centrality[to]) %>%
  as_tibble %>%
  qplot(cent, similarity, data = ., geom = 'point', alpha = I(.1))
```

```{r echo = FALSE, eval = FALSE}
phyda <- ncdf4::nc_open('data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc')
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_reof$amplitude[,1])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_reof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_reof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_reof$amplitude[,4])

plot(ncdf4::ncvar_get(phyda, 'Pac130_mn')[1100:1999], recon_reof$amplitude[,2])
plot(ncdf4::ncvar_get(phyda, 'Pac130_mn')[1100:1999], recon_reof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_reof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_reof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_reof$amplitude[,4])
cor(ncdf4::ncvar_get(phyda, 'amo_mn')[1100:1999], recon_reof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'amo_mn')[1100:1999], recon_reof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_reof$amplitude[,1])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_reof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_reof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'EPac_mn')[1100:1999], recon_reof$amplitude[,4])
```

# References {#references .unnumbered}
