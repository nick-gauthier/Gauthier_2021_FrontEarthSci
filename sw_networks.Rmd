---
title: "Supplemental Information"
subtitle: 'Analysis scripts for "Drought Variability and the Robustness of Agrarian Social Networks"'
author: "Nicolas Gauthier"
date: "Last knit on: `r format(Sys.time(), '%d %B, %Y')`"
#bibliography: bibliography.bibtex
output:
  tufte::tufte_handout:
    citation_package: natbib
    #latex_engine: xelatex
    toc: true
    highlight: pygments
  tufte::tufte_html:
    tufte_features: ["fonts", "italics"]
    toc: true
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, message = FALSE, warning = FALSE, cache=TRUE)
```

# Introduction
\begin{fullwidth}
This RMarkdown document contains all analysis code used to produce the results and figures in the main text.

In this two-part analysis we first compare temporal patterns of temperature and precipitation, derived from a transient paleoclimate simulation, at three points in the west Mediterranean. Then we analyze the spatial pattern of climate change from the LGM to the Mid Holocene using an ensemble of downscaled equilibrium time-slice paleoclimate simulations.
\end{fullwidth}

## Data sources
This R Markdown script requires raster data of [observed present-day temperature and precipitation](http://www.prism.oregonstate.edu/normals/)^[http://www.prism.oregonstate.edu/normals/] 30-year averages and 100-year time series of the [Standardized Precipitation-Evapotranspiration Index](https://wrcc.dri.edu/wwdt/time/)^[https://wrcc.dri.edu/wwdt/time/]. We'll also need reconstructed SPEI rasters^[https://zenodo.org/record/1198817]. Finally, get the CGIAR version of the SRMTM [digital elevation model](http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/)^[http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/], resampled to 250m. With permission, also acquire the [Southwest Social Networks Database](http://www.southwestsocialnetworks.net/data.html)^[http://www.southwestsocialnetworks.net/data.html].

## Packages
Load the packages used in the analyses.
```{r packages}
library(raster) # raster data manipulation
library(tidyverse) # data analysis and plotting
library(rgdal) # reprojection of spatial points
library(gdistance) # least cost distance calculations
library(wql) # empirical orthogonal function calculation
library(tidygraph) # network analysis
library(ggraph) # network plotting
library(mgcv) # GAM fitting
library(mgcViz) # GAM plotting
library(dismo)
```

```{r include = FALSE}
theme_set(theme_bw())
```

# Archaeological network data
Now we move on to the archaeological social network proxies^[Go to http://www.southwestsocialnetworks.net for more information on this dataset and the project]. First we read in a csv file containing information for all of the archaeological sites in the database, including spatial location and number of rooms present from each time period. These will become the node-level data for the network.
```{r}
node_data <- read_csv('Data/attributes_orig.csv') %>%
  select(-c(SWSN_ID,GKPM:Periods, N0S1))
```

```{r, echo = FALSE}
knitr::kable(
  node_data[1:6,1:8], caption = 'A subset of the node level data.'
)
```

Next import the edge data. Our data come in the form of similarity matrices, one for each time step. We need to define a function for importing and processing a single time step of the edge datasets. This function imports the csv, turns it into a matrix, and then a ddirected graph. We then filter out seeelf loops, calculate eigenvector centrality for each of the nodes, then for each edge record the time period and centralities of the neighboring nodes.
```{r echo = FALSE, fig.margin = TRUE, fig.cap = 'Example 6x6 simililarity matrix. Note the symmetry'}
sim_tmp <- read.csv('Data/Sim/AD1200sim.csv', row.names = 1, check.names = FALSE) %>%
  as.matrix %>%
  .[1:5,1:5]
knitr::kable(sim_tmp, caption = 'Example similarity matrix')
```

```{r read_swsn}
read_swsn <- function(net, time){
  read.csv(net, row.names = 1, check.names = FALSE) %>%
    as.matrix %>%
    as_tbl_graph(directed = TRUE) %E>%
    rename(similarity = weight) %>%
    filter(!edge_is_loop()) %N>%
    mutate(centrality = centrality_eigen(weights = similarity)) %E>%
    mutate(time = 1150 + 50 * time,
           centrality_from = .N()$centrality[from],
           centrality_to = .N()$centrality[to]) %N>%
    select(-centrality)
}
```

Next we map this function over a lists of simlaritity matrices..Get a list of the similarity files, map the above function over them, reduce to a single graph.  Use reduce and gjin to combiine all of these networks inito a single neetwtork, rataiining the edge weightss sfr eac  separate period.
```{r swsn_import}
swsn <- list.files('Data/Sim', full.names = TRUE) %>%
  imap(read_swsn) %>%
  reduce(graph_join, by = 'name') %>%
  left_join(node_data, by = c('name' = 'SWSN_Site'))
```

Create a separate spatial points object, storing the locations of the sites. Convert the coordinates from utm to lat lon, and add back to the original data
```{r site_points}
pts <- swsn %N>% 
  select(x = EASTING, y = NORTHING) %>%
  as_tibble %>%
  SpatialPoints(proj4string=CRS("+proj=utm +zone=12 +datum=NAD27")) %>%
  spTransform(CRS("+proj=longlat +datum=WGS84")) %>% 
  coordinates %>%
  data.frame
```


```{r network_plot, eval = FALSE, echo = FALSE}
states <- maps::map('state', regions = c('arizona', 'new mexico'), fill = TRUE, plot = FALSE)

ggraph(swsn, 'manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = similarity, color = similarity)) +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  geom_node_point() +#aes(size = centrality, color = centrality)) +
  facet_edges(~time) +
  scale_edge_alpha() +
  coord_equal() +
  theme_graph()
```

# Least Cost Distances
```{r eval = FALSE}
altDiff <- function(x){x[2] - x[1]}
hd <- transition(elev, altDiff, 8, symm = FALSE)
cell.targets <- Which(!is.na(elev), cells = TRUE)
adj <- adjacent(elev, cells = cell.targets, target = cell.targets, pairs = TRUE, directions = 8)
slope <- geoCorrection(hd, type = 'c')
speed <- slope
speed[adj] <- 6 * exp(-3.5 * abs(slope[adj] + 0.05))
conductance <- geoCorrection(speed, type = 'c')
```

```{r eval = FALSE, echo = FALSE}
rm(hd, slope, adj, speed)
gc()
```

```{r eval=FALSE,echo = FALSE}
plot(raster(conductance))
```

Calculate the least cost distance matrix between all sites and turn into a graph.
```{r eval = FALSE}
distances <- swsn %N>%
  select(lon, lat) %>%
  as_tibble %>%
  as.matrix %>%
  costDistance(conductance, .) %>%
  as_tbl_graph(directed = TRUE) %E>%
  rename(distance = weight) %>%
  as_tibble %>%
  mutate(distance = replace_na(distance, 0))
saveRDS(distances, 'distances.rds')
```

```{r echo = FALSE, eval = FALSE}
gc()
```

```{r}
distances <- readRDS('distances.rds')
```

# Climate Analysis
First we estimate present and past climate patterns.

## Study Area
First we define a boundary box covering much of the western United States, ranging between W$124\deg$ and W$105\deg$ and N$31\deg$ and N$41\deg$, which we'll use to constrain all subsequent climate analyses. This study area is significantly larger than the one we'll define in the next section for the network analysis. This allows us to sample a much wider range of climatic variability, while still remaining within the broader western US climate zone. This ensures that both A) our statistical analyses will be more robust to sampling error and B) the results will be less sensitive to the exact location and dimensions of our study area.
```{r bbox_map, fig.margin=TRUE, echo = FALSE, fig.cap = 'Locations of 2 bounding boxes.'}
bbox_wus <- extent(c(-124, -105, 31, 41))
bbox <- extent(c(-113.5, -106.5, 31, 37.5))
bb1 <- as(bbox_wus, 'SpatialPolygons') %>%
  fortify
bb2 <- as(bbox, 'SpatialPolygons') %>%
  fortify

map_data("usa") %>%
  ggplot(aes(x = long, y = lat, group = group)) + 
  geom_polygon(color = 'darkgrey', fill = NA) + 
  geom_polygon(data = bb1, color = 'black', fill = NA) +
  geom_polygon(data = bb2, color = 'black', fill = NA) +
  coord_fixed(1.3) +
  theme_void()
```

```{r bbox}
bbox_wus <- extent(c(-124, -105, 31, 41))
bbox <- extent(c(-113.5, -106.5, 31, 37.5))
```

## Climate Dataa
Import high resolution SPEI maps generated from PRISM data. Calculate the average summertime (JJA) SPEI value for each year.
```{r import_observations}
spei_obs <- ((brick('Data/spei12_6_PRISM.nc') + 
              brick('Data/spei12_7_PRISM.nc') + 
              brick('Data/spei12_8_PRISM.nc')) / 3) %>%
  crop(bbox_wus) %>% # crop to the western US bounding box
  `names<-`(1895:2017) %>% # add year names
  .[[2:105]]  # SPEI calculated on 12 month lag, so drop 1st year
```

Import the reconstructed SPEI fields from PHYDA. PHYDA uses a novel off-line data assimilation approach, using simulated SPEI from the CESM LME experiments as physically-consistent model priors and a network of tree rings, ice cores, and corals as assimilated observations.
```{r import_reconstructions}
spei_recon <- brick('Data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc',
                    varname = 'spei_mn') %>%
   .[[1100:1999]] %>% # extract years of interest
   rotate %>% # rotate longitudes to -180 to 180
   crop(bbox_wus, snap = 'out') # crop to western US
```


```{r plot_variance_trend, echo = FALSE, fig.margin=TRUE, fig.cap = 'Plotting out the time series of the reconstruction reveals no clear trend in the mean. It does, however, show a pattern of increasing variance over time, which is entirely a function of the changing number of proxies used in the data assimilation approach.'}
spei_recon %>% 
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, value, 3:902) %>%
  mutate(year = str_sub(year, 2),
         year = as.numeric(year)) %>%
  ggplot(aes(year, value)) +
  geom_line(alpha = .1, aes(group = interaction(x, y))) +
  geom_smooth() +
  ggtitle('Standardized Precipitation-Evapotranspiration Index in the American Southwest',
          'June-August, 12 month lag') +
  theme_bw()
```

## Empiricial Orthogonal Functions
We calculate the leading Empirical Orthogonal Functions of a ~100 year series of the summertime average Standardized Precipitation-Evapotranspiration Index. These patterns then undergo a varimax rotation to highlight more physically meaningful spatial patterns. We compare these patterns to those derived from a long term (~2ka) reconstruction based on data assimilation and CESM LME. We confirm that the spatial patterns detected for the past 100 years have been robust over time and explain up to 85% of the variance in a full 1,000 year sequence of reocnstructed drought dynamics.

Now we calculate the rotated empirical orthogonal functions for both the observed and reconstructed drought maps. For the observations, we see that the leading 5 eofs explain 85% of the variance in the series, so we'll retain those for rotation.
```{r plot_variance_prism, echo = FALSE}
#this code is the same as the eofNum function above, but needs a slight adjustment here because that function assumes more time than spatial indices
eigs <- prcomp(t(as.data.frame(spei_obs, na.rm = TRUE)), scale. = FALSE)[["sdev"]]^2
eigs.pct <- 100 * eigs/sum(eigs)
eigs.lo <- eigs * (1 - sqrt(2/104))
eigs.hi <- eigs * (1 + sqrt(2/104))
cumvar <- round(cumsum(eigs.pct), 1)
p <- 104
d <- data.frame(rank = factor(1:p), eigs, eigs.lo, eigs.hi, 
        cumvar)
d <- within(d, cumvar.line <- eigs.hi + 0.02 * max(eigs.hi))
d <- d[1:min(p, 10), ]
ggplot(data = d, aes(x = rank, y = eigs)) + geom_errorbar(aes(x = rank, 
        ymin = eigs.lo, ymax = eigs.hi), width = 0.3) + geom_point(size = 3) + 
        geom_text(aes(x = rank, y = cumvar.line, label = cumvar), 
            size = 3, vjust = 0) + labs(list(x = "Rank", y = "Eigenvalue")) + 
        theme(panel.grid.minor = element_blank())
```

Look at the variances for the eofs of each field, to inform truncation. Let's retain the leading four modes from PHYDA
```{r plot_variance_phyda, fig.margin=TRUE, echo = FALSE,fig.cap = 'Variance explained'}
spei_recon %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  eofNum(scale. = FALSE)
```
Repeat for the prism observations. Again, we'll retain the leading four modes.

The standard errors on the above plots assume there is no autocorrelation. Check to see if that is indeed the case.
```{r plot_acf, fig.margin=TRUE, echo = FALSE,fig.cap = 'Temporal autocorrelation'}
spei_recon %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  as_tibble %>%
  gather(key, value) %>%
  pull(value) %>%
  acf

spei_obs %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  as_tibble %>%
  gather(key, value) %>%
  pull(value) %>%
  acf
```
It looks like there is some autocorrelaiton, will have to adjust in the future.


Now, calculate the eofs for both fields, retaining the 4 leading components in each case for rotation
```{r calc_eofs}
# Decide how many modes to retain
n_modes <- 5 

obs_eof <- spei_obs %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  eof(n_modes, scale. = FALSE)

recon_eof <- spei_recon %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  eof(n_modes, scale. = FALSE)
```

Let's map out the spatial and temporal patterns in the REOFs. First we'll look at the spatial patterns. It looks like the observed and reconstructed datasets reveal very similar spatial patterns for the 5 leading modes.
```{r plot_eof_recons, fig.margin = TRUE, echo = FALSE, fig.cap = 'Reconstructed REOFs'}
as.data.frame(spei_recon[[1]], xy = TRUE, na.rm = TRUE) %>% cbind(recon_eof$REOF) %>%
  select(-3) %>%
  gather(eof, value, 3:(n_modes+2)) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
    scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() + 
  coord_quickmap()
```

```{r plot_eof_obs, echo = FALSE, fig.width = 6, fig.height = 4, fig.cap = 'Observed drought REOFs'}
as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>% 
  cbind(obs_eof$REOF) %>%
  select(-3) %>%
  gather(eof, value, 3:(n_modes + 2)) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() +
  coord_quickmap(xlim = c(-124, -106), ylim = c(31, 40)) +
  geom_polygon(data =  map_data("state"), aes(x = long, y = lat, group = group), color = 'darkgrey', fill = NA) +
  ggtitle('Leading 5 rotated empirical orthogonal functions',
          'SPEI') +
  theme(legend.position = "bottom")
```

A key assumption here is that the REOFs calculated from both the observations and reconstructions correspond to the same physical phenomena. That way we can just use the observed patterns as the high resolution patterns, and infer their temporal evolution from the reconstructions. This means we don't have to downscale the reconstructed reofs to use them, as otherwise we'd have to deal with issues of spatial represnetaitveness an non-overlap. Now we can be ensured that these are the same signals.

To confirm this, let's plot the time series (PCs) of each observed and reconstructed REOF against each other, to confirm that they correspond to the same patterns. Look at the correlations between the modes.
```{r amplitudes}
amplitudes <- left_join(
  recon_eof$amplitude %>%
    .[797:900,] %>%
    as_tibble %>%
    mutate(year = 1896:1999) %>%
    gather(eof, recon, 1:n_modes),
  obs_eof$amplitude %>%
    as_tibble %>%
    mutate(year = 1896:1999) %>%
    gather(eof, obs, 1:n_modes))
```

```{r plot_amplitudes, fig.fullwidth=TRUE, fig.height=4, fig.width=10, echo = FALSE, fig.cap = 'Strong linear correlation between the observed and reconstructed EOFs'}
ggplot(amplitudes, aes(recon, obs)) +
  geom_point(aes(color = year)) +
  geom_smooth(method = 'lm') +
  scale_color_viridis() +
  facet_wrap(~eof, nrow = 1) +
  ggtitle('Linear fits between observed and reconstructed amplitudes', '1896-1999') +
  theme_minimal() +
  coord_fixed() +
  theme(legend.position = "bottom")
```

Now let's fit these models in practice. First create a data frame of past amplitudes.
```{r past_amplitudes}
past_amplitudes <- recon_eof$amplitude %>%
    as_tibble %>%
    mutate(year = 1100:1999) %>%
    gather(eof, recon, 1:n_modes) %>%
  group_by(eof) %>%
  nest(.key = 'recons')
```


We can fit gams between the observed and reconstrcuted PC time series, and then use these to predict the eof amplitudes back in time. Adding in this regression step essentalially bias-corrects the reconstructed amplitudes
```{r recon_amplitudes}
recon_amplitudes <- amplitudes %>%
  group_by(eof) %>%
  nest %>%
  mutate(mod = purrr::map(data, ~gam(obs ~ s(recon, bs = 'cr'), 
                                     data = ., method = 'REML', 
                                     select = TRUE))) %>%
  left_join(past_amplitudes, by = 'eof') %>%
  mutate(predictions = purrr::map2(mod, recons, 
                                   ~predict(.x, .y, type = 'response')),
         predictions = purrr::map(predictions, ~.[1:400]),
         predictions = purrr::map(predictions, 
                                  ~tibble(year = 1100:1499, amplitude = .)),
         predictions = purrr::map(predictions, 
                                  ~mutate(., 
                                          amp_smooth = zoo::rollmean(amplitude, k = 51, fill = NA)))) %>%
  select(eof, predictions) %>%
  unnest %>%
  mutate(period = floor((year/50)) * 50) 
```

How have the amplitudes of these patterns changed over time? Let's plot the reconstructed time series for each, along with an estimated trend line.
```{r plot_recon_amplitudes, fig.width = 10, fig.height = 4, echo = FALSE, fig.fullwidth=TRUE}
ggplot(recon_amplitudes, aes(year, amplitude, group = eof)) +
  geom_line(aes(color = eof), alpha = .7) +
  geom_line(aes(y = amp_smooth)) +
  scale_color_brewer(palette = 'Spectral') +
  facet_wrap(~eof) +
  theme_minimal()+
  theme(legend.position = "bottom")
```

These results point to EOFs 3 and 4 as having major shocks at aropund 1300 AD.
```{r, echo = FALSE, fig.margin=TRUE, fig.cap='Are there any time periods with marked signals?'}
recon_amplitudes %>%
  filter(between(period, 1200, 1400)) %>%
  ggplot(aes(factor(period), amplitude)) +
  geom_boxplot() +
  geom_hline(yintercept = 0, color = 'red', linetype = 2) +
  scale_color_brewer(palette = 'Spectral') +
  facet_wrap(~eof) +
  theme_minimal()
```

## Environmental Data
Import higher resolution environmental layers and crop the reof files
```{r import_prism}
get_normals <- function(var){
  paste0('Data/PRISM/PRISM_', var, '_30yr_normal_800mM2_all_asc/') %>%
  list.files(pattern = '\\.asc$', full.names = TRUE) %>%
  .[1:12] %>% # the last file in the list is annual values, drop it
  purrr::map(raster) %>%
  purrr::map(crop, bbox_wus) %>%
  brick
}

prec <- get_normals('ppt')
tmin <- get_normals('tmin')
tmax <- get_normals('tmax')
```

Use the biovars function from dismo to calculate 19 bioclimatic predictor variables from monthly precipitation and temperature data.
```{r bioclim_pca}
bio_clim <- biovars(prec, tmin, tmax)
```

Reduce the dimensionality of the bioclimatic data using an R-mode PCA.
```{r}
clim_pca <- RStoolbox::rasterPCA(bio_clim, spca = TRUE)
```

Retain the leading 4 PCs.
```{r}
screeplot(clim_pca$model, npcs = 20, type = 'lines')
bio_clim %>%
  as.data.frame(na.rm = TRUE) %>%
  eofNum(scale. = TRUE)
```

```{r reof_raster}
reof_raster <- as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>%
  select(x:y) %>%
  cbind(obs_eof$REOF) %>%
  rasterFromXYZ %>%
  `crs<-`('+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0') %>%
  `names<-`(c('reof1', 'reof2', 'reof3', 'reof4', 'reof5')) %>%
  projectRaster(bio_clim)
```

```{r raster_reprojection}
env_rasters <- clim_pca$map %>%
  .[[1:4]] %>%
  `names<-`(c('pc1', 'pc2', 'pc3', 'pc4')) %>%
  c(reof_raster) %>%
  brick
```

```{r}
plot(env_rasters)
pairs(env_rasters)
```

# Model fitting
Now that we have all the data imported and preprocessed, let's put it all together into a data frame.
```{r}
dat <- swsn %N>%
  mutate(ID = 1:n(),
         lon = pts$x, 
         lat = pts$y) %>%
  left_join(raster::extract(env_rasters, pts, df = TRUE), 
            by = 'ID') %E>%
  left_join(distances, by = c('from', 'to')) %>%
  mutate(size_from = case_when(time == 1200 ~ .N()$P1room[from],
                               time == 1250 ~ .N()$P2room[from],
                               time == 1300 ~ .N()$P3room[from],
                               time == 1350 ~ .N()$P4room[from],
                               time == 1400 ~ .N()$P5room[from]),
         size_to = case_when(time == 1200 ~ .N()$P1room[to],
                             time == 1250 ~ .N()$P2room[to],
                             time == 1300 ~ .N()$P3room[to],
                             time == 1350 ~ .N()$P4room[to],
                             time == 1400 ~ .N()$P5room[to]),
         from_x = .N()$lon[from],
         from_y = .N()$lat[from],
         to_x = .N()$lon[to],
         to_y = .N()$lat[to],
         macro_from = .N()$Macro[from],
         macro_to =.N()$Macro[to],
         macro_same = macro_from == macro_to,
         micro_from = .N()$Micro[from],
         micro_to = .N()$Micro[to],
         micro_same = micro_from == micro_to,
         pc1_from = .N()$pc1[from],
         pc1_to = .N()$pc1[to],
         pc2_from = .N()$pc2[from],
         pc2_to = .N()$pc2[to],
         pc3_from = .N()$pc3[from],
         pc3_to = .N()$pc3[to],
         pc4_from = .N()$pc4[from],
         pc4_to = .N()$pc4[to],
         reof1_from = .N()$reof1[from],
         reof1_to = .N()$reof1[to],
         reof2_from = .N()$reof2[from],
         reof2_to = .N()$reof2[to],
         reof3_from = .N()$reof3[from],
         reof3_to = .N()$reof3[to],
         reof4_from = .N()$reof4[from],
         reof4_to = .N()$reof4[to]) %>%
  as_tibble %>%
  filter(centrality_from >= centrality_to)
```
```{r, echo = FALSE}
knitr::kable(
  dat[1:6,], caption = 'A subset of the edge level data.'
)
```

```{r, eval = FALSE, echo = FALSE}
test <- recon_amplitudes %>%
  filter(between(period, 1200, 1400)) %>%
  group_by(eof, period) %>% 
  summarise(amplitude = mean(amplitude))
eof_data <- swsn %N>%
  select(lon, lat) %>%
  as_tibble %>%
  raster::extract(reof_raster, ., df = TRUE) %>%
  gather(eof, value, 2:5) %>%
  full_join(test) %>%
  mutate(key = paste(eof, period, sep = '_'),
            value = value * amplitude) %>%
  select(ID, key, value) %>%
  spread(key, value, -1)
```

## Distance modeling

First, how does similarity of two settlements vary as a function of the distance between them?
```{r echo = FALSE, fig.width = 6, fig.height = 4.5, fig.fullwidth=TRUE}
ggplot(dat, aes(distance, similarity)) +
  geom_point(alpha = .1) +
  geom_smooth() +
  labs(title = 'Empirical distance decay function', x = 'Distance', y = 'Similarity') +
  theme_bw()
```


```{r, echo=FALSE, fig.margin = TRUE, fig.cap='In log log space, we get the downward function from devries et al 2009!'}
ggplot(dat, aes(distance + .01,similarity + .001)) +
  geom_point(alpha = .1) +
  geom_smooth() +
  scale_x_log10()+
  scale_y_log10()+
  theme_minimal()
```

Do the fits to distance change with time? a little bit, but seems more to have to do with the decreae in sample size with time.
```{r echo=FALSE, distance_time_plot}
ggplot(dat, aes(distance, similarity)) +
  geom_point(alpha = .1, aes(color = time)) +
    scale_color_distiller(palette = 'Spectral', direction = 1) +
  geom_smooth(aes(group = time, color = time)) +
  geom_smooth() +
  theme_minimal()
```

```{r echo=FALSE, centrality_plot}
ggplot(dat, aes(centrality_to, similarity)) +
  geom_point(alpha = .1, aes(color = time)) +
    scale_color_distiller(palette = 'Spectral', direction = 1) +
  geom_smooth(aes(group = time, color = time)) +
  geom_smooth() +
  theme_minimal()
```


```{r include = FALSE, eval = FALSE}
ggplot(dat, aes(log((similarity * (nrow(dat) - 1) + .5 )/nrow(dat)) / (1 - (similarity * (nrow(dat) - 1) + .5 )/nrow(dat)))) +
  geom_density()
ggplot(dat, aes(log(-log((1 - (similarity * (nrow(dat) - 1) + .5 )/nrow(dat)))))) +
  geom_density()
dat %>% 
  mutate(group = paste(from, to, sep = '-')) %>%
ggplot(aes(time, similarity, group = group)) +
  geom_line(alpha = .1)

hist(dat$similarity)
```

Since we're running a beta regression, we have a choice of multiple link functions. Let's compare cloglog and logit links. Define a function that, when applied to a dataset and a gam formula, will fit a gam of that formula. Let's define one for each link function.
```{r fit_gam}
fit_gam_cloglog <- function(x){
  bam(x, method = 'REML', select = TRUE, 
         family = betar(link = 'cloglog', eps = .001),#1/(2*nrow(dat))), # MacMillan and Creelman 2005 pp 8-9
         data = dat)#(dat * (nrow(dat) - 1) + .5) / nrow(dat))
}

fit_gam_logit <- function(x){
  bam(x, method = 'REML', select = TRUE, 
         family = betar(link = 'logit', eps = .001),#1/(2*nrow(dat)))
         data = dat)#(dat * (nrow(dat) - 1) + .5) / nrow(dat))
}
```

Now we write out a list of potential model structures, and fit gams using all of them. Calculate the AICs for each formula, link function combination, and reorder the amount from the lowest AIC
```{r geo_models}
geo_models <- c(
    "s(distance, bs = 'cr')",
    "s(centrality_from, bs = 'cr') + s(centrality_to, bs = 'cr')",
    "s(distance, bs = 'cr') + s(size_from, bs = 'cr') + s(size_to, bs = 'cr')",
    "s(distance, bs = 'cr') + s(centrality_from, bs = 'cr') + s(centrality_to, bs = 'cr')",
    "s(distance, bs = 'cr') + s(size_from, size_to)",
    "s(distance, bs = 'cr') + te(size_from, size_to)",
    "s(distance, bs = 'cr') + s(centrality_from, centrality_to)",
    "s(distance, bs = 'cr') + te(centrality_from, centrality_to)",
    "s(distance, bs = 'cr') + s(size_from, bs = 'cr') + s(size_to, bs = 'cr') + s(centrality_from, bs = 'cr') + s(centrality_to, bs = 'cr')",
    "s(distance, bs = 'cr') + te(size_from, size_to) + te(centrality_from, centrality_to)"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         cloglog = purrr::map(formula, fit_gam_cloglog),
         logit = purrr::map(formula, fit_gam_logit)) %>%
  gather(link, model, 3:4) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```

Here we can see that the best model includes distance, centrality, and size in a purely additive structure with a logit link function. This model gives an adjusted R2 of ~70%.
```{r}
geo_models
```

Let's visualize the model fits.
```{r geo_models_output}
geo_models[[2,4]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)
```

So let's select this model as our baseline geographic model.
```{r real_geo_mod}
mod1 <- gam(similarity ~ s(distance, bs = 'cr') + 
            s(size_from, bs = 'cr') + 
            s(size_to, bs = 'cr') + 
            s(centrality_from, bs = 'cr') + 
            s(centrality_to, bs = 'cr'), 
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)
```

## Average climmate models
Now we can look weather average climate conditions have any predictive power for link strength.
```{r clim_mods}
clim_models <- c(
      "s(distance, bs = 'cr') + s(size_from, bs = 'cr') + s(size_to, bs = 'cr')",
      "s(distance, bs = 'cr') + s(size_from, bs = 'cr') + s(size_to, bs = 'cr') + s(prec_from, bs = 'cr') + s(prec_to, bs = 'cr') ",
      "s(distance, bs = 'cr') + s(size_from, bs = 'cr') + s(size_to, bs = 'cr') + s(temp_from, bs = 'cr') + s(temp_to, bs = 'cr') ",
      "s(distance, bs = 'cr') + s(size_from, bs = 'cr') + s(size_to, bs = 'cr') + s(prec_from, bs = 'cr') + s(prec_to, bs = 'cr') + s(temp_from, bs = 'cr') + s(temp_to, bs = 'cr') ",
      "s(distance, bs = 'cr') + s(size_from, bs = 'cr') + s(size_to, bs = 'cr') + s(temp_from, temp_to) + s(prec_from, prec_to)",
      "s(distance, bs = 'cr') + s(size_from, bs = 'cr') + s(size_to, bs = 'cr') + s(temp_from, bs = 'cr') + s(temp_to, bs = 'cr') + s(prec_from, prec_to)",
      "s(distance, bs = 'cr') + s(size_from, bs = 'cr') + s(size_to, bs = 'cr') + s(prec_from, bs = 'cr') + s(prec_to, bs = 'cr') + s(temp_from, temp_to)"
      ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         cloglog = purrr::map(formula, fit_gam_cloglog),
         logit = purrr::map(formula, fit_gam_logit)) %>%
  gather(link, model, 3:4) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```


## Variability models
```{r include = FALSE, eval = F}
  #  "ti(distance) + ti(size) + ti(distance, size)",
  #  "te(distance, size, k = 10)"
 #"s(distance, bs = 'cr') + s(abs(eof1), bs = 'cr', k = 15) + s(abs(eof2), bs = 'cr') + s(abs(eof3), bs = 'cr', k = 15) + s(abs(eof4), bs = 'cr', k = 15)",
#    "s(distance, bs = 'cr') + s(sqrt(abs(eof1)), bs = 'cr', k = 15) + s(abs(eof2), bs = 'cr') + s(abs(eof3), bs = 'cr', k = 15) + s(abs(eof4), bs = 'cr', #k = 15)"
```

```{r eval = FALSE}
mod5 <- gam(fractional_similarity ~ s(distance, bs = 'cr') +
              s(eof1, bs = 'cr') +
              s(eof2, bs = 'cr') +
              s(eof3, bs = 'cr') +
              s(eof4, bs = 'cr'),
            select = TRUE,
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)

mod6 <- gam(similarity ~ s(distance, bs = 'cr') +
              s(abs(eof1), bs = 'cr') +
              s(abs(eof2), bs = 'cr') +
              s(abs(eof3), bs = 'cr') +
              s(abs(eof4), bs = 'cr'),
            select = TRUE,
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)

summary(mod1)
plot(mod1)
gam.check(mod1)
```


```{r eval = FALSE}
mod1.5 <- gam(similarity ~ s(distance, bs = 'cr', k = 30),
            method = 'REML',
            select = TRUE,
            family = betar(link = 'cloglog', eps = .0001),
            data = filter(dat, local_significance <= .5))
summary(mod1.5)
plot(mod1.5, residuals= FALSE)
gam.check(mod1.5)

hist(degree_distribution(swsn %E>% filter(local_significance <= .05 & similarity > 0), mode = 'out'))
swsn %E>%
  filter(time == 1) %>%
  filter(local_significance < .05 & similarity > 0) %N>%
  mutate(degree = centrality_degree(weights = similarity, mode = 'out')) %>%
  as_tibble %>%
  ggplot(aes(degree)) +
  geom_density()



dat %>%
  filter(local_significance <= .05) %>%
  ggplot(aes(similarity)) +
  geom_density() +
  facet_wrap(~time)
```


```{r eval = FALSE}
mod2 <- gam(similarity ~ s(distance, bs = 'cr') +
                         s(size_from, bs = 'cr') +
                        s(size_to, bs = 'cr'),
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)


mod7 <- gam(similarity ~ s(distance, bs = 'cr') +
              eof2_same +
              s(eof1, bs = 'cr') +
              s(eof2, bs = 'cr', by = eof2_same) +
              s(eof3, bs = 'cr') +
              s(eof4, bs = 'cr'),
            select = TRUE,
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)
mod8 <- gam(similarity ~ s(distance, bs = 'cr') +
                eof2_same +
              s(abs(eof1), bs = 'cr') +
              s(abs(eof2), bs = 'cr', by = eof2_same) +
              s(abs(eof3), bs = 'cr') +
              s(abs(eof4), bs = 'cr'),
            select = TRUE,
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)

levels(dat$eof4_same)
mod2 <- gamm(similarity * 1000 ~ s(distance, bs = 'cr'),
            method = 'REML',
            family = poisson,
            correlation = corAR1(form = ~time|edge),
            data = dat)

plot(mod1, residuals = TRUE);plot(mod2, residuals = FALSE);plot(mod3, scheme = 1, residuals = TRUE);plot(mod4, scheme = 1);plot(mod5, residuals = FALSE)
plot(mod6);plot(mod7);plot(mod8)
AIC(mod1);AIC(mod2);AIC(mod3);AIC(mod4);AIC(mod5);AIC(mod6);AIC(mod7);AIC(mod8)
gam.check(mod5)
summary(mod7)

gam.check(mod1)
summary(residuals(mod1, type = 'response'))
cbind(dat, test = residuals(mod1)) %>%
  filter(test > 6)
```


```{r eval = FALSE}
swsn %E>%
  filter(centrality_from >= centrality_to) %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
  filter(resid > .25 | resid < -.25) %>%
  mutate(fac = if_else(resid > 0, 'pos', 'neg')) %>%
  ggraph('manual', node.positions = pts) +
  geom_edge_link(aes(alpha = abs(resid), color = resid)) +
  facet_graph(time ~ fac, col_type = 'edge') +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  scale_edge_colour_distiller(palette = 'RdBu', limits = c(-1,1)) +
  coord_equal() +
  theme_graph()
```
```{r eval = FALSE}
swsn %N>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[6,2]], type = 'response')) %>%
  filter(resid > .25 | resid < -.25) %>%
  mutate(fac = if_else(resid > 0, 'pos', 'neg')) %>%
  ggraph('manual', node.positions = pts) +
  geom_edge_link(aes(alpha = abs(resid), color = resid)) +
  facet_graph(time ~ fac, col_type = 'edge') +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  scale_edge_colour_distiller(palette = 'RdBu', limits = c(-1,1)) +
  coord_equal() +
  theme_graph()
```
Looks like distance is correlated with residuals
```{r eval = FALSE}
dat %>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  as_tibble %>%
  qplot(distance, resid, data = ., geom = 'point', alpha = I(.1), color = as.factor(time))
```
```{r eval = FALSE}
dat %>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  as_tibble %>%
  qplot(abs(eof1), resid, data = ., geom = 'point', alpha = I(.1), color = as.factor(time))
```
```{r eval = FALSE}
swsn %E>%
    filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  filter(time == 1) %N>%
  mutate(centrality = centrality_eigen(weights = similarity)) %E>%
  mutate(cent = .N()$centrality[from] * .N()$centrality[to]) %>%
  as_tibble %>%
  qplot(cent, resid, data = ., geom = 'point', alpha = I(.1))
```

```{r eval = FALSE}
swsn %E>%
    filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  filter(time == 3) %N>%
  mutate(centrality = centrality_eigen(weights = similarity)) %E>%
  mutate(cent = .N()$centrality[from] + .N()$centrality[to]) %>%
  as_tibble %>%
  qplot(cent, similarity, data = ., geom = 'point', alpha = I(.1))
```

```{r echo = FALSE, eval = FALSE}
phyda <- ncdf4::nc_open('Data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc')
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,1])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,4])

plot(ncdf4::ncvar_get(phyda, 'Pac130_mn')[1100:1999], recon_eof$amplitude[,2])
plot(ncdf4::ncvar_get(phyda, 'Pac130_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_eof$amplitude[,4])
cor(ncdf4::ncvar_get(phyda, 'amo_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'amo_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_eof$amplitude[,1])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'EPac_mn')[1100:1999], recon_eof$amplitude[,4])
```

# References {#references .unnumbered}
