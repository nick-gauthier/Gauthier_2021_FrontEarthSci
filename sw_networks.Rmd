---
title: "Supplemental Information"
subtitle: 'Analysis scripts for "Drought Variability and the Robustness of Agrarian Social Networks"'
author: "Nicolas Gauthier"
date: "Last knit on: `r format(Sys.time(), '%d %B, %Y')`"
#bibliography: bibliography.bibtex
output:
  tufte::tufte_handout:
    citation_package: natbib
    #latex_engine: xelatex
    toc: true
    highlight: pygments
  tufte::tufte_html:
    tufte_features: ["fonts", "italics"]
    toc: true
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, message = FALSE, warning = FALSE)
```

# Introduction
\begin{fullwidth}
This RMarkdown document contains all analysis code used to produce the results and figures in the main text.

In this two-part analysis we first 
\end{fullwidth}

## Data sources
This R Markdown script requires raster data of [observed present-day temperature and precipitation](http://www.prism.oregonstate.edu/normals/)^[http://www.prism.oregonstate.edu/normals/] 30-year averages and 100-year time series of the [Standardized Precipitation-Evapotranspiration Index](https://wrcc.dri.edu/wwdt/time/)^[https://wrcc.dri.edu/wwdt/time/]. We'll also need reconstructed SPEI rasters^[https://zenodo.org/record/1198817]. Finally, get the CGIAR version of the SRMTM [digital elevation model](http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/)^[http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/], resampled to 250m. With permission, also acquire the [Southwest Social Networks Database](http://www.southwestsocialnetworks.net/data.html)^[http://www.southwestsocialnetworks.net/data.html].

## Packages
Load the packages used in the analyses.
```{r packages}
library(raster) # raster data manipulation
library(tidyverse) # data analysis and plotting
library(rgdal) # reprojection of spatial points
library(gdistance) # least cost distance calculations
library(wql) # empirical orthogonal function calculation
library(tidygraph) # network analysis
library(ggraph) # network plotting
library(mgcv) # GAM fitting
library(mgcViz) # GAM plotting
library(dismo)
library(sf)
library(smoothr)
library(rayshader)
```

```{r include = FALSE}
theme_set(theme_bw())
```

# Archaeological network data
Now we move on to the archaeological social network proxies.^[Go to http://www.southwestsocialnetworks.net for more information on this dataset and the project] First we read in a csv file containing information for all of the archaeological sites in the database, including spatial location and number of rooms present from each time period. These will become the node-level data for the network.
```{r}
node_data <- read_csv('data/attributes_orig.csv') %>%
  select(-c(SWSN_ID,GKPM:Periods, N0S1))
```

```{r, echo = FALSE}
knitr::kable(
  node_data[1:3,1:7], caption = 'A subset of the node level data.'
)
```

Next import the edge data. Our data come in the form of similarity matrices, one for each time step. We need to define a function for importing and processing a single time step of the edge datasets. This function imports the csv, turns it into a matrix, and then a directed graph. We then filter out self loops, calculate eigenvector centrality for each of the nodes, then for each edge record the time period and centralities of the neighboring nodes.
```{r read_swsn}
read_swsn <- function(net, time){
  read.csv(net, row.names = 1, check.names = FALSE) %>% 
    as.matrix %>% # convert to a matrix
    replace(. == 0, 999) %>% # replace 0 values with 999 temporarily
    as_tbl_graph(directed = TRUE) %E>% # convert to directed graph
    mutate(weight = if_else(weight == 999, 0, weight)) %>% # convert 999 values back to 0
    rename(similarity = weight) %>% # rename the edge weights
    filter(!edge_is_loop()) %>% # remove self loops
    mutate(time = 1150 + 50 * time) %>% # set the time period
    activate('nodes') # activate node data
}
```

Next we map this function over a lists of simlaritity matrices. Get a list of the similarity files, map the above function over them, reduce to a single graph.  Use reduce and graph join to combiine all of these networks into a single netwtork, retaining the edge weights for each  separate period.
```{r swsn_import}
swsn <- list.files('data/Sim', full.names = TRUE) %>% # get a list of all the similarity data
  imap(read_swsn) %>% # map the read_swsn function over these files
  reduce(graph_join, by = 'name') %>% # combine into a single graph
  left_join(node_data, by = c('name' = 'SWSN_Site')) # join to node data by site name
```

```{r echo = FALSE, fig.margin = TRUE, fig.cap = 'Example 6x6 simililarity matrix. Note the symmetry'}
sim_tmp <- read.csv('data/Sim/AD1200sim.csv', row.names = 1, check.names = FALSE) %>%
  as.matrix %>%
  .[1:3,1:3]
knitr::kable(sim_tmp, caption = 'Example similarity matrix')
```


Create a separate spatial points object, storing the locations of the sites. Convert the coordinates from utm to lat lon, and add back to the original data
```{r site_points}
pts <- swsn %N>% # start with the full SWSN data
  select(x = EASTING, y = NORTHING) %>% # pull the spatial information
  as_tibble %>% # convert to a tibble
  SpatialPoints(proj4string=CRS("+proj=utm +zone=12 +datum=NAD27")) %>% # points are in UTM
  spTransform(CRS("+proj=longlat +datum=WGS84")) %>%  # convert to lat lon
  coordinates %>% # extract the coordinates
  data.frame # turn into a data frame
```

# Least Cost Distances

Next we calculate the impact of rugged terrain on the potential flow of people and information between the sites in the SWSN database. For this we need to use the SRTM digital elevation model. From the height data in the DEM, we calculate slope, accounting for cognitive biases people have when assessing the steepness of high slopes (people tend to exaggerate slopes above a certain threshold). From this map of perceived slope, we calculate "perceived walking speeds", using Tobler's hiking function. We uses these perceived walking speeds as a measure of the perceived, symmetric costs of traveling between two locations on the landscape, and from these estimate the least cost paths from every site to every site. From there, we extract the 15 nearest neighbors of each site in each time period, and use this nearest neighbor network as a spatial network, from which to estimate new travel costs (limited to paths along this nearest neighbor network).

First import the elevation dataset, crop to the study area, and resample.
```{r elevation}
bbox <- extent(c(-113, -107, 31, 37.5)) # define our study area

elev <- raster('~/SRTM_NE_250m_TIF/SRTM_W_250m.tif') %>% # import the SRTM DEM
  crop(bbox) %>% # crop to study area
  aggregate(fact  = 3) # aggregate to 750m resolution
```


The topography is very rugged. Plot it out with some vertical exaggeration to see.
```{r rayshader, echo = FALSE, fig.margin = TRUE}
matrix(raster::extract(elev,raster::extent(elev),buffer=1000),
               nrow=ncol(elev),ncol=nrow(elev)) %>%
  sphere_shade(texture = "imhof1", zscale = 10, progbar = FALSE) %>%
  plot_map()
```

```{r}
tobler_adjusted <- function(x){
  6 * exp(-3.5 * 2.15 * x)
}
```

```{fig.margin = TRUE, echo = FALSE}
slope <- seq(-1, 1, .01)
qplot(slope, tobler_adjusted(slope), geom = 'line' ) +
  geom_vline(xintercept = 0, linetype = 2) +
  labs(x = 'Slope (rise over run)', y = 'Walking speed (m/s)', title = "Tobler's hiking function") +
  theme_classic(base_size = 20)
```
Now we use the package *gdistance* to calculate our LCPs. This package uses a transition matrix approach, whereby we work with matrices that contain the costs or transmissiveness of travel from each cell to its immediate 16 neighbor cells. This allows for a sparse matrix representation, because there is no possible connection to non neighboring cells, which considerable reduces the computational burden.
```{r conductance, cache = TRUE, dependson='elevation'}
# Calculate the absolute difference in elevation between each cell and its 16 neighbors.
altDiff <- function(x){abs(x[2] - x[1])}
hd <- transition(elev, altDiff, 16, symm = TRUE)

# Divide the height differences by the horizontal distances between cells, resulting in slopes.
slope_c <- geoCorrection(hd, type = 'c')

# Figure out which cells are adjacent to one another, queen's case.
adj <- adjacent(elev, cells = 1:ncell(elev), directions = 16)

# Use Tobler's hiking function to calculate walking speed from *cognitive* slope.
speed_c <- slope_c
speed_c[adj] <- tobler_adjusted(slope_c[adj])

# Again divide by intercell distance, resulting in our final conductance matrix.
conductance_c <- geoCorrection(speed_c, type = 'c')
```


```{r eval = FALSE}
slope_r <- geoCorrection(hd, type = 'r')
speed_r <- slope_r
speed_r[adj] <- 6 * exp(-3.5 * 2.15 * slope_r[adj])
conductance_r <- geoCorrection(speed_r, type = 'r')

pas <- passage(conductance_r, as.matrix(pts)[1:10,], as.matrix(pts)[200:210,], theta = .05)
commute_dist <- pts %>%
  as.matrix %>%
  rSPDistance(conductance_r, ., ., theta = 0.05) 
```

Using the conductance matrix, calculate the full pairwise least cost distance matrix.
```{r distance_matrix, cache = TRUE, dependson=c('site_points','conductance')}
dist_mat <- pts %>% # start with all the site locations
  as.matrix %>% # convert to a matrix of x and y points
  costDistance(conductance_c, .) %>% # calculate the least cost distances
  as.matrix %>% # convert to a matrix
  replace(. == 0, 60) # make close sites close instead of overlapping
```

Now we prune the full pairwise distance matrix to only include each site's 15 nearest neighbors.^[This is assuming each site has a conserved amount of potential short distance travel destinations, rather than a fixed travel threshold.] In the case where the path from site i to j is in i's shortest paths but not in j's, the path is included anyway.
```{r time_mat, fig.margin = TRUE, echo = FALSE}
time_mat <- dist_mat %>%
  as_tbl_graph(directed = FALSE) %E>%
  filter(!edge_is_loop()) %>%
  rename(distance = weight) %>%
  filter(distance <= 3600 * 6) %>%
  #mutate(max_dist = case_when(
  #    distance <= 3600 * 1 ~'1 Hours',
  #    distance <= 3600 * 2 ~'2 Hours',
  #    distance <= 3600 * 3 ~'3 Hours',
  #    distance <= 3600 * 4 ~'4 Hours',
  #    distance <= 3600 * 5 ~'5 Hours',
  #    distance <= 3600 * 6 ~'6 Hours',
  #    distance <= 3600 * 7 ~'7 Hours',
  #    distance <= 3600 * 8 ~'8 Hours')) %>%
  as_tibble %>%
  inner_join(activate(swsn, edges), ., by = c('from', 'to'))

ggraph(time_mat, 'manual', node.positions = pts) +
  geom_edge_link(alpha = .1) +
  theme_void() +
  facet_edges(~time) +
  coord_equal()
```

```{r nearest_neighbor}
nn_mat <- dist_mat %>% 
  as_tbl_graph(directed = TRUE) %E>%
  filter(!edge_is_loop()) %>% # remove self loops
  rename(distance = weight) %>% 
  as_tibble %>% 
  left_join(activate(swsn, edges), ., copy = TRUE, by = c('from', 'to')) %>% # join full SWSN edge data
  morph(to_split, time, split_by = 'edges') %>% # split into subgraphs
  group_by(from) %>% 
  top_n(-15, distance) %>% # retain the top 15 smallest (i.e. shortest) edges
  crystallize %>% # make new graphs from the transformed graphs
  pull(graph) %>% # extract just the graph information
  map(as.undirected, mode = 'collapse', edge.attr.comb = 'first') %>%
  map(as_tbl_graph) %>% # convert back to tidygraph objects
  reduce(graph_join) # rejoin the subgraphs from each period into a single graph
```

```{r, echo = FALSE, fig.fullwidth = TRUE, fig.width=10}
ggraph(nn_mat, 'manual', node.positions = pts) +
  geom_edge_link(alpha = .1) +
  facet_edges(~time, nrow = 1) +
  theme_void() +
  coord_equal() +
  ggtitle('Nearest neighbor network', 'k = 15')
```

```{r eval = FALSE, echo = FALSE}
shortest_path <- function(graph, src, dest){
  path <- suppressWarnings(get.shortest.paths(graph, src, dest))
  path <- names(path$vpath[[1]])
  if (length(path)==1) NULL else path
} 

#'@return the sum of the weights of all the edges in the given path
path_weight <- function(path, graph) sum(E(graph, path=path)$weight)

#'@description sorts a list of paths based on the weight of the path
sort_paths <- function(graph, paths) paths[paths %>% sapply(path_weight, graph) %>% order]

#'@description creates a list of edges that should be deleted
find_edges_to_delete <- function(A,i,rootPath){
  edgesToDelete <- NULL
  for (p in A){
    rootPath_p <- p[1:i]
    if (all(rootPath_p == rootPath)){
      edge <- paste(p[i], ifelse(is.na(p[i+1]),p[i],p[i+1]), sep = '|')
      edgesToDelete[length(edgesToDelete)+1] <- edge
    }
  }
  unique(edgesToDelete)
}

#returns the k shortest path from src to dest
#sometimes it will return less than k shortest paths. This occurs when the max possible number of paths are less than k
k_shortest_yen <- function(graph, src, dest, k){
  if (src == dest) stop('src and dest can not be the same (currently)')

  #accepted paths
  A <- list(shortest_path(graph, src, dest))
  if (k == 1) return (A)
  #potential paths
  B <- list()

  for (k_i in 2:k){
    prev_path <- A[[k_i-1]]
    num_nodes_to_loop <- length(prev_path)-1
    for(i in 1:num_nodes_to_loop){
      spurNode <- prev_path[i]
      rootPath <- prev_path[1:i]

      edgesToDelete <- find_edges_to_delete(A, i,rootPath)
      t_g <- delete.edges(graph, edgesToDelete)
      #for (edge in edgesToDelete) t_g <- delete.edges(t_g, edge)

      spurPath <- shortest_path(t_g,spurNode, dest)

      if (!is.null(spurPath)){
        total_path <- list(c(rootPath[-i], spurPath))
        if (!total_path %in% B) B[length(B)+1] <- total_path
      }
    }
    if (length(B) == 0) break
    B <- sort_paths(graph, B)
    A[k_i] <- B[1]
    B <- B[-1]
    }
  A
}

k_shortest_yen(as.directed(nn_mat %E>% filter(time == 1200), mode = 'mutual'), 1, 12, 10)
```

```{r eval = FALSE}

all_shortest_paths(graph= spatial_net, from = 1, to = 12, mode = 'all', weights = as_tibble(spatial_net, activate = 'edges')$distance)
```

```{r, eval = FALSE, echo = FALSE}
#devtools::install_github('ecohealthalliance/yenpathy')
library(yenpathy)
spatial_net <- nn_mat %E>%
  filter(time == 1200) %>%
  as_tibble %>%
  select(from, to, distance) %>%
  makedirect()
sim_net <- swsn %E>%
  as_tibble %>%
  filter(time == 1200) %>% 
  filter(to < from)
makedirect <- function(x){
  x %>%
    select(from = to, to = from, distance = distance) %>%
    bind_rows(x)
}
sim_net[[1,1]]
t1 <- c()
sim_net$paths <- lapply(1:nrow(sim_net), function(x){
  k_shortest_paths(graph_df = spatial_net, start_vertex = sim_net[[x,1]], end_vertex = sim_net[[x,2]], k = 4, verbose = FALSE)}
)

test[[1]]
for(i in 1:10){
  t1 <- c(t1, k_shortest_paths(graph_df = spatial_net, start_vertex = sim_net[[i,1]], end_vertex = sim_net[[i,2]], k = 4, verbose = FALSE))
}

t1 <- sim_net %>% mutate(k_paths = map2(from, to, ~k_shortest_paths(graph_df = spatial_net, start_vertex = .x, end_vertex = .y, k = 4, verbose = FALSE)))


swsn %E>% filter(time == 1200) %>% select(from, to, di)
swsn %>% as.undirected %>% as_tbl_graph
test %>% k_shortest_paths(1, 3, k = 5)
k_shortest_paths(spatial_net,1, 12, k = 1)


nn_mat %E>%
  filter(time == 1200) %N>%
  filter(!node_is_isolated())
```



```{r shortest_paths, eval = FALSE}
path_dat <- nn_mat %E>% # get the neighbor matrix edgelist
  as_tibble %>% # convert to tibble
  select(from,to) %>% # extract the node ids for each edge pair
  distinct %>% # limit to distinct edges
  group_by(from) %>% # group by origin node
  nest %>% # nest by origin node
  mutate(to = map(data, ~c(.$to) %>% unlist)) %>% # turn the "to" column into list of destinations for each origin
  select(-data) %>% # drop the nested data frame
  # calculate the shortest paths
  mutate(paths = map2(from, to, ~shortestPath(conductance_c, 
                                    as.matrix(pts)[.x,], as.matrix(pts)[.y,], 
                                    output = 'SpatialLines')))
```


Merge all the lines into one object.
```{r eval = FALSE}
paths_sf <- path_dat %>% 
  mutate(paths = map(paths, st_as_sf)) %>% 
  pull(paths) %>% 
  do.call(rbind, .) %>%
  bind_cols((path_dat %>% select(-paths) %>% unnest), .) %>%
  st_sf %>%
  filter(., st_is_valid(.)) %>%
  smooth(method = 'ksmooth', smoothness = 20) 
```

```{r eval = FALSE, include = FALSE}
paths_sf %>%
  inner_join(activate(swsn, 'edges') %>% as_tibble, .) %>%
  ggplot() +
  geom_sf(alpha = .1) +
  facet_wrap(~time, nrow = 1)

paths_sf %>%
  inner_join(activate(swsn, 'edges') %>% as_tibble, .) %>%
  st_sf %>%
  as_Spatial %>%
  fortify(region = 'id') %>%
  left_join(inner_join(activate(swsn, 'edges') %>% as_tibble, paths_sf) %>% mutate(id = as.character(1:n())) %>% select(-geometry)) %>%
ggplot(aes(long, lat)) +
  geom_raster(data = as.data.frame(aggregate(raster(conductance_c), fact = 2), xy = T, na.rm = T), aes(x, y, fill = layer)) +
  geom_path(aes(group = group), alpha = .4) +
  scale_fill_viridis_c() +
  facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_equal()
```


```{r network_plot, eval = FALSE, echo = FALSE}
states <- maps::map('state', regions = c('arizona', 'new mexico'), fill = TRUE, plot = FALSE)

swsn %E>%
  arrange(similarity) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = similarity, color = similarity)) +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  geom_node_point() +#aes(size = centrality, color = centrality)) +
  facet_edges(~time) +
  scale_edge_alpha() +
  coord_equal() +
  theme_graph()
```

# Climate Analysis
First we estimate present and past climate patterns.

## Study Area
First we define a boundary box covering much of the western United States, ranging between W$124\deg$ and W$105\deg$ and N$31\deg$ and N$41\deg$, which we'll use to constrain all subsequent climate analyses. This study area is significantly larger than the one we'll define in the next section for the network analysis. This allows us to sample a much wider range of climatic variability, while still remaining within the broader western US climate zone. This ensures that both A) our statistical analyses will be more robust to sampling error and B) the results will be less sensitive to the exact location and dimensions of our study area.
```{r bbox_map, fig.margin=TRUE, echo = FALSE, fig.cap = 'Locations of 2 bounding boxes.'}
bbox_wus <- extent(c(-124, -105, 31, 41))
bbox <- extent(c(-113.5, -106.5, 31, 37.5))
bb1 <- as(bbox_wus, 'SpatialPolygons') %>%
  fortify
bb2 <- as(bbox, 'SpatialPolygons') %>%
  fortify

map_data("usa") %>%
  ggplot(aes(x = long, y = lat, group = group)) + 
  geom_polygon(color = 'darkgrey', fill = NA) + 
  geom_polygon(data = bb1, color = 'black', fill = NA) +
  geom_polygon(data = bb2, color = 'black', fill = NA) +
  coord_fixed(1.3) +
  theme_void()
```

```{r bbox}
bbox_wus <- extent(c(-124, -105, 31, 41))
```

## Climate Dataa
Import high resolution SPEI maps generated from PRISM data. Calculate the average summertime (JJA) SPEI value for each year.
```{r import_observations}
# calculate average JJA SPEI
spei_obs <- ((brick('data/spei12_6_PRISM.nc') + 
              brick('data/spei12_7_PRISM.nc') + 
              brick('data/spei12_8_PRISM.nc')) / 3) %>%
  crop(bbox_wus) %>% # crop to the western US bounding box
  `names<-`(1895:2017) %>% # add year names
  .[[2:105]]  # SPEI calculated on 12 month lag, so drop 1st year
```

Import the reconstructed SPEI fields from PHYDA. PHYDA uses a novel off-line data assimilation approach, using simulated SPEI from the CESM LME experiments as physically-consistent model priors and a network of tree rings, ice cores, and corals as assimilated observations.
```{r import_reconstructions}
spei_recon <- brick('data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc',
                    varname = 'spei_mn') %>%
   .[[1100:1999]] %>% # extract years of interest
   rotate %>% # rotate longitudes to -180 to 180
   crop(bbox_wus, snap = 'out') # crop to western US
```


```{r plot_variance_trend, echo = FALSE, fig.margin=TRUE, fig.cap = 'Plotting out the time series of the reconstruction reveals no clear trend in the mean. It does, however, show a pattern of increasing variance over time, which is entirely a function of the changing number of proxies used in the data assimilation approach.'}
spei_recon %>% 
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, value, 3:902) %>%
  mutate(year = str_sub(year, 2),
         year = as.numeric(year)) %>%
  ggplot(aes(year, value)) +
  geom_line(alpha = .1, aes(group = interaction(x, y))) +
  geom_smooth() +
  ggtitle('Standardized Precipitation-Evapotranspiration Index in the American Southwest',
          'June-August, 12 month lag') +
  theme_bw()
```

## Empiricial Orthogonal Functions
We calculate the leading Empirical Orthogonal Functions of a ~100 year series of the summertime average Standardized Precipitation-Evapotranspiration Index. These patterns then undergo a varimax rotation to highlight more physically meaningful spatial patterns. We compare these patterns to those derived from a long term (~2ka) reconstruction based on data assimilation and CESM LME. We confirm that the spatial patterns detected for the past 100 years have been robust over time and explain up to 85% of the variance in a full 1,000 year sequence of reconstructed drought dynamics.

Now we calculate the rotated empirical orthogonal functions for both the observed and reconstructed drought maps. For the observations, we see that the leading 5 eofs explain 85% of the variance in the series, so we'll retain those for rotation.
```{r plot_variance_prism, fig.margin = TRUE, echo = FALSE}
#this code is the same as the eofNum function above, but needs a slight adjustment here because that function assumes more time than spatial indices
eigs <- prcomp(t(as.data.frame(spei_obs, na.rm = TRUE)), scale. = FALSE)[["sdev"]]^2
eigs.pct <- 100 * eigs/sum(eigs)
eigs.lo <- eigs * (1 - sqrt(2/104))
eigs.hi <- eigs * (1 + sqrt(2/104))
cumvar <- round(cumsum(eigs.pct), 1)
p <- 104
d <- data.frame(rank = factor(1:p), eigs, eigs.lo, eigs.hi, 
        cumvar)
d <- within(d, cumvar.line <- eigs.hi + 0.02 * max(eigs.hi))
d <- d[1:min(p, 10), ]
ggplot(data = d, aes(x = rank, y = eigs)) + geom_errorbar(aes(x = rank, 
        ymin = eigs.lo, ymax = eigs.hi), width = 0.3) + geom_point(size = 3) + 
        geom_text(aes(x = rank, y = cumvar.line, label = cumvar), 
            size = 3, vjust = 0) + labs(list(x = "Rank", y = "Eigenvalue")) + 
        theme(panel.grid.minor = element_blank())
```

Look at the variances for the eofs of each field, to inform truncation. Let's retain the leading four modes from PHYDA
```{r plot_variance_phyda, fig.margin=TRUE, echo = FALSE,fig.cap = 'Variance explained'}
spei_recon %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  eofNum(scale. = FALSE)
```
Repeat for the prism observations. Again, we'll retain the leading four modes.

The standard errors on the above plots assume there is no autocorrelation. Check to see if that is indeed the case.
```{r plot_acf, fig.margin=TRUE, echo = FALSE,fig.cap = 'Temporal autocorrelation'}
spei_recon %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  as_tibble %>%
  gather(key, value) %>%
  pull(value) %>%
  acf

spei_obs %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  as_tibble %>%
  gather(key, value) %>%
  pull(value) %>%
  acf
```
It looks like there is some autocorrelation, will have to adjust in the future.


Now, calculate the eofs for both fields, retaining the 4 leading components in each case for rotation
```{r calc_eofs}
# Decide how many modes to retain
n_modes <- 4 

obs_eof <- spei_obs %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  eof(n_modes, scale. = FALSE)

recon_eof <- spei_recon %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  eof(n_modes, scale. = FALSE)
```
Repeat for the unroated eofs
```{r}
pr <- prcomp(t(as.data.frame(spei_obs, na.rm = TRUE)), scale. = FALSE)
eigenval <- pr[["sdev"]][1:4]^2
eigenvec <- pr[["rotation"]][, 1:4]
eof <- eigenvec %*% diag(sqrt(eigenval), 4, 4)
scores <- pr[["x"]][, 1:4]
amp <- scale(scores)
attributes(amp)$`scaled:center` <- attributes(amp)$`scaled:scale` <- NULL
eof_obs <- as.matrix(eof)
colnames(eof_obs) <- colnames(amp) <- paste("EOF", 1:4, sep = "")
rownames(eof_obs) <- colnames(t(as.data.frame(spei_obs, na.rm = T)))

pr <- prcomp(t(as.data.frame(spei_recon, na.rm = TRUE)), scale. = FALSE)
eigenval <- pr[["sdev"]][1:4]^2
eigenvec <- pr[["rotation"]][, 1:4]
eof <- eigenvec %*% diag(sqrt(eigenval), 4, 4)
scores <- pr[["x"]][, 1:4]
amp <- scale(scores)
attributes(amp)$`scaled:center` <- attributes(amp)$`scaled:scale` <- NULL
eof_recon <- as.matrix(eof)
colnames(eof_recon) <- colnames(amp) <- paste("EOF", 1:4, sep = "")
rownames(eof_recon) <- colnames(t(as.data.frame(spei_recon, na.rm = T)))
```


Let's map out the spatial and temporal patterns in the REOFs. First we'll look at the spatial patterns. It looks like the observed and reconstructed datasets reveal very similar spatial patterns for the 4 leading modes.
```{r plot_reof_recons, fig.margin = TRUE, echo = FALSE, fig.cap = 'Reconstructed REOFs'}
as.data.frame(spei_recon[[1]], xy = TRUE, na.rm = TRUE) %>% cbind(recon_eof$REOF) %>%
  select(-3) %>%
  gather(eof, value, 3:(n_modes+2)) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
    scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() + 
  coord_quickmap()
```

```{r plot_eof_recons, fig.margin = TRUE, echo = FALSE, fig.cap = 'Reconstructed REOFs'}
as.data.frame(spei_recon[[1]], xy = TRUE, na.rm = TRUE) %>% cbind(eof_recon) %>%
  select(-3) %>%
  gather(eof, value, 3:(4+2)) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
    scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() + 
  coord_quickmap()
```

```{r plot_eof_obs, echo = FALSE, fig.width = 6, fig.height = 4, fig.cap = 'Observed drought REOFs'}
as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>% 
  cbind(eof_obs) %>%
  select(-3) %>%
  gather(eof, value, 3:(4 + 2)) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() +
  coord_quickmap(xlim = c(-124, -106), ylim = c(31, 40)) +
  geom_polygon(data =  map_data("state"), aes(x = long, y = lat, group = group), color = 'darkgrey', fill = NA) +
  ggtitle('Leading 4 unrotated empirical orthogonal functions',
          'SPEI') +
  theme(legend.position = "bottom")
```

```{r plot_reof_obs, echo = FALSE, fig.width = 6, fig.height = 4, fig.cap = 'Observed drought REOFs'}
as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>% 
  cbind(obs_eof$REOF) %>%
  select(-3) %>%
  mutate(EOF1 = -1 * EOF1) %>%
  gather(eof, value, 3:(n_modes + 2)) %>%
  mutate(value = value * -1) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
  scale_fill_distiller(palette = 'BrBG', direction = -1, limits = c(-1, 1)) +
  #scale_fill_viridis_c(option = 'magma', limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() +
  coord_quickmap(xlim = c(-124, -106), ylim = c(31, 40)) +
  geom_polygon(data =  map_data("state"), aes(x = long, y = lat, group = group), color = 'darkgrey', fill = NA) +
  ggtitle('Leading 4 rotated empirical orthogonal functions',
          'SPEI') +
  theme(legend.position = "bottom")
```

A key assumption here is that the REOFs calculated from both the observations and reconstructions correspond to the same physical phenomena. That way we can just use the observed patterns as the high resolution patterns, and infer their temporal evolution from the reconstructions. This means we don't have to downscale the reconstructed reofs to use them, as otherwise we'd have to deal with issues of spatial represnetaitveness an non-overlap. Now we can be ensured that these are the same signals.

To confirm this, let's plot the time series (PCs) of each observed and reconstructed REOF against each other, to confirm that they correspond to the same patterns. Look at the correlations between the modes.
```{r amplitudes}
amplitudes <- left_join(
  recon_eof$amplitude %>%
    .[797:900,] %>%
    as_tibble %>%
    mutate(year = 1896:1999) %>%
    gather(eof, recon, 1:n_modes),
  obs_eof$amplitude %>%
    as_tibble %>%
    mutate(year = 1896:1999) %>%
    gather(eof, obs, 1:n_modes))
```

```{r plot_amplitudes, fig.fullwidth=TRUE, fig.height=4, fig.width=10, echo = FALSE, fig.cap = 'Strong linear correlation between the observed and reconstructed EOFs'}
ggplot(amplitudes, aes(recon, obs)) +
  geom_point(aes(color = year)) +
  geom_smooth(method = 'lm') +
  scale_color_viridis() +
  facet_wrap(~eof, nrow = 1) +
  ggtitle('Linear fits between observed and reconstructed amplitudes', '1896-1999') +
  theme_minimal() +
  coord_fixed() +
  theme(legend.position = "bottom")
```

```{r eval = F}
library(remote)
sst_jja <- brick('~/Downloads/sst.mnmean.v4.nc') %>%
  .[[505:1752]] %>%
  .[[sort.int(c(seq(6, 1248, 12),seq(7, 1248, 12),seq(8, 1248, 12)))]] %>%
  stackApply(rep(1:104, each = 3), mean) %>%
  anomalize

test <- calc(sst_jja, fun=function(x) cor(x,obs_eof$amplitude[,1]))
plot(test)
cor(sst_jja[1,1], t1$EOF1)


sst_ann <- brick('~/Downloads/sst.mnmean.v4.nc') %>%
   .[[508:1755]] %>%
  stackApply(rep(1:104, each = 12), mean) %>%
  anomalize
```

Now let's fit these models in practice. First create a data frame of past amplitudes.
```{r past_amplitudes}
past_amplitudes <- recon_eof$amplitude %>%
    as_tibble %>%
    mutate(year = 1100:1999) %>%
    gather(eof, recon, 1:n_modes) %>%
  group_by(eof) %>%
  nest(.key = 'recons')
```


We can fit gams between the observed and reconstrcuted PC time series, and then use these to predict the eof amplitudes back in time. Adding in this regression step essentalially bias-corrects the reconstructed amplitudes
```{r recon_amplitudes}
recon_amplitudes <- amplitudes %>%
  group_by(eof) %>%
  nest %>%
  mutate(mod = purrr::map(data, ~gam(obs ~ s(recon, bs = 'cr'), 
                                     data = ., method = 'REML', 
                                     select = TRUE))) %>%
  left_join(past_amplitudes, by = 'eof') %>%
  mutate(predictions = purrr::map2(mod, recons, 
                                   ~predict(.x, .y, type = 'response')),
         predictions = purrr::map(predictions, ~.[1:400]),
         predictions = purrr::map(predictions, 
                                  ~tibble(year = 1100:1499, amplitude = .)),
         predictions = purrr::map(predictions, 
                                  ~mutate(., 
                                          amp_smooth = zoo::rollmean(amplitude, k = 51, fill = NA)))) %>%
  select(eof, predictions) %>%
  unnest %>%
  mutate(period = floor((year/50)) * 50) 
```

How have the amplitudes of these patterns changed over time? Let's plot the reconstructed time series for each, along with an estimated trend line.
```{r plot_recon_amplitudes, fig.width = 10, fig.height = 4, echo = FALSE, fig.fullwidth=TRUE}
ggplot(recon_amplitudes, aes(year, amplitude, group = eof)) +
  geom_line(aes(color = eof), alpha = .7) +
  geom_line(aes(y = amp_smooth)) +
  scale_color_brewer(palette = 'Spectral') +
  facet_wrap(~eof) +
  theme_minimal()+
  theme(legend.position = "bottom")
```

These results point to EOFs 3 and 4 as having major shocks at aropund 1300 AD.
```{r, echo = FALSE, fig.margin=TRUE, fig.cap='Are there any time periods with marked signals?'}
recon_amplitudes %>%
  filter(between(period, 1200, 1400)) %>%
  ggplot(aes(factor(period), amplitude)) +
  geom_boxplot() +
  geom_hline(yintercept = 0, color = 'red', linetype = 2) +
  scale_color_brewer(palette = 'Spectral') +
  facet_wrap(~eof) +
  theme_minimal()
```

## Environmental Data
Import higher resolution environmental layers and crop the reof files. Define a function that will import all the prism normals for a given variable name. outputs a brick.
```{r import_prism}
get_normals <- function(var){
  paste0('data/PRISM/PRISM_', var, '_30yr_normal_800mM2_all_asc/') %>%
  list.files(pattern = '\\.asc$', full.names = TRUE) %>%
  .[1:12] %>% # the last file in the list is annual values, drop it
  purrr::map(raster) %>% # load the rasters from file list
  purrr::map(crop, bbox_wus) %>% # crop to study area
  brick # combine list of rasters into a brick
}
```

Use the above function to import precipitation and temperature range normals. Use the biovars function from dismo to calculate 19 bioclimatic predictor variables from monthly precipitation and temperature data.
```{r bioclim_pca, cache = TRUE}
prec <- get_normals('ppt')
tmin <- get_normals('tmin')
tmax <- get_normals('tmax')

# calculate bioclim variables from the monthly normals
bio_clim <- biovars(prec, tmin, tmax)
```

Reduce the dimensionality of the bioclimatic data using an R-mode PCA.
```{r cache = TRUE}
clim_pca <- RStoolbox::rasterPCA(bio_clim, spca = TRUE)
```

Retain the leading 4 PCs.
```{r}
screeplot(clim_pca$model, npcs = 20, type = 'lines')
bio_clim %>%
  as.data.frame(na.rm = TRUE) %>%
  eofNum(scale. = TRUE)
```

```{r reof_raster}
reof_raster <- as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>%
  select(x:y) %>%
  cbind(obs_eof$REOF) %>%
  rasterFromXYZ %>%
  `crs<-`('+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0') %>%
  `names<-`(c('reof1', 'reof2', 'reof3', 'reof4')) %>%
  projectRaster(bio_clim)
eof_raster <- as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>%
  select(x:y) %>%
  cbind(eof_obs) %>%
  rasterFromXYZ %>%
  `crs<-`('+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0') %>%
  `names<-`(c('eof1', 'eof2', 'eof3', 'eof4')) %>%
  projectRaster(bio_clim)
```

```{r raster_reprojection}
env_rasters <- clim_pca$map %>%
  .[[1:4]] %>%
  `names<-`(c('pc1', 'pc2', 'pc3', 'pc4')) %>%
  c(eof_raster) %>%
  c(reof_raster) %>%
  brick
```

```{r}
plot(env_rasters)
```

```{r eval = FALSE}
pairs(env_rasters)
```




# Model fitting

Now that we have all the data imported and preprocessed, let's put it all together into a data frame.
```{r data_preparation}
dat <- swsn %N>%
  mutate(ID = 1:n(),
         lon = pts$x, 
         lat = pts$y) %>%
  left_join(raster::extract(env_rasters, pts, df = TRUE), by = 'ID') %E>%
  left_join((dist_mat %>% as_tbl_graph(directed = T) %E>% filter(!edge_is_loop()) %>% as_tibble %>% rename(distance = weight)), by = c('from', 'to')) %>%
  mutate(
    size = case_when(time == 1200 ~ .N()$P1room[from] * .N()$P1room[to],
                     time == 1250 ~ .N()$P2room[from] * .N()$P2room[to],
                     time == 1300 ~ .N()$P3room[from] * .N()$P3room[to],
                     time == 1350 ~ .N()$P4room[from] * .N()$P4room[to],
                     time == 1400 ~ .N()$P5room[from] * .N()$P5room[to]),
   from_x = .N()$lon[from],
   from_y = .N()$lat[from],
   to_x = .N()$lon[to],
   to_y = .N()$lat[to],
   macro_same = .N()$Macro[from] == .N()$Macro[to],
   micro_same = .N()$Micro[from] == .N()$Micro[to],
   pc1 = abs(.N()$pc1[from] - .N()$pc1[to]),
   pc2 = abs(.N()$pc2[from] - .N()$pc2[to]),
   pc3 = abs(.N()$pc3[from] - .N()$pc3[to]),
   pc4 = abs(.N()$pc4[from] - .N()$pc4[to]),
   eof1 = abs(.N()$eof1[from] - .N()$eof1[to]),
   eof2 = abs(.N()$eof2[from] - .N()$eof2[to]),
   eof3 = abs(.N()$eof3[from] - .N()$eof3[to]),
   eof4 = abs(.N()$eof4[from] - .N()$eof4[to]),
   reof1 = abs(.N()$reof1[from] - .N()$reof1[to]),
   reof2 = abs(.N()$reof2[from] - .N()$reof2[to]),
   reof3 = abs(.N()$reof3[from] - .N()$reof3[to]),
   reof4 = abs(.N()$reof4[from] - .N()$reof4[to])) %>%
  as_tibble %>%
  group_by(time) %>%
  filter(from < to) %>%
  ungroup %>%
  mutate(from = as.factor(from), to = as.factor(to), time = as.factor(time)) %>%
  mutate(tie = similarity > 0)
```

```{r}
saveRDS(dat, 'dat')
dat <- readRDS('dat')
```

```{r, echo = FALSE}
knitr::kable(
  dat[1:6,], caption = 'A subset of the edge level data.'
)
```


The distribution for the final time step is very different from the others.
```{r}
library(ggridges)
ggplot(dat, aes(x = similarity, y = as.factor(time), fill = ..x.., height = ..density..)) +
  geom_density_ridges_gradient(stat = 'density', scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')
ggplot(filter(dat, similarity > 0 & similarity < 1), aes(x = similarity, y = as.factor(time), fill = ..x.., height = ..density..)) +
  geom_density_ridges_gradient(stat = 'density', scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')

ggplot(dat, aes(x = similarity, y = as.factor(time), fill = ..x..)) +
  geom_density_ridges_gradient(scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')
ggplot(filter(dat, similarity > 0 & similarity < 1), aes(x = similarity, y = as.factor(time), fill = ..x..)) +
  geom_density_ridges_gradient(scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')
```


Let's just isolate the distributions from the first time period. Clearly these are non gaussian, so modeling them as such would be inappropriate. log and sqrt transforms seem similarly inappropriate.

Since we'll be using a logit link in the regressions, check the logit transform

```{r}
library(ggridges)
ggplot(dat %>%
  mutate(similarity = if_else(similarity == 1, 1 - .000001, if_else(similarity == 0, .000001, similarity))), aes(x = qlogis(similarity), y = as.factor(time), fill = ..x.., height = ..density..)) +
  geom_density_ridges_gradient(stat = 'density', scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')
ggplot(filter(dat, similarity > 0 & similarity < 1), aes(x = qlogis(similarity), y = as.factor(time), fill = ..x.., height = ..density..)) +
  geom_density_ridges_gradient(stat = 'density', scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')
```


By adding a rug plot, we can see that this is likely because the small number of edges sampled overall in the final period. Possibly this has additional meaning due to something like the salado phenomenon, by which ceramic assemblage similarities take on a different meaning?

We can confirm this by looking at the number of edges from each time period.
```{r}
dat %>%
  group_by(time) %>%
  count
```


```{r, eval = FALSE, echo = FALSE}
test <- recon_amplitudes %>%
  filter(between(period, 1200, 1400)) %>%
  group_by(eof, period) %>% 
  summarise(amplitude = mean(amplitude))
eof_data <- raster::extract(reof_raster, pts, df=TRUE) %>%
  gather(eof, value, 2:6) %>%
  full_join(test) %>%
  mutate(key = paste(eof, period, sep = '_'),
            value = value * amplitude) %>%
  select(ID, key, value) %>%
  spread(key, value, -1)
```

## Distance modeling

```{r}
hist(log(dat$distance))
hist(log(dat$size))
```

Do the fits to distance change with time? a little bit, but seems more to have to do with the decrease in sample size with time.
```{r echo=FALSE, distance_time_plot}
ggplot(dat, aes(distance, qlogis(similarity))) +
  geom_point(alpha = .1, aes(color = time)) +
    scale_color_brewer(palette = 'Spectral', direction = 1) +
  geom_smooth(aes(group = time, color = time)) +
  geom_smooth() +
  theme_minimal()
```
```{r}
knitr::knit_exit()
```


Our data have a lot of zeros in them. To do this, we'll actually fit two models: a logistic regression for the presence or absence of any material culture similarity between two sites, and a beta regression for the intensity of the interaction when it does exist. 

First we predict the presence of a tie via logistic regression. Should we include size as a default predictor in the model? It doesn't increase the deviance explaned at all, but AIC says yes

Now we write out a list of potential model structures, and fit gams using all of them. Calculate the AICs for each formula, link function combination, and reorder the amount from the lowest AIC
```{r}
ctrl <- gam.control(nthreads = 2)

fit_gam_presence <- function(x){
  gam(x, method = 'ML', 
         family = binomial(link = 'logit'),
         control = ctrl,
         data = filter(dat))
}
```

```{r logistic_models, cache = TRUE}
logistic_models <- c(
  "s(distance, bs = 'tp', m = 2) +
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) + 
  time + macro_same",
  "s(distance, bs = 'tp', m = 2) + 
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) +
  s(pc1, bs = 'ts', m = 2) + s(pc1, by = time, bs = 'ts', m = 1) + 
  s(pc2, bs = 'ts', m = 2) + s(pc2, by = time, bs = 'ts', m = 1) +
  s(pc3, bs = 'ts', m = 2) + s(pc3, by = time, bs = 'ts', m = 1) + 
  s(pc4, bs = 'ts', m = 2) + s(pc4, by = time, bs = 'ts', m = 1) +
  time + macro_same",
  "s(distance, bs = 'tp', m = 2) + 
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) +
  s(reof1, bs = 'ts', m = 2) + s(reof1, by = time, bs = 'ts', m = 1) + 
  s(reof2, bs = 'ts', m = 2) + s(reof2, by = time, bs = 'ts', m = 1) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, by = time, bs = 'ts', m = 1) + 
  s(reof4, bs = 'ts', m = 2) + s(reof4, by = time, bs = 'ts', m = 1) + 
  time + macro_same"
   ) %>%
  paste('tie ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam_presence)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```

```{r}
logistic_models
```

We fit 3 models to predict the prsence of an interaction tie between two sites. The best performing model including distance, size, and reofs as predictors, with an aic of `r logistic_models[[1,4]]`. In second is the model with distance, size, and reofs as predictors, with an aic of `r logistic_models[[2,4]]`. Third is the base geographic model with distance and size, with an AIC of `r logistic_models[[3,4]]`. That said, even the best fitting model only captures 30% of the variance in tie presence. This means we are still missing key variables that drive the presence or absence of social relations.

```{r}
plot(logistic_models[[1,3]], scale= 0)
summary(logistic_models[[1,3]])
gam.check(logistic_models[[1,3]])
```

Next model the interaction intensities.

Define a function that, when applied to a dataset and a gam formula, will fit a gam of that formula.
```{r fit_gam}
fit_gam_similarity <- function(x){
  gam(x, method = 'ML', 
         family = betar(link = 'logit'),
         control = ctrl,
         eps = .001,
         data = filter(dat, similarity > 0))
}
```

```{r}
beta_models <- c(
  "s(distance, bs = 'tp', m = 2) +
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) + 
  time + macro_same",
  "s(distance, bs = 'tp', m = 2) + 
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) +
  s(pc1, bs = 'ts', m = 2) + s(pc1, by = time, bs = 'ts', m = 1) + 
  s(pc2, bs = 'ts', m = 2) + s(pc2, by = time, bs = 'ts', m = 1) +
  s(pc3, bs = 'ts', m = 2) + s(pc3, by = time, bs = 'ts', m = 1) + 
  s(pc4, bs = 'ts', m = 2) + s(pc4, by = time, bs = 'ts', m = 1) +
  time + macro_same",
  "s(distance, bs = 'tp', m = 2) + 
  s(distance, by = time, bs = 'ts', m = 1) + 
  s(log(size), bs = 'tp', m = 2) + 
  s(log(size), by = time, bs = 'ts', m = 1) +
  s(reof1, bs = 'ts', m = 2) + s(reof1, by = time, bs = 'ts', m = 1) + 
  s(reof2, bs = 'ts', m = 2) + s(reof2, by = time, bs = 'ts', m = 1) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, by = time, bs = 'ts', m = 1) + 
  s(reof4, bs = 'ts', m = 2) + s(reof4, by = time, bs = 'ts', m = 1) + 
  time + macro_same"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam_presence)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```


Now we write out a list of potential model structures, and fit gams using all of them. Calculate the AICs for each formula, link function combination, and reorder the amount from the lowest AIC
```{r geo_models}
geo_models <- c(
    "s(distance, bs = 'tp') + s(time, bs = 're', k = 5)",
    "s(distance, m = 2) + s(distance, time, bs = 'fs', m = 2)",
    "s(distance, time, bs = 'fs', m = 2)",
    "s(distance, by = time, bs = 'tp', m = 2) + s(time, bs = 're', k = 5)",
    "s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + s(time, bs = 're', k = 5)"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam_similarity)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```

Best fitting size model has log of distance and size, second best has log of sie but not distance
```{r}
size_models <-  c(
      "s(distance, by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5)",
    "s(distance, by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5) + s(size, by = time, bs = 'ts', m = 2)",
    "s(distance, by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5) + s(log(size), by = time, bs = 'ts', m = 2)",
      "s(log(distance), by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5)",
    "s(log(distance), by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5) + s(size, by = time, bs = 'ts', m = 2)",
    "s(log(distance), by = time, bs = 'ts', m = 2) + s(time, bs = 're', k = 5) + s(log(size), by = time, bs = 'ts', m = 2)"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)

size_models
summary(size_models[[1,3]])
plot(size_models[[1,3]])
```

Here we can see that the best model includes distance, centrality, and size in a purely additive structure with a logit link function. This model gives an adjusted R2 of ~70%.
```{r}
geo_models
```

Let's visualize the model fits.
```{r geo_models_output}
gam.check(geo_models[[1,3]])
plot(geo_models[[1,3]])
summary(geo_models[[1,3]])
geo_models[[1,3]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)
```


```{r}
#devtools::install_github('nspope/corMLPE')
library(corMLPE)

test <-  dat %>%
  filter(similarity > 0 & similarity < 1) %>%
  filter(time == 1300) %>%
  gamm(qlogis(similarity) ~ s(distance, bs = 'cr') + 
         s(log(size), bs = 'cr'),
  # s(reof1, by = time, bs = 'ts', m = 2) + 
   #s(reof2, by = time, bs = 'ts', m = 2) +
   #s(reof3, by = time, bs = 'ts', m = 2) + 
  #s(reof4, by = time, bs = 'ts', m = 2) + 
  #s(time, bs = 're', k = 5), 
         correlation = corMLPE(form = ~from + to),
         data = .)
test
plot(test$gam)
summary(qlogis(dat$similarity))
plot(test$gam)
summary(test$gam)

test <- lme(qlogis(similarity) ~ distance, random = ~distance|time, correlation = corMLPE(form = ~from + to | time), data = dat %>%
  mutate(similarity = if_else(similarity == 1, 1 - .0001, if_else(similarity == 0, .0001, similarity)), from = as.character(from), to = 
    as.character(to)))
test
hist(plogis(filter(dat,similarity > 0 & similarity < 1)$similarity))
cbind(filter(dat,similarity > 0 & similarity < 1)$similarity, plogis(qlogis(filter(dat,similarity > 0 & similarity < 1)$similarity)))
plot(test)
```


So let's select this model as our baseline geographic model.

```{r}
expand.grid(distance = seq(0, 100000, by = 100), 
            time = as.factor(c(1200, 1250, 1300, 1350, 1400)), 
            macro_same = c(T, F), micro_same = c(T,F),
            size = 500,
            pc1 = 0, pc2 = 0, pc3 = 0, pc4 = 0, reof1 = 0, reof2 = 0, reof3 = 0, reof4 = 0) %>%
  mutate(., pred = predict(test$gam, ., type = 'response')) %>%
  ggplot(aes(distance, plogis(pred), group = time, color = time)) +
  facet_grid(macro_same ~ micro_same) +
  geom_line()

expand.grid(distance = 100000, 
            time = as.factor(c(1200, 1250, 1300, 1350, 1400)), 
            macro_same = F, micro_same = F,
            size = 500,
            pc1 = seq(0,1,.01), pc2 = 0, pc3 = 0, pc4 = 0, reof1 = 0, reof2 = 0, reof3 = 0, reof4 = 0) %>%
  mutate(., pred = predict(mod, ., type = 'response')) %>%
  ggplot(aes(pc1, pred, group = time, color = time)) +
  facet_grid(macro_same ~ micro_same) +
  geom_line()

expand.grid(distance = 100000, 
            time = as.factor(c(1200, 1250, 1300, 1350, 1400)), 
            macro_same = F, micro_same = F,
            size = 1000,
            pc1 = 0, pc2 = 0, pc3 = 0, pc4 = 0, reof1 = 0, reof2 = 0, reof3 = 0, reof4 = seq(0,.5,.01)) %>%
  mutate(., pred = predict(all_models[[1,3]], ., type = 'response')) %>%
  ggplot(aes(reof4, pred, group = time, color = time)) +
  facet_grid(macro_same ~ micro_same) +
  geom_line()
```

```{r}
mod <- bam(similarity ~ 
  s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + 
  s(size, bs = 'ts', m = 2) + s(size, by = time, bs = 'ts', m = 1) + 
  s(pc1, bs = 'ts', m = 2) + s(pc1, by = time, bs = 'ts', m = 1) + 
  s(pc2, bs = 'ts', m = 2) + s(pc2, by = time, bs = 'ts', m = 1) +
  s(pc3, bs = 'ts', m = 2) + s(pc3, by = time, bs = 'ts', m = 1) + 
  s(pc4, bs = 'ts', m = 2) + s(pc4, by = time, bs = 'ts', m = 1) +
  s(reof1, bs = 'ts', m = 2) + s(reof1, by = time, bs = 'ts', m = 1) + 
  s(reof2, bs = 'ts', m = 2) + s(reof2, by = time, bs = 'ts', m = 1) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, by = time, bs = 'ts', m = 1) + 
  s(reof4, bs = 'ts', m = 2) + s(reof4, by = time, bs = 'ts', m = 1) +
  micro_same + macro_same + s(time, bs = 're', k = 5),
  family = betar(eps = 0.0001),
  nthreads = 3,
  discrete = TRUE,
  data = dat)

summary(mod)
plot(mod)
```
```{r}
concurvity(geo_models[[1,3]])
concurvity(mod)
```

```{r}
mod3 <- bam(similarity ~ 
   s(distance, time, bs = 'fs', m = 2) + 
   s(log(size), time, bs = 'fs', m = 2) + 
   s(reof1, time, bs = 'fs', m = 2) + 
   s(reof2, time, bs = 'fs', m = 2) +
   s(reof3, time, bs = 'fs', m = 2) + 
  s(reof4, time, bs = 'fs', m = 2),
  family = betar(eps = 0.0001),
  nthreads = 3,
  discrete = TRUE,
  data = dat)

summary(mod3)
plot(mod3)
```


```{r}
mod2 <- bam(similarity ~ 
   s(distance, by = time, bs = 'cr') + 
   s(log(size), by = time, bs = 'cr') + 
   s(reof1, by = time, bs = 'cr') + 
   s(reof2, by = time, bs = 'cr') +
   s(reof3, by = time, bs = 'cr') + 
  s(reof4, by = time, bs = 'cr') +
    s(time, bs = 're', k = 5),
  family = betar(eps = 0.0001),
  select = TRUE,
  discrete = TRUE,
  nthreads = 3,
  data = dat)
dat
summary(mod2)
plot(mod2, residuals = TRUE)
gam.check(mod2)
```
```{r}
all_models <- c(
    "s(distance, bs = 'ts') + s(time, bs = 're', k = 5)",
    "s(distance, bs = 'ts', m = 2) + s(distance, time, bs = 'fs', m = 2)",
    "s(distance, time, bs = 'fs', m = 2)",
    "s(distance, by = time, bs = 'tp', m = 2) + s(time, bs = 're', k = 5)",
    "s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + s(time, bs = 're', k = 5)",
    "s(distance, by = time, bs = 'ts', m = 2) + 
   s(pc1, by = time, bs = 'ts', m = 2) + 
   s(pc2, by = time, bs = 'ts', m = 2) +
   s(pc3, by = time, bs = 'ts', m = 2) + 
  s(pc4, by = time, bs = 'ts', m = 2) +
  s(time, bs = 're', k = 5)",
 "s(distance, time, bs = 'fs', m = 2) + 
   s(pc1, time, bs = 'fs', m = 2) + 
   s(pc2, time, bs = 'fs', m = 2) +
   s(pc3, time, bs = 'fs', m = 2) + 
  s(pc4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + 
  s(pc1, bs = 'ts', m = 2) + s(pc1, by = time, bs = 'ts', m = 1) + 
  s(pc2, bs = 'ts', m = 2) + s(pc2, by = time, bs = 'ts', m = 1) +
  s(pc3, bs = 'ts', m = 2) + s(pc3, by = time, bs = 'ts', m = 1) + 
  s(pc4, bs = 'ts', m = 2) + s(pc4, by = time, bs = 'ts', m = 1) +
 s(time, bs = 're', k = 5)",
  "s(distance, bs = 'ts', m = 2) + s(distance, time, bs = 'fs', m = 2) + 
  s(pc1, bs = 'ts', m = 2) +   s(pc1, time, bs = 'fs', m = 2) + 
  s(pc2, bs = 'ts', m = 2) +  s(pc2, time, bs = 'fs', m = 2) +
  s(pc3, bs = 'ts', m = 2) +     s(pc3, time, bs = 'fs', m = 2) +
  s(pc4, bs = 'ts', m = 2) +  s(pc4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'ts') + 
  s(pc1, bs = 'ts') + 
  s(pc2, bs = 'ts') + 
  s(pc3, bs = 'ts') + 
  s(pc4, bs = 'ts') + 
  s(time, bs = 're', k = 5)",
     "s(distance, by = time, bs = 'ts', m = 2) + 
   s(reof1, by = time, bs = 'ts', m = 2) + 
   s(reof2, by = time, bs = 'ts', m = 2) +
   s(reof3, by = time, bs = 'ts', m = 2) + 
  s(reof4, by = time, bs = 'ts', m = 2) + 
  s(time, bs = 're', k = 5)",
 "s(distance, time, bs = 'fs', m = 2) + 
   s(reof1, time, bs = 'fs', m = 2) + 
   s(reof2, time, bs = 'fs', m = 2) +
   s(reof3, time, bs = 'fs', m = 2) + 
  s(reof4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + 
  s(reof1, bs = 'ts', m = 2) + s(reof1, by = time, bs = 'ts', m = 1) + 
  s(reof2, bs = 'ts', m = 2) + s(reof2, by = time, bs = 'ts', m = 1) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, by = time, bs = 'ts', m = 1) + 
  s(reof4, bs = 'ts', m = 2) + s(reof4, by = time, bs = 'ts', m = 1) + 
 s(time, bs = 're', k = 5)",
  "s(distance, bs = 'ts', m = 2) + s(distance, time, bs = 'fs', m = 2) + 
  s(reof1, bs = 'ts', m = 2) +  s(reof1, time, bs = 'fs', m = 2) + 
  s(reof2, bs = 'ts', m = 2) +  s(reof2, time, bs = 'fs', m = 2) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, time, bs = 'fs', m = 2) + 
  s(reof4, bs = 'ts', m = 2) +  s(reof4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'ts') + 
  s(reof1, bs = 'ts') + 
  s(reof2, bs = 'ts') + 
  s(reof3, bs = 'ts') + 
  s(reof4, bs = 'ts') +
  s(time, bs = 're', k = 5)"
   ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```
```{r}
saveRDS(all_models, 'all_models')
```

```{r}
all_models <- readRDS('all_models')
all_models
```
```{r}
summary(all_models[[1,3]]);summary(all_models[[6,3]])
plot(all_models[[2,3]], scale = -1)
```


```{r}
    "s(distance, by = time, bs = 'ts', m = 2) + 
   s(pc1, by = time, bs = 'ts', m = 2) + 
   s(pc2, by = time, bs = 'ts', m = 2) +
   s(pc3, by = time, bs = 'ts', m = 2) + 
  s(pc4, by = time, bs = 'ts', m = 2) +
   s(reof1, by = time, bs = 'ts', m = 2) + 
   s(reof2, by = time, bs = 'ts', m = 2) +
   s(reof3, by = time, bs = 'ts', m = 2) + 
  s(reof4, by = time, bs = 'ts', m = 2) + 
  s(time, bs = 're', k = 5)",
 "s(distance, time, bs = 'fs', m = 2) + 
   s(pc1, time, bs = 'fs', m = 2) + 
   s(pc2, time, bs = 'fs', m = 2) +
   s(pc3, time, bs = 'fs', m = 2) + 
  s(pc4, time, bs = 'fs', m = 2) +
   s(reof1, time, bs = 'fs', m = 2) + 
   s(reof2, time, bs = 'fs', m = 2) +
   s(reof3, time, bs = 'fs', m = 2) + 
  s(reof4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'tp', m = 2) + s(distance, by = time, bs = 'ts', m = 1) + 
  s(pc1, bs = 'ts', m = 2) + s(pc1, by = time, bs = 'ts', m = 1) + 
  s(pc2, bs = 'ts', m = 2) + s(pc2, by = time, bs = 'ts', m = 1) +
  s(pc3, bs = 'ts', m = 2) + s(pc3, by = time, bs = 'ts', m = 1) + 
  s(pc4, bs = 'ts', m = 2) + s(pc4, by = time, bs = 'ts', m = 1) +
  s(reof1, bs = 'ts', m = 2) + s(reof1, by = time, bs = 'ts', m = 1) + 
  s(reof2, bs = 'ts', m = 2) + s(reof2, by = time, bs = 'ts', m = 1) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, by = time, bs = 'ts', m = 1) + 
  s(reof4, bs = 'ts', m = 2) + s(reof4, by = time, bs = 'ts', m = 1) + 
 s(time, bs = 're', k = 5)",
  "s(distance, bs = 'ts', m = 2) + s(distance, time, bs = 'fs', m = 2) + 
  s(pc1, bs = 'ts', m = 2) +   s(pc1, time, bs = 'fs', m = 2) + 
  s(pc2, bs = 'ts', m = 2) +  s(pc2, time, bs = 'fs', m = 2) +
  s(pc3, bs = 'ts', m = 2) +     s(pc3, time, bs = 'fs', m = 2) +
  s(pc4, bs = 'ts', m = 2) +  s(pc4, time, bs = 'fs', m = 2) +
  s(reof1, bs = 'ts', m = 2) +  s(reof1, time, bs = 'fs', m = 2) + 
  s(reof2, bs = 'ts', m = 2) +  s(reof2, time, bs = 'fs', m = 2) +
  s(reof3, bs = 'ts', m = 2) + s(reof3, time, bs = 'fs', m = 2) + 
  s(reof4, bs = 'ts', m = 2) +  s(reof4, time, bs = 'fs', m = 2)",
  "s(distance, bs = 'ts') + 
  s(pc1, bs = 'ts') + 
  s(pc2, bs = 'ts') + 
  s(pc3, bs = 'ts') + 
  s(pc4, bs = 'ts') + 
  s(reof1, bs = 'ts') + 
  s(reof2, bs = 'ts') + 
  s(reof3, bs = 'ts') + 
  s(reof4, bs = 'ts') +
  s(time, bs = 're', k = 5)"
```

```{r}
ggplot(dat, aes(abs(reof1_from - reof1_to), similarity)) +
  geom_point(alpha = .1, aes(color = time)) +
    scale_color_brewer(palette = 'Spectral', direction = 1) +
  geom_smooth(aes(group = time, color = time)) +
  geom_smooth() +
  theme_minimal()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(abs(reof4_from - reof4_to), resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_brewer(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(abs(size_from - size_to), resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_brewer(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(abs(pc4_from - pc4_to), resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_brewer(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}

mod1 <- gam(similarity ~ s(distance, bs = 'cr') + 
            s(size_from, bs = 'cr') + 
            s(size_to, bs = 'cr') + 
            s(centrality_from, bs = 'cr') + 
            s(centrality_to, bs = 'cr'), 
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod15 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(size_from, bs = 'cr') + 
            s(size_to, bs = 'cr') + 
            s(centrality_from, bs = 'cr') + 
            s(centrality_to, bs = 'cr'), 
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod2 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(centrality_from, bs = 'cr') +
            s(centrality_to, bs = 'cr') +
            s(pc1_from, bs = 'cr') + 
            s(pc1_to, bs = 'cr') + 
            s(pc2_from, bs = 'cr') +
            s(pc2_to, bs = 'cr') + 
            s(pc3_from, bs = 'cr') +
            s(pc3_to, bs = 'cr') + 
            s(pc4_from, bs = 'cr') +
            s(pc4_to, bs = 'cr') + 
            s(reof1_from, bs = 'cr') + 
            s(reof1_to, bs = 'cr') + 
            s(reof2_from, bs = 'cr') +
            s(reof2_to, bs = 'cr') + 
            s(reof3_from, bs = 'cr') +
            s(reof3_to, bs = 'cr') + 
            s(reof4_from, bs = 'cr') +
            s(reof4_to, bs = 'cr'),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod3 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(centrality_from, bs = 'cr') +
            s(centrality_to, bs = 'cr') +
            s(pc1_from, bs = 'cr') + 
            s(pc1_to, bs = 'cr') + 
            s(pc2_from, bs = 'cr') +
            s(pc2_to, bs = 'cr') + 
            s(pc3_from, bs = 'cr') +
            s(pc3_to, bs = 'cr') + 
            s(pc4_from, bs = 'cr') +
            s(pc4_to, bs = 'cr'),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod4 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(centrality_from, bs = 'cr') +
            s(centrality_to, bs = 'cr') +
            s(reof1_from, bs = 'cr') + 
            s(reof1_to, bs = 'cr') + 
            s(reof2_from, bs = 'cr') +
            s(reof2_to, bs = 'cr') + 
            s(reof3_from, bs = 'cr') +
            s(reof3_to, bs = 'cr') + 
            s(reof4_from, bs = 'cr') +
            s(reof4_to, bs = 'cr'),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod5 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(centrality_from, centrality_to) +
            s(reof1_from, reof1_to) + 
            s(reof2_from, reof2_to) + 
            s(reof3_from, reof3_to) + 
            s(reof4_from, reof4_to),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod6 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(centrality_from, centrality_to) +
            s(pca1_from, pca1_to) + 
            s(pca2_from, pca2_to) + 
            s(pca3_from, pca3_to) + 
            s(pca4_from, pca4_to),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

plot(mod2)
summary(mod2)


summary(mod3);summary(mod4)
```


## Cultural costs
Higher than average similarity within macroregions, but not significantly when you control for distance. Looks like there are a lot of outliers withhigh simliarity between macroregion that we're not yet capturing. the difference shrinks when we control for difference, because the "same macrogregion" will covary with distance

```{r}
ggplot(dat, aes(macro_same, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(macro_same, distance)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Distance') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(macro_same, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
ggplot(dat, aes(macro_from, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(macro_to, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()
```

```{r}
dat %>%
  na.omit %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
ggplot(aes(macro_from, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
ggplot(dat, aes(micro_same, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share microregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(micro_same, distance)) +
  geom_boxplot() +
  labs(title = '', x = 'Share microregion?', y = 'Distance') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(micro_same, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
ggplot(dat, aes(micro_from, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(micro_to, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()


dat %>%
  na.omit %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
ggplot(aes(micro_to, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
geo_models
```

## Average climmate models
Now we can look weather average climate conditions have any predictive power for link strength.
```{r clim_mods}
clim_models <- c(
      "s(distance, bs = 'cr') + s(size_from, size_to) +
          s(reof1_from, reof1_to) + s(reof2_from, reof2_to) + 
          s(reof3_from, reof3_to) + s(reof4_from, reof4_to) + s(to, bs = 're')",
      "s(distance, bs = 'cr') + s(size_from, size_to) + 
          s(pc1_from, pc1_to) + s(pc2_from, pc2_to) + 
          s(pc3_from, pc3_to) + s(pc4_from, pc4_to) + s(to, bs = 're')"
      ) %>%
  paste('rel_sim ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```
```{r}
clim_models
```

```{r}
dat %>%
  na.omit %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
ggplot(aes(macro_from == macro_to, resid)) +
  geom_point(alpha = .1) +
  geom_smooth() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```
So think about the models to fit. micro same, macro same, and micro and macro same. repeat for micro/macro from/to pairs. Nest all that in same not same. then compare with no distance, distance, and distance + centrality

```{r}
clim_models
```

Let's visualize the model fits.
```{r clim_models_output}
clim_models[[1,3]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)

clim_models[[1,3]] %>% plot(scheme = 1)

gam.check(clim_models[[2,3]])
```
```{r}
clim_models2 <- c(
        "s(distance, bs = 'cr')",
      #"s(distance, bs = 'cr') + s(size_from, bs = 'cr', id = 1) + s(size_to, bs = 'cr', id = 1)",
       #     "s(distance, bs = 'cr') + s((size_from * size_to), bs = 'cr')",
      #"s(distance, bs = 'cr') + s((size_from + size_to), bs = 'cr')",
      #"s(distance, bs = 'cr') + s(abs(size_from - size_to), bs = 'cr')",
      "s(distance, bs = 'cr') + s(abs(eof1_from - eof1_to), bs = 'cr') + s(abs(eof2_from - eof2_to), bs = 'cr') + s(abs(eof3_from - eof3_to), bs = 'cr') + s(abs(eof4_from - eof4_to), bs = 'cr')",
      "s(distance, bs = 'cr') + s(abs(reof1_from - reof1_to), bs = 'cr') + s(abs(reof2_from - reof2_to), bs = 'cr') + s(abs(reof3_from - reof3_to), bs = 'cr') + s(abs(reof4_from - reof4_to), bs = 'cr')",
      "s(distance, bs = 'cr') + s(abs(pc1_from - pc1_to), bs = 'cr') + s(abs(pc2_from - pc2_to), bs = 'cr') + s(abs(pc3_from - pc3_to), bs = 'cr') + s(abs(pc4_from - pc4_to), bs = 'cr')"
           #"s(distance, bs = 'cr') + s(abs(reof1_from - reof1_to), bs = 'cr') + s(abs(reof2_from - reof2_to), bs = 'cr') + s(abs(reof3_from - reof3_to), bs = 'cr') + s(abs(reof4_from - reof4_to), bs = 'cr') + s(abs(pc1_from - pc1_to), bs = 'cr') + s(abs(pc2_from - pc2_to), bs = 'cr') + s(abs(pc3_from - pc3_to), bs = 'cr') + s(abs(pc4_from - pc4_to), bs = 'cr')"
      ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```

```{r}
clim_models2
```

Let's visualize the model fits.
```{r clim_models_output2}
clim_models2[[2,3]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)

clim_models2[[1,3]] %>% plot(scheme = 1, residuals = T)
clim_models2[[2,3]] %>% plot(scheme = 1, residuals = T)

summary(clim_models2[[1,3]])
gam.check(clim_models2[[1,3]])
```


Interesting. Looks like there is increased similarity between locations with similar precipitation patterns or different temperature patterns. I should:
1. compare temperature and elevation regressions alone to see which is better
2. use unrotated EOFs, maybe winter ones too?
3. Use PCA of bioclim variables

## Variability models
```{r include = FALSE, eval = F}
  #  "ti(distance) + ti(size) + ti(distance, size)",
  #  "te(distance, size, k = 10)"
 #"s(distance, bs = 'cr') + s(abs(eof1), bs = 'cr', k = 15) + s(abs(eof2), bs = 'cr') + s(abs(eof3), bs = 'cr', k = 15) + s(abs(eof4), bs = 'cr', k = 15)",
#    "s(distance, bs = 'cr') + s(sqrt(abs(eof1)), bs = 'cr', k = 15) + s(abs(eof2), bs = 'cr') + s(abs(eof3), bs = 'cr', k = 15) + s(abs(eof4), bs = 'cr', #k = 15)"
```

```{r eval = FALSE}
mod5 <- gam(fractional_similarity ~ s(distance, bs = 'cr') +
              s(eof1, bs = 'cr') +
              s(eof2, bs = 'cr') +
              s(eof3, bs = 'cr') +
              s(eof4, bs = 'cr'),
            select = TRUE,
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)

mod6 <- gam(similarity ~ s(distance, bs = 'cr') +
              s(abs(eof1), bs = 'cr') +
              s(abs(eof2), bs = 'cr') +
              s(abs(eof3), bs = 'cr') +
              s(abs(eof4), bs = 'cr'),
            select = TRUE,
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)

summary(mod1)
plot(mod1)
gam.check(mod1)
```


```{r eval = FALSE}
swsn %E>%
  filter(centrality_from >= centrality_to) %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
  filter(resid > .25 | resid < -.25) %>%
  mutate(fac = if_else(resid > 0, 'pos', 'neg')) %>%
  ggraph('manual', node.positions = pts) +
  geom_edge_link(aes(alpha = abs(resid), color = resid)) +
  facet_graph(time ~ fac, col_type = 'edge') +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  scale_edge_colour_distiller(palette = 'RdBu', limits = c(-1,1)) +
  coord_equal() +
  theme_graph()
```
```{r eval = FALSE}
swsn %N>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[6,2]], type = 'response')) %>%
  filter(resid > .25 | resid < -.25) %>%
  mutate(fac = if_else(resid > 0, 'pos', 'neg')) %>%
  ggraph('manual', node.positions = pts) +
  geom_edge_link(aes(alpha = abs(resid), color = resid)) +
  facet_graph(time ~ fac, col_type = 'edge') +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  scale_edge_colour_distiller(palette = 'RdBu', limits = c(-1,1)) +
  coord_equal() +
  theme_graph()
```
Looks like distance is correlated with residuals
```{r eval = FALSE}
dat %>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  as_tibble %>%
  qplot(distance, resid, data = ., geom = 'point', alpha = I(.1), color = as.factor(time))
```
```{r eval = FALSE}
dat %>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  as_tibble %>%
  qplot(abs(eof1), resid, data = ., geom = 'point', alpha = I(.1), color = as.factor(time))
```
```{r eval = FALSE}
swsn %E>%
    filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  filter(time == 1) %N>%
  mutate(centrality = centrality_eigen(weights = similarity)) %E>%
  mutate(cent = .N()$centrality[from] * .N()$centrality[to]) %>%
  as_tibble %>%
  qplot(cent, resid, data = ., geom = 'point', alpha = I(.1))
```

```{r eval = FALSE}
swsn %E>%
    filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  filter(time == 3) %N>%
  mutate(centrality = centrality_eigen(weights = similarity)) %E>%
  mutate(cent = .N()$centrality[from] + .N()$centrality[to]) %>%
  as_tibble %>%
  qplot(cent, similarity, data = ., geom = 'point', alpha = I(.1))
```

```{r echo = FALSE, eval = FALSE}
phyda <- ncdf4::nc_open('data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc')
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,1])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,4])

plot(ncdf4::ncvar_get(phyda, 'Pac130_mn')[1100:1999], recon_eof$amplitude[,2])
plot(ncdf4::ncvar_get(phyda, 'Pac130_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_eof$amplitude[,4])
cor(ncdf4::ncvar_get(phyda, 'amo_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'amo_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_eof$amplitude[,1])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'EPac_mn')[1100:1999], recon_eof$amplitude[,4])
```

# References {#references .unnumbered}
