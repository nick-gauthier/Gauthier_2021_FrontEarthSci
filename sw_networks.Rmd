---
title: "Supplemental Information"
subtitle: 'Analysis scripts for "Drought Variability and the Robustness of Agrarian Social Networks"'
author: "Nicolas Gauthier"
date: "Last knit on: `r format(Sys.time(), '%d %B, %Y')`"
#bibliography: bibliography.bibtex
output:
  tufte::tufte_handout:
    citation_package: natbib
    #latex_engine: xelatex
    toc: true
    highlight: pygments
  tufte::tufte_html:
    tufte_features: ["fonts", "italics"]
    toc: true
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, message = FALSE, warning = FALSE)
```

# Introduction
\begin{fullwidth}
This RMarkdown document contains all analysis code used to produce the results and figures in the main text.

In this two-part analysis we first 
\end{fullwidth}

## Data sources
This R Markdown script requires raster data of [observed present-day temperature and precipitation](http://www.prism.oregonstate.edu/normals/)^[http://www.prism.oregonstate.edu/normals/] 30-year averages and 100-year time series of the [Standardized Precipitation-Evapotranspiration Index](https://wrcc.dri.edu/wwdt/time/)^[https://wrcc.dri.edu/wwdt/time/]. We'll also need reconstructed SPEI rasters^[https://zenodo.org/record/1198817]. Finally, get the CGIAR version of the SRMTM [digital elevation model](http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/)^[http://gisweb.ciat.cgiar.org/TRMM/SRTM_Resampled_250m/], resampled to 250m. With permission, also acquire the [Southwest Social Networks Database](http://www.southwestsocialnetworks.net/data.html)^[http://www.southwestsocialnetworks.net/data.html].

## Packages
Load the packages used in the analyses.
```{r packages}
library(raster) # raster data manipulation
library(tidyverse) # data analysis and plotting
library(rgdal) # reprojection of spatial points
library(gdistance) # least cost distance calculations
library(wql) # empirical orthogonal function calculation
library(tidygraph) # network analysis
library(ggraph) # network plotting
library(mgcv) # GAM fitting
library(mgcViz) # GAM plotting
library(dismo)
library(sf)
library(smoothr)
```

```{r include = FALSE}
theme_set(theme_bw())
```

# Archaeological network data
Now we move on to the archaeological social network proxies^[Go to http://www.southwestsocialnetworks.net for more information on this dataset and the project]. First we read in a csv file containing information for all of the archaeological sites in the database, including spatial location and number of rooms present from each time period. These will become the node-level data for the network.
```{r}
node_data <- read_csv('data/attributes_orig.csv') %>%
  select(-c(SWSN_ID,GKPM:Periods, N0S1))
```

```{r, echo = FALSE}
knitr::kable(
  node_data[1:6,1:8], caption = 'A subset of the node level data.'
)
```

Next import the edge data. Our data come in the form of similarity matrices, one for each time step. We need to define a function for importing and processing a single time step of the edge datasets. This function imports the csv, turns it into a matrix, and then a directed graph. We then filter out self loops, calculate eigenvector centrality for each of the nodes, then for each edge record the time period and centralities of the neighboring nodes.
```{r echo = FALSE, fig.margin = TRUE, fig.cap = 'Example 6x6 simililarity matrix. Note the symmetry'}
sim_tmp <- read.csv('data/Sim/AD1200sim.csv', row.names = 1, check.names = FALSE) %>%
  as.matrix %>%
  .[1:5,1:5]
knitr::kable(sim_tmp, caption = 'Example similarity matrix')
```

```{r read_swsn}
read_swsn <- function(net, time){
  read.csv(net, row.names = 1, check.names = FALSE) %>%
    as.matrix %>%
    replace(. == 0, 999) %>%
    as_tbl_graph(directed = TRUE) %E>%
    mutate(weight = if_else(weight == 999, 0, weight)) %>%
    rename(similarity = weight) %>%
    filter(!edge_is_loop()) %N>%
    mutate(centrality = centrality_eigen(weights = similarity)) %E>%
    mutate(time = 1150 + 50 * time) %N>%#,
           #centrality_from = .N()$centrality[from],
           #centrality_to = .N()$centrality[to]) %N>%
    select(-centrality)
}
```

Next we map this function over a lists of simlaritity matrices. Get a list of the similarity files, map the above function over them, reduce to a single graph.  Use reduce and graph join to combiine all of these networks into a single netwtork, retaining the edge weights for each  separate period.
```{r swsn_import}
swsn <- list.files('data/Sim', full.names = TRUE) %>%
  imap(read_swsn) %>%
  reduce(graph_join, by = 'name') %>%
  left_join(node_data, by = c('name' = 'SWSN_Site'))
```

Create a separate spatial points object, storing the locations of the sites. Convert the coordinates from utm to lat lon, and add back to the original data
```{r site_points}
pts <- swsn %N>% 
  select(x = EASTING, y = NORTHING) %>%
  as_tibble %>%
  SpatialPoints(proj4string=CRS("+proj=utm +zone=12 +datum=NAD27")) %>%
  spTransform(CRS("+proj=longlat +datum=WGS84")) %>% 
  coordinates %>%
  data.frame
```

# Least Cost Distances

Import the elevation dataset.

```{r elevation}
bbox <- extent(c(-113, -107, 31, 37.5))#extent(c(-113.5, -106.5, 31, 37.5))

elev <- raster('~/SRTM_NE_250m_TIF/SRTM_W_250m.tif') %>%
  crop(bbox) %>% 
  aggregate(fact  = 3)
```


The topography is very rugged.
```{r rayshader, echo = FALSE}
#devtools::install_github("tylermorganwall/rayshader")
library(rayshader)

elmat <-  matrix(raster::extract(elev,raster::extent(elev),buffer=1000),
               nrow=ncol(elev),ncol=nrow(elev))

elmat %>%
  sphere_shade(texture = "imhof1", zscale = 10) %>%
  plot_map()
```

```{r conductance, cache = TRUE, dependson='elevation'}
# Calculate the difference in elevation between each cell and its 16 neighbors.
altDiff <- function(x){abs(x[2] - x[1])}
hd <- transition(elev, altDiff, 16, symm = TRUE)

# Divide the height differences by the horizontal distances between cells, resulting in slopes.
slope_c <- geoCorrection(hd, type = 'c')

# Figure out which cells are adjacent to one another, queen's case.
adj <- adjacent(elev, cells = 1:ncell(elev), directions = 16)

# Use Tobler's hiking function to calculate walking speed from *cognitive* slope.
speed_c <- slope_c
speed_c[adj] <- 6 * exp(-3.5 * 2.15 * slope_c[adj])

# Again divide by intercell distance, resulting in our final conductance matrix.
conductance_c <- geoCorrection(speed_c, type = 'c')
```


```{r eval = FALSE}
slope_r <- geoCorrection(hd, type = 'r')
speed_r <- slope_r
speed_r[adj] <- 6 * exp(-3.5 * 2.15 * slope_r[adj])
conductance_r <- geoCorrection(speed_r, type = 'r')

pas <- passage(conductance_r, as.matrix(pts)[1:10,], as.matrix(pts)[200:210,], theta = .05)
commute_dist <- pts %>%
  as.matrix %>%
  rSPDistance(conductance_r, ., ., theta = 0.05) 
```

Calculate the full pairwise least cost distance matrix.
```{r distance_matrix, cache = TRUE, dependson=c('site_points','conductance')}
dist_mat <- pts %>%
  as.matrix %>%
  costDistance(conductance_c, .)
```

```{r}
time_mat <- dist_mat %>%
  as.matrix %>%
  replace(. == 0, 60) %>%
  as_tbl_graph(directed = FALSE) %E>%
  filter(!edge_is_loop()) %>%
  rename(distance = weight) %>%
  filter(distance <= 3600 * 6) %>%
  #mutate(max_dist = case_when(
  #    distance <= 3600 * 1 ~'1 Hours',
  #    distance <= 3600 * 2 ~'2 Hours',
  #    distance <= 3600 * 3 ~'3 Hours',
  #    distance <= 3600 * 4 ~'4 Hours',
  #    distance <= 3600 * 5 ~'5 Hours',
  #    distance <= 3600 * 6 ~'6 Hours',
  #    distance <= 3600 * 7 ~'7 Hours',
  #    distance <= 3600 * 8 ~'8 Hours')) %>%
  as_tibble %>%
  inner_join(activate(swsn, edges), ., by = c('from', 'to'))
```

```{r echo = FALSE}
ggraph(time_mat, 'manual', node.positions = pts) +
  geom_edge_link(alpha = .1) +
  theme_void() +
  facet_edges(~time, nrow = 1) +
  coord_equal()
```

```{r}
nn_mat <- dist_mat %>% # start with the gdistance output
  as.matrix %>% # convert to a matrix
  replace(. == 0, 60) %>% # make close sites close instead of overlapping
  as_tbl_graph(directed = TRUE) %E>% # turn into a graph
  filter(!edge_is_loop()) %>% # remove self loops
  rename(distance = weight) %>% # rename weight column
  as_tibble %>% # extract edges into a tibble
  left_join(activate(swsn, edges), ., copy = TRUE, by = c('from', 'to')) %>%
  morph(to_split, time, split_by = 'edges') %>%
  group_by(from) %>%
  top_n(-15, distance) %>% 
  crystallize %>%
  pull(graph) %>%
  map(as.undirected, mode = 'collapse', edge.attr.comb = 'first') %>%
  map(as_tbl_graph) %>% 
  reduce(graph_join)
```

```{r, echo = FALSE}
ggraph(nn_mat, 'manual', node.positions = pts) +
  geom_edge_link(alpha = .1) +
  facet_edges(~time, nrow = 1) +
  theme_void() +
  coord_equal()
```

```{r eval = FALSE}
shortest_path <- function(graph, src, dest){
  path <- suppressWarnings(get.shortest.paths(graph, src, dest))
  path <- names(path$vpath[[1]])
  if (length(path)==1) NULL else path
} 

#'@return the sum of the weights of all the edges in the given path
path_weight <- function(path, graph) sum(E(graph, path=path)$weight)

#'@description sorts a list of paths based on the weight of the path
sort_paths <- function(graph, paths) paths[paths %>% sapply(path_weight, graph) %>% order]

#'@description creates a list of edges that should be deleted
find_edges_to_delete <- function(A,i,rootPath){
  edgesToDelete <- NULL
  for (p in A){
    rootPath_p <- p[1:i]
    if (all(rootPath_p == rootPath)){
      edge <- paste(p[i], ifelse(is.na(p[i+1]),p[i],p[i+1]), sep = '|')
      edgesToDelete[length(edgesToDelete)+1] <- edge
    }
  }
  unique(edgesToDelete)
}

#returns the k shortest path from src to dest
#sometimes it will return less than k shortest paths. This occurs when the max possible number of paths are less than k
k_shortest_yen <- function(graph, src, dest, k){
  if (src == dest) stop('src and dest can not be the same (currently)')

  #accepted paths
  A <- list(shortest_path(graph, src, dest))
  if (k == 1) return (A)
  #potential paths
  B <- list()

  for (k_i in 2:k){
    prev_path <- A[[k_i-1]]
    num_nodes_to_loop <- length(prev_path)-1
    for(i in 1:num_nodes_to_loop){
      spurNode <- prev_path[i]
      rootPath <- prev_path[1:i]

      edgesToDelete <- find_edges_to_delete(A, i,rootPath)
      t_g <- delete.edges(graph, edgesToDelete)
      #for (edge in edgesToDelete) t_g <- delete.edges(t_g, edge)

      spurPath <- shortest_path(t_g,spurNode, dest)

      if (!is.null(spurPath)){
        total_path <- list(c(rootPath[-i], spurPath))
        if (!total_path %in% B) B[length(B)+1] <- total_path
      }
    }
    if (length(B) == 0) break
    B <- sort_paths(graph, B)
    A[k_i] <- B[1]
    B <- B[-1]
    }
  A
}

k_shortest_yen(as.directed(nn_mat %E>% filter(time == 1200), mode = 'mutual'), 1, 12, 10)
```
```{r eval = FALSE}
set.seed(1)
g <- erdos.renyi.game(100,.2)

plist <- do.call(c, lapply(V(g), function(v) get.shortest.paths(g,v,V(g), output ='epath')$epath))
psize <- data.frame(i = 1:length(plist), plength = sapply(plist, length))
top10 <- head(psize[order(-psize$plength),],10)
elist <- unlist(plist[top10$i])
finalg <- subgraph.edges(g, elist)
plot(g);plot(finalg)
```

```{r eval = FALSE}

all_shortest_paths(graph= spatial_net, from = 1, to = 12, mode = 'all', weights = as_tibble(spatial_net, activate = 'edges')$distance)
```

```{r, eval = FALSE}
#devtools::install_github('ecohealthalliance/yenpathy')
library(yenpathy)
spatial_net <- nn_mat %E>%
  filter(time == 1200) %>%
  as_tibble %>%
  select(from, to, distance) %>%
  makedirect()
sim_net <- swsn %E>%
  as_tibble %>%
  filter(time == 1200) %>% 
  filter(to < from)
makedirect <- function(x){
  x %>%
    select(from = to, to = from, distance = distance) %>%
    bind_rows(x)
}
sim_net[[1,1]]
t1 <- c()
sim_net$paths <- lapply(1:nrow(sim_net), function(x){
  k_shortest_paths(graph_df = spatial_net, start_vertex = sim_net[[x,1]], end_vertex = sim_net[[x,2]], k = 4, verbose = FALSE)}
)

test[[1]]
for(i in 1:10){
  t1 <- c(t1, k_shortest_paths(graph_df = spatial_net, start_vertex = sim_net[[i,1]], end_vertex = sim_net[[i,2]], k = 4, verbose = FALSE))
}

t1 <- sim_net %>% mutate(k_paths = map2(from, to, ~k_shortest_paths(graph_df = spatial_net, start_vertex = .x, end_vertex = .y, k = 4, verbose = FALSE)))


swsn %E>% filter(time == 1200) %>% select(from, to, di)
swsn %>% as.undirected %>% as_tbl_graph
test %>% k_shortest_paths(1, 3, k = 5)
k_shortest_paths(spatial_net,1, 12, k = 1)


nn_mat %E>%
  filter(time == 1200) %N>%
  filter(!node_is_isolated())
```



```{r shortest_paths, eval = FALSE}
path_dat <- nn_mat %E>%
  as_tibble %>%
  select(from,to) %>%
  distinct %>% # limit to distinct edges
  group_by(from) %>% # group by origin node
  nest %>% # nest by origin node
  mutate(to = map(data, ~c(.$to) %>% unlist)) %>% # turn the "to" column into list of destinations for each origin
  select(-data) %>% # drop the nested data frame
  # calculate the shortest paths
  mutate(paths = map2(from, to, ~shortestPath(conductance_c, 
                                    as.matrix(pts)[.x,], as.matrix(pts)[.y,], 
                                    output = 'SpatialLines')))
```


Merge all the lines into one object.
```{r eval = FALSE}
paths_sf <- path_dat %>% 
  mutate(paths = map(paths, st_as_sf)) %>% 
  pull(paths) %>% 
  do.call(rbind, .) %>%
  bind_cols((path_dat %>% select(-paths) %>% unnest), .) %>%
  st_sf %>%
  filter(., st_is_valid(.)) %>%
  smooth(method = 'ksmooth', smoothness = 20) 
```

```{r eval = FALSE}
paths_sf %>%
  inner_join(activate(swsn, 'edges') %>% as_tibble, .) %>%
  ggplot() +
  geom_sf(alpha = .1) +
  facet_wrap(~time, nrow = 1)

paths_sf %>%
  inner_join(activate(swsn, 'edges') %>% as_tibble, .) %>%
  st_sf %>%
  as_Spatial %>%
  fortify(region = 'id') %>%
  left_join(inner_join(activate(swsn, 'edges') %>% as_tibble, paths_sf) %>% mutate(id = as.character(1:n())) %>% select(-geometry)) %>%
ggplot(aes(long, lat)) +
  geom_raster(data = as.data.frame(aggregate(raster(conductance_c), fact = 2), xy = T, na.rm = T), aes(x, y, fill = layer)) +
  geom_path(aes(group = group), alpha = .4) +
  scale_fill_viridis_c() +
  facet_wrap(~time, nrow = 1) +
  theme_void() +
  coord_equal()
```


```{r network_plot, eval = FALSE, echo = FALSE}
states <- maps::map('state', regions = c('arizona', 'new mexico'), fill = TRUE, plot = FALSE)

swsn %E>%
  arrange(similarity) %>%
ggraph('manual', node.positions = pts) +
  geom_edge_fan(aes(alpha = similarity, color = similarity)) +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  geom_node_point() +#aes(size = centrality, color = centrality)) +
  facet_edges(~time) +
  scale_edge_alpha() +
  coord_equal() +
  theme_graph()
```

# Climate Analysis
First we estimate present and past climate patterns.

## Study Area
First we define a boundary box covering much of the western United States, ranging between W$124\deg$ and W$105\deg$ and N$31\deg$ and N$41\deg$, which we'll use to constrain all subsequent climate analyses. This study area is significantly larger than the one we'll define in the next section for the network analysis. This allows us to sample a much wider range of climatic variability, while still remaining within the broader western US climate zone. This ensures that both A) our statistical analyses will be more robust to sampling error and B) the results will be less sensitive to the exact location and dimensions of our study area.
```{r bbox_map, fig.margin=TRUE, echo = FALSE, fig.cap = 'Locations of 2 bounding boxes.'}
bbox_wus <- extent(c(-124, -105, 31, 41))
bbox <- extent(c(-113.5, -106.5, 31, 37.5))
bb1 <- as(bbox_wus, 'SpatialPolygons') %>%
  fortify
bb2 <- as(bbox, 'SpatialPolygons') %>%
  fortify

map_data("usa") %>%
  ggplot(aes(x = long, y = lat, group = group)) + 
  geom_polygon(color = 'darkgrey', fill = NA) + 
  geom_polygon(data = bb1, color = 'black', fill = NA) +
  geom_polygon(data = bb2, color = 'black', fill = NA) +
  coord_fixed(1.3) +
  theme_void()
```

```{r bbox}
bbox_wus <- extent(c(-124, -105, 31, 41))
```

## Climate Dataa
Import high resolution SPEI maps generated from PRISM data. Calculate the average summertime (JJA) SPEI value for each year.
```{r import_observations}
# calculate average JJA SPEI
spei_obs <- ((brick('data/spei12_6_PRISM.nc') + 
              brick('data/spei12_7_PRISM.nc') + 
              brick('data/spei12_8_PRISM.nc')) / 3) %>%
  crop(bbox_wus) %>% # crop to the western US bounding box
  `names<-`(1895:2017) %>% # add year names
  .[[2:105]]  # SPEI calculated on 12 month lag, so drop 1st year
```

Import the reconstructed SPEI fields from PHYDA. PHYDA uses a novel off-line data assimilation approach, using simulated SPEI from the CESM LME experiments as physically-consistent model priors and a network of tree rings, ice cores, and corals as assimilated observations.
```{r import_reconstructions}
spei_recon <- brick('data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc',
                    varname = 'spei_mn') %>%
   .[[1100:1999]] %>% # extract years of interest
   rotate %>% # rotate longitudes to -180 to 180
   crop(bbox_wus, snap = 'out') # crop to western US
```


```{r plot_variance_trend, echo = FALSE, fig.margin=TRUE, fig.cap = 'Plotting out the time series of the reconstruction reveals no clear trend in the mean. It does, however, show a pattern of increasing variance over time, which is entirely a function of the changing number of proxies used in the data assimilation approach.'}
spei_recon %>% 
  as.data.frame(xy = TRUE, na.rm = TRUE) %>%
  gather(year, value, 3:902) %>%
  mutate(year = str_sub(year, 2),
         year = as.numeric(year)) %>%
  ggplot(aes(year, value)) +
  geom_line(alpha = .1, aes(group = interaction(x, y))) +
  geom_smooth() +
  ggtitle('Standardized Precipitation-Evapotranspiration Index in the American Southwest',
          'June-August, 12 month lag') +
  theme_bw()
```

## Empiricial Orthogonal Functions
We calculate the leading Empirical Orthogonal Functions of a ~100 year series of the summertime average Standardized Precipitation-Evapotranspiration Index. These patterns then undergo a varimax rotation to highlight more physically meaningful spatial patterns. We compare these patterns to those derived from a long term (~2ka) reconstruction based on data assimilation and CESM LME. We confirm that the spatial patterns detected for the past 100 years have been robust over time and explain up to 85% of the variance in a full 1,000 year sequence of reconstructed drought dynamics.

Now we calculate the rotated empirical orthogonal functions for both the observed and reconstructed drought maps. For the observations, we see that the leading 5 eofs explain 85% of the variance in the series, so we'll retain those for rotation.
```{r plot_variance_prism, echo = FALSE}
#this code is the same as the eofNum function above, but needs a slight adjustment here because that function assumes more time than spatial indices
eigs <- prcomp(t(as.data.frame(spei_obs, na.rm = TRUE)), scale. = FALSE)[["sdev"]]^2
eigs.pct <- 100 * eigs/sum(eigs)
eigs.lo <- eigs * (1 - sqrt(2/104))
eigs.hi <- eigs * (1 + sqrt(2/104))
cumvar <- round(cumsum(eigs.pct), 1)
p <- 104
d <- data.frame(rank = factor(1:p), eigs, eigs.lo, eigs.hi, 
        cumvar)
d <- within(d, cumvar.line <- eigs.hi + 0.02 * max(eigs.hi))
d <- d[1:min(p, 10), ]
ggplot(data = d, aes(x = rank, y = eigs)) + geom_errorbar(aes(x = rank, 
        ymin = eigs.lo, ymax = eigs.hi), width = 0.3) + geom_point(size = 3) + 
        geom_text(aes(x = rank, y = cumvar.line, label = cumvar), 
            size = 3, vjust = 0) + labs(list(x = "Rank", y = "Eigenvalue")) + 
        theme(panel.grid.minor = element_blank())
```

Look at the variances for the eofs of each field, to inform truncation. Let's retain the leading four modes from PHYDA
```{r plot_variance_phyda, fig.margin=TRUE, echo = FALSE,fig.cap = 'Variance explained'}
spei_recon %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  eofNum(scale. = FALSE)
```
Repeat for the prism observations. Again, we'll retain the leading four modes.

The standard errors on the above plots assume there is no autocorrelation. Check to see if that is indeed the case.
```{r plot_acf, fig.margin=TRUE, echo = FALSE,fig.cap = 'Temporal autocorrelation'}
spei_recon %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  as_tibble %>%
  gather(key, value) %>%
  pull(value) %>%
  acf

spei_obs %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>%
  as_tibble %>%
  gather(key, value) %>%
  pull(value) %>%
  acf
```
It looks like there is some autocorrelaiton, will have to adjust in the future.


Now, calculate the eofs for both fields, retaining the 4 leading components in each case for rotation
```{r calc_eofs}
# Decide how many modes to retain
n_modes <- 4 

obs_eof <- spei_obs %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  eof(n_modes, scale. = FALSE)

recon_eof <- spei_recon %>%
  as.data.frame(na.rm = TRUE) %>%
  t %>% # transpose space and time
  eof(n_modes, scale. = FALSE)
```
Repeat for the unroated eofs
```{r}
pr <- prcomp(t(as.data.frame(spei_obs, na.rm = TRUE)), scale. = FALSE)
eigenval <- pr[["sdev"]][1:4]^2
eigenvec <- pr[["rotation"]][, 1:4]
eof <- eigenvec %*% diag(sqrt(eigenval), 4, 4)
scores <- pr[["x"]][, 1:4]
amp <- scale(scores)
attributes(amp)$`scaled:center` <- attributes(amp)$`scaled:scale` <- NULL
eof_obs <- as.matrix(eof)
colnames(eof_obs) <- colnames(amp) <- paste("EOF", 1:4, sep = "")
rownames(eof_obs) <- colnames(t(as.data.frame(spei_obs, na.rm = T)))

pr <- prcomp(t(as.data.frame(spei_recon, na.rm = TRUE)), scale. = FALSE)
eigenval <- pr[["sdev"]][1:4]^2
eigenvec <- pr[["rotation"]][, 1:4]
eof <- eigenvec %*% diag(sqrt(eigenval), 4, 4)
scores <- pr[["x"]][, 1:4]
amp <- scale(scores)
attributes(amp)$`scaled:center` <- attributes(amp)$`scaled:scale` <- NULL
eof_recon <- as.matrix(eof)
colnames(eof_recon) <- colnames(amp) <- paste("EOF", 1:4, sep = "")
rownames(eof_recon) <- colnames(t(as.data.frame(spei_recon, na.rm = T)))
```


Let's map out the spatial and temporal patterns in the REOFs. First we'll look at the spatial patterns. It looks like the observed and reconstructed datasets reveal very similar spatial patterns for the 4 leading modes.
```{r plot_reof_recons, fig.margin = TRUE, echo = FALSE, fig.cap = 'Reconstructed REOFs'}
as.data.frame(spei_recon[[1]], xy = TRUE, na.rm = TRUE) %>% cbind(recon_eof$REOF) %>%
  select(-3) %>%
  gather(eof, value, 3:(n_modes+2)) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
    scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() + 
  coord_quickmap()
```

```{r plot_eof_recons, fig.margin = TRUE, echo = FALSE, fig.cap = 'Reconstructed REOFs'}
as.data.frame(spei_recon[[1]], xy = TRUE, na.rm = TRUE) %>% cbind(eof_recon) %>%
  select(-3) %>%
  gather(eof, value, 3:(4+2)) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
    scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() + 
  coord_quickmap()
```

```{r plot_eof_obs, echo = FALSE, fig.width = 6, fig.height = 4, fig.cap = 'Observed drought REOFs'}
as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>% 
  cbind(eof_obs) %>%
  select(-3) %>%
  gather(eof, value, 3:(4 + 2)) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
  scale_fill_distiller(palette = 'BrBG', direction = 1, limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() +
  coord_quickmap(xlim = c(-124, -106), ylim = c(31, 40)) +
  geom_polygon(data =  map_data("state"), aes(x = long, y = lat, group = group), color = 'darkgrey', fill = NA) +
  ggtitle('Leading 4 unrotated empirical orthogonal functions',
          'SPEI') +
  theme(legend.position = "bottom")
```

```{r plot_reof_obs, echo = FALSE, fig.width = 6, fig.height = 4, fig.cap = 'Observed drought REOFs'}
as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>% 
  cbind(obs_eof$REOF) %>%
  select(-3) %>%
  mutate(EOF1 = -1 * EOF1) %>%
  gather(eof, value, 3:(n_modes + 2)) %>%
  mutate(value = value * -1) %>%
  ggplot(aes(x,y)) +
  geom_raster(aes(fill = value)) +
  scale_fill_distiller(palette = 'BrBG', direction = -1, limits = c(-1, 1)) +
  #scale_fill_viridis_c(option = 'magma', limits = c(-1, 1)) +
  facet_wrap(~eof) +
  theme_void() +
  coord_quickmap(xlim = c(-124, -106), ylim = c(31, 40)) +
  geom_polygon(data =  map_data("state"), aes(x = long, y = lat, group = group), color = 'darkgrey', fill = NA) +
  ggtitle('Leading 4 rotated empirical orthogonal functions',
          'SPEI') +
  theme(legend.position = "bottom")
```

A key assumption here is that the REOFs calculated from both the observations and reconstructions correspond to the same physical phenomena. That way we can just use the observed patterns as the high resolution patterns, and infer their temporal evolution from the reconstructions. This means we don't have to downscale the reconstructed reofs to use them, as otherwise we'd have to deal with issues of spatial represnetaitveness an non-overlap. Now we can be ensured that these are the same signals.

To confirm this, let's plot the time series (PCs) of each observed and reconstructed REOF against each other, to confirm that they correspond to the same patterns. Look at the correlations between the modes.
```{r amplitudes}
amplitudes <- left_join(
  recon_eof$amplitude %>%
    .[797:900,] %>%
    as_tibble %>%
    mutate(year = 1896:1999) %>%
    gather(eof, recon, 1:n_modes),
  obs_eof$amplitude %>%
    as_tibble %>%
    mutate(year = 1896:1999) %>%
    gather(eof, obs, 1:n_modes))
```

```{r plot_amplitudes, fig.fullwidth=TRUE, fig.height=4, fig.width=10, echo = FALSE, fig.cap = 'Strong linear correlation between the observed and reconstructed EOFs'}
ggplot(amplitudes, aes(recon, obs)) +
  geom_point(aes(color = year)) +
  geom_smooth(method = 'lm') +
  scale_color_viridis() +
  facet_wrap(~eof, nrow = 1) +
  ggtitle('Linear fits between observed and reconstructed amplitudes', '1896-1999') +
  theme_minimal() +
  coord_fixed() +
  theme(legend.position = "bottom")
```

```{r eval = F}
library(remote)
sst_jja <- brick('~/Downloads/sst.mnmean.v4.nc') %>%
  .[[505:1752]] %>%
  .[[sort.int(c(seq(6, 1248, 12),seq(7, 1248, 12),seq(8, 1248, 12)))]] %>%
  stackApply(rep(1:104, each = 3), mean) %>%
  anomalize

test <- calc(sst_jja, fun=function(x) cor(x,obs_eof$amplitude[,1]))
plot(test)
cor(sst_jja[1,1], t1$EOF1)


sst_ann <- brick('~/Downloads/sst.mnmean.v4.nc') %>%
   .[[508:1755]] %>%
  stackApply(rep(1:104, each = 12), mean) %>%
  anomalize
```

Now let's fit these models in practice. First create a data frame of past amplitudes.
```{r past_amplitudes}
past_amplitudes <- recon_eof$amplitude %>%
    as_tibble %>%
    mutate(year = 1100:1999) %>%
    gather(eof, recon, 1:n_modes) %>%
  group_by(eof) %>%
  nest(.key = 'recons')
```


We can fit gams between the observed and reconstrcuted PC time series, and then use these to predict the eof amplitudes back in time. Adding in this regression step essentalially bias-corrects the reconstructed amplitudes
```{r recon_amplitudes}
recon_amplitudes <- amplitudes %>%
  group_by(eof) %>%
  nest %>%
  mutate(mod = purrr::map(data, ~gam(obs ~ s(recon, bs = 'cr'), 
                                     data = ., method = 'REML', 
                                     select = TRUE))) %>%
  left_join(past_amplitudes, by = 'eof') %>%
  mutate(predictions = purrr::map2(mod, recons, 
                                   ~predict(.x, .y, type = 'response')),
         predictions = purrr::map(predictions, ~.[1:400]),
         predictions = purrr::map(predictions, 
                                  ~tibble(year = 1100:1499, amplitude = .)),
         predictions = purrr::map(predictions, 
                                  ~mutate(., 
                                          amp_smooth = zoo::rollmean(amplitude, k = 51, fill = NA)))) %>%
  select(eof, predictions) %>%
  unnest %>%
  mutate(period = floor((year/50)) * 50) 
```

How have the amplitudes of these patterns changed over time? Let's plot the reconstructed time series for each, along with an estimated trend line.
```{r plot_recon_amplitudes, fig.width = 10, fig.height = 4, echo = FALSE, fig.fullwidth=TRUE}
ggplot(recon_amplitudes, aes(year, amplitude, group = eof)) +
  geom_line(aes(color = eof), alpha = .7) +
  geom_line(aes(y = amp_smooth)) +
  scale_color_brewer(palette = 'Spectral') +
  facet_wrap(~eof) +
  theme_minimal()+
  theme(legend.position = "bottom")
```

These results point to EOFs 3 and 4 as having major shocks at aropund 1300 AD.
```{r, echo = FALSE, fig.margin=TRUE, fig.cap='Are there any time periods with marked signals?'}
recon_amplitudes %>%
  filter(between(period, 1200, 1400)) %>%
  ggplot(aes(factor(period), amplitude)) +
  geom_boxplot() +
  geom_hline(yintercept = 0, color = 'red', linetype = 2) +
  scale_color_brewer(palette = 'Spectral') +
  facet_wrap(~eof) +
  theme_minimal()
```

## Environmental Data
Import higher resolution environmental layers and crop the reof files. Define a function that will import all the prism normals for a given variable name. outputs a brick.
```{r import_prism}
get_normals <- function(var){
  paste0('data/PRISM/PRISM_', var, '_30yr_normal_800mM2_all_asc/') %>%
  list.files(pattern = '\\.asc$', full.names = TRUE) %>%
  .[1:12] %>% # the last file in the list is annual values, drop it
  purrr::map(raster) %>% # load the rasters from file list
  purrr::map(crop, bbox_wus) %>% # crop to study area
  brick # combine list of rasters into a brick
}
```

Use the above function to import precipitation and temperature range normals. Use the biovars function from dismo to calculate 19 bioclimatic predictor variables from monthly precipitation and temperature data.
```{r bioclim_pca, cache = TRUE}
prec <- get_normals('ppt')
tmin <- get_normals('tmin')
tmax <- get_normals('tmax')

# calculate bioclim variables from the monthly normals
bio_clim <- biovars(prec, tmin, tmax)
```

Reduce the dimensionality of the bioclimatic data using an R-mode PCA.
```{r cache = TRUE}
clim_pca <- RStoolbox::rasterPCA(bio_clim, spca = TRUE)
```

Retain the leading 4 PCs.
```{r}
screeplot(clim_pca$model, npcs = 20, type = 'lines')
bio_clim %>%
  as.data.frame(na.rm = TRUE) %>%
  eofNum(scale. = TRUE)
```

```{r reof_raster}
reof_raster <- as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>%
  select(x:y) %>%
  cbind(obs_eof$REOF) %>%
  rasterFromXYZ %>%
  `crs<-`('+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0') %>%
  `names<-`(c('reof1', 'reof2', 'reof3', 'reof4')) %>%
  projectRaster(bio_clim)
eof_raster <- as.data.frame(spei_obs[[1]], xy = TRUE, na.rm = TRUE) %>%
  select(x:y) %>%
  cbind(eof_obs) %>%
  rasterFromXYZ %>%
  `crs<-`('+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0') %>%
  `names<-`(c('eof1', 'eof2', 'eof3', 'eof4')) %>%
  projectRaster(bio_clim)
```

```{r raster_reprojection}
env_rasters <- clim_pca$map %>%
  .[[1:4]] %>%
  `names<-`(c('pc1', 'pc2', 'pc3', 'pc4')) %>%
  c(eof_raster) %>%
  c(reof_raster) %>%
  brick
```

```{r}
plot(env_rasters)
```

```{r}
pairs(env_rasters)
```
```{r}
knitr::knit_exit()
```

# Model fitting
Now that we have all the data imported and preprocessed, let's put it all together into a data frame.
```{r}
dat <- swsn %N>%
  mutate(ID = 1:n(),
         lon = pts$x, 
         lat = pts$y) %>%
  left_join(raster::extract(env_rasters, pts, df = TRUE), 
            by = 'ID') %E>%
  left_join(distances, by = c('from', 'to')) %>%
  mutate(size_from = case_when(time == 1200 ~ .N()$P1room[from],
                               time == 1250 ~ .N()$P2room[from],
                               time == 1300 ~ .N()$P3room[from],
                               time == 1350 ~ .N()$P4room[from],
                               time == 1400 ~ .N()$P5room[from]),
         size_to = case_when(time == 1200 ~ .N()$P1room[to],
                             time == 1250 ~ .N()$P2room[to],
                             time == 1300 ~ .N()$P3room[to],
                             time == 1350 ~ .N()$P4room[to],
                             time == 1400 ~ .N()$P5room[to]),
         from_x = .N()$lon[from],
         from_y = .N()$lat[from],
         to_x = .N()$lon[to],
         to_y = .N()$lat[to],
         macro_from = .N()$Macro[from],
         macro_to =.N()$Macro[to],
         macro_same = macro_from == macro_to,
         micro_from = .N()$Micro[from],
         micro_to = .N()$Micro[to],
         micro_same = micro_from == micro_to,
         pc1_from = .N()$pc1[from],
         pc1_to = .N()$pc1[to],
         pc2_from = .N()$pc2[from],
         pc2_to = .N()$pc2[to],
         pc3_from = .N()$pc3[from],
         pc3_to = .N()$pc3[to],
         pc4_from = .N()$pc4[from],
         pc4_to = .N()$pc4[to],
         eof1_from = .N()$eof1[from],
         eof1_to = .N()$eof1[to],
         eof2_from = .N()$eof2[from],
         eof2_to = .N()$eof2[to],
         eof3_from = .N()$eof3[from],
         eof3_to = .N()$eof3[to],
         eof4_from = .N()$eof4[from],
         eof4_to = .N()$eof4[to],
         reof1_from = .N()$reof1[from],
         reof1_to = .N()$reof1[to],
         reof2_from = .N()$reof2[from],
         reof2_to = .N()$reof2[to],
         reof3_from = .N()$reof3[from],
         reof3_to = .N()$reof3[to],
         reof4_from = .N()$reof4[from],
         reof4_to = .N()$reof4[to]) %>%
  as_tibble %>%
  filter(centrality_from >= centrality_to)
  #mutate(rel_sim = similarity * (centrality_to / centrality_from),
  #       to = as.factor(to))
```

```{r, echo = FALSE}
knitr::kable(
  dat[1:6,], caption = 'A subset of the edge level data.'
)
```


The distribution for the final time step is very different from the others.
```{r}
library(ggridges)
ggplot(dat, aes(x = similarity, y = as.factor(time), fill = ..x.., height = ..density..)) +
  geom_density_ridges_gradient(stat = 'density', scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')

ggplot(dat, aes(x = similarity, y = as.factor(time), fill = ..x..)) +
  geom_density_ridges_gradient(scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')
```
Since we'll be using a logit link in the regressions, check the logit transform
```{r}
ggplot(dat, aes(x = qlogis(similarity), y = as.factor(time), fill = ..x.., height = ..density..)) +
  geom_density_ridges_gradient(stat = 'density', scale = 1.5) +
  scale_fill_viridis(guide = F) +
  theme_ridges(grid = FALSE, center_axis_labels = TRUE) +
  ggtitle('Change in similarity distributions with time')
```


By adding a rug plot, we can see that this is likely because the small number of edges sampled overall in the final period. Possibly this has additional meaning due to something like the salado phenomenon, by which ceramic assemblage similarities take on a different meaning?
```{r, fig.margin = TRUE}
ggplot(dat, aes(x = similarity, y = time, group = time, fill = ..x..)) +
  geom_density_ridges_gradient(jittered_points = TRUE,
                      position = position_points_jitter(width = 0.05, height = 0),
                      point_shape = '|', point_size = 3, alpha = 0.7) +
  scale_fill_viridis()+
  theme_ridges()

ggplot(dat, aes(x=similarity, group=time, fill=time)) + 
    geom_density(adjust=1.5, position="stack")
```
We can confirm this by looking at the number of edges from each time period.
```{r}
dat %>%
  group_by(time) %>%
  count
```

Let's just isolate the distributions from the first time period. Clearly these are non gaussian, so modeling them as such would be inappropriate. log and sqrt transforms seem similarly inappropriate.
```{r}
dat %>%
  filter(time == 1200) %>%
  qplot(x = log(similarity/(1-similarity)), data = ., geom = 'density')
```


```{r, eval = FALSE, echo = FALSE}
test <- recon_amplitudes %>%
  filter(between(period, 1200, 1400)) %>%
  group_by(eof, period) %>% 
  summarise(amplitude = mean(amplitude))
eof_data <- raster::extract(reof_raster, pts, df=TRUE) %>%
  gather(eof, value, 2:6) %>%
  full_join(test) %>%
  mutate(key = paste(eof, period, sep = '_'),
            value = value * amplitude) %>%
  select(ID, key, value) %>%
  spread(key, value, -1)
```

## Distance modeling

First, how does similarity of two settlements vary as a function of the distance between them?
```{r echo = FALSE, fig.width = 6, fig.height = 4.5, fig.fullwidth=TRUE}
ggplot(dat, aes(distance, rel_sim)) +
  geom_point(alpha = .1) +
  geom_smooth() +
  labs(title = 'Empirical distance decay function', x = 'Distance', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(distance, similarity)) +
  geom_point(alpha = .1) +
  geom_smooth(method = "gam", method.args = list(family = betar())) +
  labs(title = 'Empirical distance decay function', x = 'Distance', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(log(distance), log(similarity))) +
  geom_point(alpha = .1) +
  geom_smooth() +
  labs(title = 'Empirical distance decay function', x = 'Distance', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(distance, qlogis(similarity))) +
  geom_point(alpha = .1) +
  geom_smooth() +
  labs(title = 'Empirical distance decay function', x = 'Distance', y = 'Similarity') +
  theme_bw()
```
```{r}
dat %>%
  filter(time < 1301) %>%
  ggplot(aes(log(size_from * size_to), rel_similarity)) +
  geom_point(alpha = .1) +
  geom_smooth()#method = "gam", method.args = list(family = betar())) +
  labs(title = 'Empirical distance decay function', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
ggplot(dat, aes(centrality_to, size_to)) + geom_point()
```

```{r}
dat %>%
  filter(time < 1301) %>%
  ggplot(aes(distance, similarity)) +
  geom_point(alpha = .1) +
  geom_smooth(method = "gam", method.args = list(family = betar())) +
  labs(title = 'Empirical distance decay function', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

Do the fits to distance change with time? a little bit, but seems more to have to do with the decrease in sample size with time.
```{r echo=FALSE, distance_time_plot}
ggplot(dat, aes(distance, rel_sim)) +
  geom_point(alpha = .1, aes(color = time)) +
    scale_color_distiller(palette = 'Spectral', direction = 1) +
  geom_smooth(aes(group = time, color = time)) +
  geom_smooth() +
  theme_minimal()
```

```{r echo=FALSE, centrality_plot}
ggplot(dat, aes(centrality_to, rel_sim)) +
  geom_point(alpha = .1, aes(color = time)) +
    scale_color_distiller(palette = 'Spectral', direction = 1) +
  geom_smooth(aes(group = time, color = time)) +
  geom_smooth() +
  theme_minimal()
```


Since we're running a beta regression, we have a choice of multiple link functions. Let's compare cloglog and logit links. Define a function that, when applied to a dataset and a gam formula, will fit a gam of that formula. Let's define one for each link function.
```{r fit_gam}
fit_gam <- function(x){
  bam(x, method = 'REML', select = TRUE, 
         family = betar(link = 'logit'),#1/(2*nrow(dat)))
         data = filter(dat, similarity > 0 & similarity < 1))#(dat * (nrow(dat) - 1) + .5) / nrow(dat))
}
```

Now we write out a list of potential model structures, and fit gams using all of them. Calculate the AICs for each formula, link function combination, and reorder the amount from the lowest AIC
```{r geo_models}
geo_models <- c(
    "s(distance, bs = 'cr') + s(to, bs = 're')",
    "s(distance, bs = 'cr') + s(size_from, bs = 'cr') + s(size_to, bs = 'cr') + s(to, bs = 're')",
    "s(distance, bs = 'cr') + log(size_from) + log(size_to) + s(to, bs = 're')",
    "s(distance, bs = 'cr') + te(size_from, size_to) + s(to, bs = 're')",
    "s(distance, bs = 'cr') + s(size_from, size_to) + s(to, bs = 're')"
   ) %>%
  paste('rel_sim ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```


Here we can see that the best model includes distance, centrality, and size in a purely additive structure with a logit link function. This model gives an adjusted R2 of ~70%.
```{r}
geo_models
```

Let's visualize the model fits.
```{r geo_models_output}
summary(geo_models[[4,3]])
geo_models[[4,3]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)
geo_models[[4,3]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)
```

```{r}
#devtools::install_github("nspope/corMLPE")
library(corMLPE)
test <- dat %>%
  filter(similarity > 0 & similarity < 1) %>%
  mutate(from = as.character(from), to = as.character(to)) %>%
  gamm(qlogis(similarity) ~ s(distance, bs = 'cr'), 
         family = gaussian(link='identity'),
         correlation = corMLPE(form = ~from + to),
         data = .)

t1<-  gam(similarity ~ s(distance, bs = 'cr'), 
         family = gaussian(link='identity'),
         data = dat %>%
  filter(similarity > 0 & similarity < 1))

t2<- gam(similarity ~ s(distance, bs = 'cr'), 
         family = gaussian(link='logit'),
         data = dat %>%
  filter(similarity > 0 & similarity < 1))
t3<- gam(similarity ~ s(distance, bs = 'cr'), 
         family = betar(link='logit'),
         data = dat %>%
  filter(similarity > 0 & similarity < 1))

t4<- bam(qlogis(similarity) ~ s(distance, bs = 'cr'), method = 'REML', select = T,
         data = dat %>%
  filter(similarity > 0 & similarity < 1))

AIC(t4, t1, t2, t3)
plot(t1, residuals = T)

simulate_IBD_corMLPE(elements = 50) %>%
  mgcv::gamm(y ~ s(x),
         correlation = corMLPE(form=~pop1+pop2),
         data = .)

```


So let's select this model as our baseline geographic model.
```{r real_geo_mod}
mod <- bam(similarity ~ s(distance, bs = 'cr'),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = filter(dat, similarity > 0 & similarity < 1))
inv.logit <-  function(x) exp(x)/(1+exp(x))
plot(mod, scheme = 1, shift = -.833739, trans = inv.logit, residuals = F)
summary(mod)

gam.check(mod)
```
```{r}
dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(size_from * size_to, resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_distiller(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(size_to, resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_distiller(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```
```{r}
dat %>%
  filter(similarity == 0)

dat %>%
  filter(similarity == 1)

mod2 <- dat %>%
  filter(similarity > 0 & similarity < 1) %>%
  bam
```

```{r}
ggplot(dat, aes(abs(reof1_from - reof1_to), similarity)) +
  geom_point(alpha = .1, aes(color = time)) +
    scale_color_distiller(palette = 'Spectral', direction = 1) +
  geom_smooth(aes(group = time, color = time)) +
  geom_smooth() +
  theme_minimal()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(abs(reof4_from - reof4_to), resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_distiller(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(abs(size_from - size_to), resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_distiller(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(abs(pc4_from - pc4_to), resid)) +
  geom_point(aes(color = time), alpha = .2) +
  geom_smooth(aes(group = time, color= time)) +
  geom_smooth() +
  scale_color_distiller(palette = 'Spectral', direction = 1) +
  #labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}

mod1 <- gam(similarity ~ s(distance, bs = 'cr') + 
            s(size_from, bs = 'cr') + 
            s(size_to, bs = 'cr') + 
            s(centrality_from, bs = 'cr') + 
            s(centrality_to, bs = 'cr'), 
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod15 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(size_from, bs = 'cr') + 
            s(size_to, bs = 'cr') + 
            s(centrality_from, bs = 'cr') + 
            s(centrality_to, bs = 'cr'), 
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod2 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(centrality_from, bs = 'cr') +
            s(centrality_to, bs = 'cr') +
            s(pc1_from, bs = 'cr') + 
            s(pc1_to, bs = 'cr') + 
            s(pc2_from, bs = 'cr') +
            s(pc2_to, bs = 'cr') + 
            s(pc3_from, bs = 'cr') +
            s(pc3_to, bs = 'cr') + 
            s(pc4_from, bs = 'cr') +
            s(pc4_to, bs = 'cr') + 
            s(reof1_from, bs = 'cr') + 
            s(reof1_to, bs = 'cr') + 
            s(reof2_from, bs = 'cr') +
            s(reof2_to, bs = 'cr') + 
            s(reof3_from, bs = 'cr') +
            s(reof3_to, bs = 'cr') + 
            s(reof4_from, bs = 'cr') +
            s(reof4_to, bs = 'cr'),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod3 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(centrality_from, bs = 'cr') +
            s(centrality_to, bs = 'cr') +
            s(pc1_from, bs = 'cr') + 
            s(pc1_to, bs = 'cr') + 
            s(pc2_from, bs = 'cr') +
            s(pc2_to, bs = 'cr') + 
            s(pc3_from, bs = 'cr') +
            s(pc3_to, bs = 'cr') + 
            s(pc4_from, bs = 'cr') +
            s(pc4_to, bs = 'cr'),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod4 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(centrality_from, bs = 'cr') +
            s(centrality_to, bs = 'cr') +
            s(reof1_from, bs = 'cr') + 
            s(reof1_to, bs = 'cr') + 
            s(reof2_from, bs = 'cr') +
            s(reof2_to, bs = 'cr') + 
            s(reof3_from, bs = 'cr') +
            s(reof3_to, bs = 'cr') + 
            s(reof4_from, bs = 'cr') +
            s(reof4_to, bs = 'cr'),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod5 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(centrality_from, centrality_to) +
            s(reof1_from, reof1_to) + 
            s(reof2_from, reof2_to) + 
            s(reof3_from, reof3_to) + 
            s(reof4_from, reof4_to),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

mod6 <- bam(similarity ~ s(distance, bs = 'cr') + 
            s(centrality_from, centrality_to) +
            s(pca1_from, pca1_to) + 
            s(pca2_from, pca2_to) + 
            s(pca3_from, pca3_to) + 
            s(pca4_from, pca4_to),
            method = 'REML', select = TRUE, 
          family = betar(link = 'logit', eps = .001),
          data = dat)

plot(mod2)
summary(mod2)


summary(mod3);summary(mod4)
```


## Cultural costs
Higher than average similarity within macroregions, but not significantly when you control for distance. Looks like there are a lot of outliers withhigh simliarity between macroregion that we're not yet capturing. the difference shrinks when we control for difference, because the "same macrogregion" will covary with distance

```{r}
ggplot(dat, aes(macro_same, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(macro_same, distance)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Distance') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(macro_same, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
ggplot(dat, aes(macro_from, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(macro_to, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()
```
```{r}
dat %>%
  na.omit %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
ggplot(aes(macro_from, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```
```{r}
ggplot(dat, aes(micro_same, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share microregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(micro_same, distance)) +
  geom_boxplot() +
  labs(title = '', x = 'Share microregion?', y = 'Distance') +
  theme_bw()

dat %>%
  na.omit %>%
  mutate(resid = residuals(mod, type = 'response')) %>%
ggplot(aes(micro_same, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
ggplot(dat, aes(micro_from, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()

ggplot(dat, aes(micro_to, similarity)) +
  geom_boxplot() +
  labs(title = '', x = 'Share macroregion?', y = 'Similarity') +
  theme_bw()


dat %>%
  na.omit %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
ggplot(aes(micro_to, resid)) +
  geom_boxplot() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```

```{r}
geo_models
```

## Average climmate models
Now we can look weather average climate conditions have any predictive power for link strength.
```{r clim_mods}
clim_models <- c(
      "s(distance, bs = 'cr') + s(size_from, size_to) +
          s(reof1_from, reof1_to) + s(reof2_from, reof2_to) + 
          s(reof3_from, reof3_to) + s(reof4_from, reof4_to) + s(to, bs = 're')",
      "s(distance, bs = 'cr') + s(size_from, size_to) + 
          s(pc1_from, pc1_to) + s(pc2_from, pc2_to) + 
          s(pc3_from, pc3_to) + s(pc4_from, pc4_to) + s(to, bs = 're')"
      ) %>%
  paste('rel_sim ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```
```{r}
clim_models
```

```{r}
dat %>%
  na.omit %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
ggplot(aes(macro_from == macro_to, resid)) +
  geom_point(alpha = .1) +
  geom_smooth() +
  labs(title = '', x = 'Distance', y = 'Similarity') +
  theme_bw()
```
So think about the models to fit. micro same, macro same, and micro and macro same. repeat for micro/macro from/to pairs. Nest all that in same not same. then compare with no distance, distance, and distance + centrality

```{r}
clim_models
```

Let's visualize the model fits.
```{r clim_models_output}
clim_models[[1,3]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)

clim_models[[1,3]] %>% plot(scheme = 1)

gam.check(clim_models[[2,3]])
```
```{r}
clim_models2 <- c(
        "s(distance, bs = 'cr')",
      #"s(distance, bs = 'cr') + s(size_from, bs = 'cr', id = 1) + s(size_to, bs = 'cr', id = 1)",
       #     "s(distance, bs = 'cr') + s((size_from * size_to), bs = 'cr')",
      #"s(distance, bs = 'cr') + s((size_from + size_to), bs = 'cr')",
      #"s(distance, bs = 'cr') + s(abs(size_from - size_to), bs = 'cr')",
      "s(distance, bs = 'cr') + s(abs(eof1_from - eof1_to), bs = 'cr') + s(abs(eof2_from - eof2_to), bs = 'cr') + s(abs(eof3_from - eof3_to), bs = 'cr') + s(abs(eof4_from - eof4_to), bs = 'cr')",
      "s(distance, bs = 'cr') + s(abs(reof1_from - reof1_to), bs = 'cr') + s(abs(reof2_from - reof2_to), bs = 'cr') + s(abs(reof3_from - reof3_to), bs = 'cr') + s(abs(reof4_from - reof4_to), bs = 'cr')",
      "s(distance, bs = 'cr') + s(abs(pc1_from - pc1_to), bs = 'cr') + s(abs(pc2_from - pc2_to), bs = 'cr') + s(abs(pc3_from - pc3_to), bs = 'cr') + s(abs(pc4_from - pc4_to), bs = 'cr')"
           #"s(distance, bs = 'cr') + s(abs(reof1_from - reof1_to), bs = 'cr') + s(abs(reof2_from - reof2_to), bs = 'cr') + s(abs(reof3_from - reof3_to), bs = 'cr') + s(abs(reof4_from - reof4_to), bs = 'cr') + s(abs(pc1_from - pc1_to), bs = 'cr') + s(abs(pc2_from - pc2_to), bs = 'cr') + s(abs(pc3_from - pc3_to), bs = 'cr') + s(abs(pc4_from - pc4_to), bs = 'cr')"
      ) %>%
  paste('similarity ~', .) %>%
  tibble(formula_chr = .) %>%
  mutate(formula = purrr::map(formula_chr, as.formula), 
         model = purrr::map(formula, fit_gam)) %>%
  mutate(aic = map_dbl(model, AIC)) %>%
  arrange(aic)
```

```{r}
clim_models2
```

Let's visualize the model fits.
```{r clim_models_output2}
clim_models2[[2,3]] %>% 
  getViz %>%
  plot %>%
  print(pages = 1)

clim_models2[[1,3]] %>% plot(scheme = 1, residuals = T)
clim_models2[[2,3]] %>% plot(scheme = 1, residuals = T)

summary(clim_models2[[1,3]])
gam.check(clim_models2[[1,3]])
```


Interesting. Looks like there is increased similarity between locations with similar precipitation patterns or different temperature patterns. I should:
1. compare temperature and elevation regressions alone to see which is better
2. use unrotated EOFs, maybe winter ones too?
3. Use PCA of bioclim variables

## Variability models
```{r include = FALSE, eval = F}
  #  "ti(distance) + ti(size) + ti(distance, size)",
  #  "te(distance, size, k = 10)"
 #"s(distance, bs = 'cr') + s(abs(eof1), bs = 'cr', k = 15) + s(abs(eof2), bs = 'cr') + s(abs(eof3), bs = 'cr', k = 15) + s(abs(eof4), bs = 'cr', k = 15)",
#    "s(distance, bs = 'cr') + s(sqrt(abs(eof1)), bs = 'cr', k = 15) + s(abs(eof2), bs = 'cr') + s(abs(eof3), bs = 'cr', k = 15) + s(abs(eof4), bs = 'cr', #k = 15)"
```

```{r eval = FALSE}
mod5 <- gam(fractional_similarity ~ s(distance, bs = 'cr') +
              s(eof1, bs = 'cr') +
              s(eof2, bs = 'cr') +
              s(eof3, bs = 'cr') +
              s(eof4, bs = 'cr'),
            select = TRUE,
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)

mod6 <- gam(similarity ~ s(distance, bs = 'cr') +
              s(abs(eof1), bs = 'cr') +
              s(abs(eof2), bs = 'cr') +
              s(abs(eof3), bs = 'cr') +
              s(abs(eof4), bs = 'cr'),
            select = TRUE,
            method = 'REML',
            family = betar(link = 'cloglog'),
            data = dat)

summary(mod1)
plot(mod1)
gam.check(mod1)
```



```{r eval = FALSE}
mod2 <- gamm(similarity * 1000 ~ s(distance, bs = 'cr'),
            method = 'REML',
            family = poisson,
            correlation = corAR1(form = ~time|edge),
            data = dat)
```


```{r eval = FALSE}
swsn %E>%
  filter(centrality_from >= centrality_to) %>%
  mutate(resid = residuals(mod1, type = 'response')) %>%
  filter(resid > .25 | resid < -.25) %>%
  mutate(fac = if_else(resid > 0, 'pos', 'neg')) %>%
  ggraph('manual', node.positions = pts) +
  geom_edge_link(aes(alpha = abs(resid), color = resid)) +
  facet_graph(time ~ fac, col_type = 'edge') +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  scale_edge_colour_distiller(palette = 'RdBu', limits = c(-1,1)) +
  coord_equal() +
  theme_graph()
```
```{r eval = FALSE}
swsn %N>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[6,2]], type = 'response')) %>%
  filter(resid > .25 | resid < -.25) %>%
  mutate(fac = if_else(resid > 0, 'pos', 'neg')) %>%
  ggraph('manual', node.positions = pts) +
  geom_edge_link(aes(alpha = abs(resid), color = resid)) +
  facet_graph(time ~ fac, col_type = 'edge') +
  geom_polygon(data = states, aes(x = long, y = lat, group = region), color = 'black', fill = NA) +
  scale_edge_colour_distiller(palette = 'RdBu', limits = c(-1,1)) +
  coord_equal() +
  theme_graph()
```
Looks like distance is correlated with residuals
```{r eval = FALSE}
dat %>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  as_tibble %>%
  qplot(distance, resid, data = ., geom = 'point', alpha = I(.1), color = as.factor(time))
```
```{r eval = FALSE}
dat %>%
  filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  as_tibble %>%
  qplot(abs(eof1), resid, data = ., geom = 'point', alpha = I(.1), color = as.factor(time))
```
```{r eval = FALSE}
swsn %E>%
    filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  filter(time == 1) %N>%
  mutate(centrality = centrality_eigen(weights = similarity)) %E>%
  mutate(cent = .N()$centrality[from] * .N()$centrality[to]) %>%
  as_tibble %>%
  qplot(cent, resid, data = ., geom = 'point', alpha = I(.1))
```

```{r eval = FALSE}
swsn %E>%
    filter(similarity > 0) %>%
  mutate(resid = residuals(geo_models[[1,2]], type = 'response')) %>%
  filter(time == 3) %N>%
  mutate(centrality = centrality_eigen(weights = similarity)) %E>%
  mutate(cent = .N()$centrality[from] + .N()$centrality[to]) %>%
  as_tibble %>%
  qplot(cent, similarity, data = ., geom = 'point', alpha = I(.1))
```

```{r echo = FALSE, eval = FALSE}
phyda <- ncdf4::nc_open('data/da_hydro_JunAug_r.1-2000_d.05-Jan-2018.nc')
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,1])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'PacDelSST_mn')[1100:1999], recon_eof$amplitude[,4])

plot(ncdf4::ncvar_get(phyda, 'Pac130_mn')[1100:1999], recon_eof$amplitude[,2])
plot(ncdf4::ncvar_get(phyda, 'Pac130_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Pac160_mn')[1100:1999], recon_eof$amplitude[,4])
cor(ncdf4::ncvar_get(phyda, 'amo_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'amo_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_eof$amplitude[,1])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_eof$amplitude[,2])
cor(ncdf4::ncvar_get(phyda, 'Atl_mn')[1100:1999], recon_eof$amplitude[,3])
cor(ncdf4::ncvar_get(phyda, 'EPac_mn')[1100:1999], recon_eof$amplitude[,4])
```

# References {#references .unnumbered}
